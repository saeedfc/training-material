{"entries":[{"title":"","url":"/404.html","tags":[],"body":""},{"title":"","url":"/about.html","tags":[],"body":""},{"title":"","url":"/contribute.html","tags":[],"body":""},{"title":"","url":"/faq.html","tags":[],"body":"Overview Questions What is this website? What are the tutorials for? What audiences are the tutorials for? How is the content licensed? How can I advertise the training materials on my posters? How do I use this material? How can I get help? For Instructors Where do I start? How can I fix mistakes or expand an existing tutorial using the GitHub interface? Sustainability of the training-material and metadata Overview Questions What is this website? This website is a collection of hands-on tutorials that are designed to be interactive. This material is developed and maintained by the VIB Bioinformatics Core. What are the tutorials for? These tutorials can be used for learning and teaching how for general data analysis, and for learning/teaching specific domains such as metagenomcis and differential gene expression analysis with RNA-Seq data. What audiences are the tutorials for? There are two distinct audiences for these materials. Self-paced individual learners. These tutorials provide everything you need to learn a topic, from explanations of concepts to detailed hands-on exercises. Instructors. They are also designed to be used by instructors in teaching/training settings. Slides, and detailed tutorials are provided. How is the content licensed? The content of this website is licensed under the Creative Commons Attribution 4.0 License. How can I advertise the training materials on my posters? We provide some QR codes and logos in the images folder. How do I use this material? Many topics include slide decks and if the topic you are interested in has slides then start there. These will introduce the topic and important concepts. How can I get help? If you have questions about this training material, you can reach us sending an email to bits@vib.be. For Instructors This material can also be used to teach the content in a group setting to students and researchers. Where do I start? Spend some time exploring the different tutorials and the different resources that are available. Become familiar with the structure of the tutorials and think about how you might use them in your teaching. How can I fix mistakes or expand an existing tutorial using the GitHub interface? Please submit an issue via github. Sustainability of the training-material and metadata This repository is hosted on GitHub using git as a DVCS. Therefore the community is hosting backups of this repository in a decentralised way. The repository is self-contained and contains all needed content and all metadata."},{"title":"","url":"/hall-of-fame.html","tags":[],"body":""},{"title":"","url":"/topics/qbase-plus/","tags":[],"body":""},{"title":"","url":"/","tags":[],"body":""},{"title":"","url":"/badges/","tags":[],"body":""},{"title":"","url":"/topics/functional_analysis/","tags":[],"body":""},{"title":"","url":"/topics/gimp-inkscape/","tags":[],"body":""},{"title":"","url":"/topics/linux/","tags":[],"body":""},{"title":"","url":"/topics/data-management-plans/","tags":[],"body":""},{"title":"","url":"/topics/protein-structure-analysis/","tags":[],"body":""},{"title":"","url":"/topics/basic-bioinformatics/","tags":[],"body":""},{"title":"","url":"/topics/eln/","tags":[],"body":""},{"title":"","url":"/topics/metagenomics/","tags":[],"body":""},{"title":"","url":"/topics/R/","tags":[],"body":""},{"title":"Protein Structure Analysis","url":"/topics/protein-structure-analysis/slides/introduction.html","tags":[],"body":"### Protein Structure Analysis ### - Sequences, structures and databases - Experimental methods (X-rays, electrons and NMR) - Finding and visualising structures from the Protein Data Bank - Comparing structures - Modelling mutations - Creating homology models --- ### Sequences and Structures ### add one slide over transcription / translation / --- ### Amino acids and peptide structure ![](/topics/protein-structure-analysis/images/amino-acids.png) --- ### The Structure-Function Connection ### .pull-left[ - Folded proteins provide a well-defined 3D arrangement of functional groups, creating microenvironments and active sites. - Structural changes are often involved in functional mechanisms (motor proteins, ...) ] .image-90[![](/topics/protein-structure-analysis/images/hemoglobin.png)] --- ### Databases .pull-left[ **Uniprot** approx. 1100,000 sequences - mainly determined by large-scale DNA sequencing of individual genes or whole genomes - increasingly automated annotation **Protein Data Bank** approx. 141,000 experimentally determined structures - Mainly determined by X-ray crystallography and high-resolution NMR spectroscopy - Automation is increasing, but there are still significant limitations in the rate of solving new structures ] ] .pull-right[ .image-50[![](/topics/protein-structure-analysis/images/uniprot-logo.png)] .image-50[![](/topics/protein-structure-analysis/images/pdb-logo.png)] ] --- ### X-Ray Crystallography ### ![](/topics/protein-structure-analysis/images/xray-tech-setup.png) .pull-left[ .left[In a crystal, a large number of macromolecules are packed together in a regular grid, with consistent orientations and relative distances. When exposed to an X-ray beam, this arrangement gives rise to diffracted rays in specific directions, resulting in discrete spots on the planar detector. By rotating the crystal, a series of images is obtained. From these, the intensities of all the diffracted rays of the crystal can be derived.] ] .pull-right[ .image-90[![](/topics/protein-structure-analysis/images/diffraction-pattern.png)] ] --- ### X-Ray Crystallography ### .pull-left[ .image-50[![](/topics/protein-structure-analysis/images/diffraction-pattern.png)] ] .pull-right[.image-50[![](/topics/protein-structure-analysis/images/electron-density.png)]] | | | | |:-------------|:--------:|--------------:| | Diffraction Spot Intensities and Phases $$F_{obs}(h,k,l)$$ and $$\\phi_{obs}(h,k,l)$$ | $$R_{cryst} = \\frac{\\sum_{h,k,l}F_{obs}-F_{calc}}{\\sum_{h,k,l}F_{obs}}$$ | Electron density $$\\rho(x,y,z)$$ | --- ### The Protein Databank ### .pull-left[ .image-80[![](/topics/protein-structure-analysis/images/wwpdb-welcome-page.png)] .pull-right[ .left[ [http://www.wwpdb.org](http://www.wwpdb.org) ] - contains structures of proteins, nucleic acids and complexes, determined by X-ray crystallography, NMR spectroscopy - No purely theoretical or ab initio models (since 2006) - Also stores supporting experimental data - Full deposition now required by all peer-reviewed journals ] ] --- ### Exercise 1: Search the PDB ### - Use the UniProt site to search for “dnak”. - Use the PDB site to search for “dnak”. - Compare the UniProt and PDB result lists. - Use the sequence search function to see if there are structures with sequences similar to that of the DnaK C-terminal domain. - Look at the summary pages of a number of structures and note some interesting properties. - Select a number of structures and create a report page. --- ### PDB File Format ### ![](/topics/protein-structure-analysis/images/pdb-file-format.png) --- ### Occupancy ### ![](/topics/protein-structure-analysis/images/occupancy.png) --- ### Related Web sites ### - Nucleic Acid Database: DNA and RNA structures .left[[http://ndbserver.rutgers.edu/](http://ndbserver.rutgers.edu/)] - PDB-REDO: automatically re-refined deposited structures, using the latest methods .left[[http://www.cmbi.ru.nl/pdb_redo/](http://www.cmbi.ru.nl/pdb_redo)] - EBI: many online tools for structure analysis [http://www.ebi.ac.uk/Tools/structure/](http://www.ebi.ac.uk/Tools/structure/).left[]] - Replaced Electron Density Server: convenient overview of quality parameters for crystal structures .left[[http://www.ebi.ac.uk/pdbe/litemol](http://www.ebi.ac.uk/pdbe/litemol)] --- ### High-Resolution NMR Spectrometry ### .left[Many atomic nuclei, including the ubiquitous hydrogen nuclei, resonate at specific radio frequencies when placed in a strong, uniform magnetic field. The chemical environment of each individual atom slightly modulates its exact resonance frequency.] .image-80[![](/topics/protein-structure-analysis/images/nmr-peaks-to-structure.png)] .left[In macromolecules with thousands of atoms, many different effects combine to generate an extremely complicated pattern of chemical shifts, which therefore more or less uniquely identify each atom. Multidimensional spectra allow these frequencies to be assigned to specific atoms.] --- ### High-Resolution NMR Spectroscopy ### .left[When two atoms are near each other in 3D space, they can exchange magnetization, giving rise to crosspeaks at the intersection of their respective frequencies.] .image-50[![](/topics/protein-structure-analysis/images/nmr-noe.jpg)] .left[This nuclear Overhauser effect (NOE) is used to effectively measure the distances between pairs of atoms, at least qualitatively.] --- ### High-Resolution NMR Spectroscopy ### .left[After identification of the atoms by means of their unique chemical shifts, distance restraints are derived from the Overhauser crosspeaks. An extended model of the protein is generated, and then condensed into a shape that is consistent with as many of distance restraints as possible.] .image-50[![](/topics/protein-structure-analysis/images/nmr-model-example.png)] --- ### Other methods ### - Electron microscopy (and especially Cryo-electron microscopy): Electron crystallography and single particle reconstruction -Small-angle X-ray and neutron scattering (SAXS and SANS) .image-80[![](/topics/protein-structure-analysis/images/saxs.png)] --- ### Related Web sites ### - BioMagResBank: experimental data for NMR- derived structures (lists of distance restraints and other experimentally derived properties) .left[[http://www.bmrb.wisc.edu/](http://www.bmrb.wisc.edu/)] - BioIsis: database of SAXS-derived structures .left[[http://www.bioisis.net](http://www.bioisis.net)] - EMBL database of SAXS-derived structures .left[[http://www.sasbdb.org](http://www.sasbdb.org)] - EM Databank for cryo-EM structures [http://www.emdatabank.org](http://www.emdatabank.org.left[)] --- ### Assessing Structure Quality ### General geometric properties (bond lengths and angles, Ramachandran distribution, …): MolProbity [Link](http://molprobity.biochem.duke.edu/) .left[ **Crystal Structures** ] - Diffraction data resolution and completeness (PDB) - Final $$ R_{cryst} $$ and $$ R_{free} $$ factors (PDB) - Local correspondence to electron density (EDS) .left[**NMR Structures**] - Number of distance restraints and additional experimental data sources (BMRB) - Local restraint density (on-line NMR constraint analyser) [link](http://molsim.sci.univr.it/bioinfo/tools/constraint/index.html) .left[**Other techniques**] - Difficult to generalise: carefully read and consider the published protocol --- ### Molecular Graphics Software ### - [PyMOL](http://www.pymol.org/): high-quality output, good examples on [Wiki](http://www.pymolwiki.org/) - [Chimera](http://www.cgl.ucsf.edu/chimera/): good documentation on website - [VMD](http://www.ks.uiuc.edu/Research/vmd/): excellent for the analysis of MD trajectories - [Yasara](http://www.yasara.org) - [SwissPDBViewer](http://spdbv.vital-it.ch/) --- ### YASARA ### .left[ Yasara View is freely available and provides basic visualisation functions. Yasara Model, Dynamics and Structure provide additional functionality. An add-on module for NMR structures is available. The program can be combined with the WHAT- IF program for structure validation, and with FoldX for energy calculations.] --- ### Exercise 2: show a structure ### .left[ Load PDB entry 1TRZ using the File menu. Compare the default representations (F1-F8) and use the various controls to change the view of the protein. Explore the Edit and View menus to change various aspects of the graphical representation of the structure. Examine the hydrogen bonding patterns. Display a molecular surface. Create an interesting view of the molecule and save it as a high-resolution image. ] --- ### Protein folds are the structures of domains ### .left[ Similarities in assembly of secondary structure elements So not based on sequence like motifs but on 3D structure Folds represent the shapes of protein domains! ] Examples: TODO (add images e.g. alpha solenoid, DNA clamp, thioredoxin fold) --- ### Databases of protein folds ### - SCOP (http://scop.mrc-lmb.cam.ac.uk/scop/) - CATH (http://www.cathdb.info/) --- ### check on slides from course Wim Vranken ### see also (http://www.ii.uib.no/~slars/bioinfocourse/PDFs/structpred_tutorial.pdf) --- ### Structure Superposition ### .left[ Structures can be superimposed to achieve the best possible match between corresponding atoms. It is possible to consider all atoms, or only a subset (such as the Cα atoms). When the structures are very similar, determining which atoms should be matched to each other is trivial. When there are larger differences, it takes more preparation. Different algorithms use different combinations of sequence- and secondary of tertiary structure-based information. ] --- ### Exercise 3: Compare Structures ### .left[ Download the five provided PDB files and open them in Yasara. Use the `Analyze|Align|Objects` with MUSTANG function to align the four last objects with the first one. Use the space bar to open the text console and see the reported root mean square deviations as well as the number of residues matched. ] $$ rmsd = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}R_{i}^{2}} $$ .left[ Color all structures by B factor and compare the distribution to the local variability of the structures. ] --- ### PDB File Surprises ### - Missing atoms, residues and loops - Multiple molecules in the asymmetric unit - Incomplete oligomers due to coincident crystal and oligomer symmetry - Cα-only models lternate conformations - Multiple models, especially for NMR ensembles - Use of B factors to represent other properties ther non-standard extensions (PDBQT, ...) --- ### Force Fields ### - Energy terms representing physical interactions - Covalent bond lengths and angles - Dihedral angles and van der Waals forces (steric effects) - Electrostatic interactions and hydrogen bonds … - Energy can be minimized, and forces can literally be derived from the potential function. - Consistency and careful consideration of the properties to be simulated are essential. --- ### Force Field Terms ### .left[Each energy term has a functional form, which includes one or more parameters:] - Covalent bond energy term To do: add formula - Van der Waals contact energy term .left[The parameters are collectively optimized to reproduce a chosen set of experimentally observed parameters. A given force field should be used as a consistent system, and can only be used to predict properties that are covered by the training set. ] --- ### FoldX ### .left[ Is designed for quantitative modelling of the contribution of structural interactions to the stability of proteins and protein complexes. It also supports protein/DNA complexes. The force field describes the different interactions in a protein structure or complex in analytical terms. It has been calibrated using a set of experimentally determined stabilities. Applications include the optimisation of structures, the calculation of the stability of complexes, and predicting the effect of amino-acid or base-pair mutations on these properties. ] --- ### The FoldX Plugin for Yasara ### .left[ In order to make FoldX more accessible and integrate its functions into Yasara, dr. Joost van Durme (SWITCH laboratory) made a Yasara plugin module that can apply FoldX functions to structures that are loaded as Yasara objects. This greatly simplifies the use of FoldX, and allows for a quick visual analysis of the resulting changes in the structures. More information can be found at [wiki](http://foldxyasara.switchlab.org/index.php/) [FoldX](http://foldxsuite.crg.eu/) ] --- ### Exercise 4a: Repair a PDB File ### - Load the 1CRN PDB file. - Use the “Repair object” option in the Analysis | FoldX menu to activate the corresponding FoldX function. - Select the object to repair. .left[This exports the object as a temporary PDB file, starts FoldX with the appropriate options, and loads the repaired PDB file as a new object in Yasara.] - Compare the original and repaired objects. - Describe the changes that were introduced. --- ### Exercise 4b: Model a Mutation ### - Load the 2AC0.sce Yasara scene file. - Set an appropriate structure representation. - Locate residue Ala159 using the sequence view, and right-click to access the `FoldX|Mutate` residue function. Change it to a Trp residue. - Look at the effect of the substitution on the structure, and use the space bar to open the text console and read the output of the FoldX calculation. - Mutate Arg273 to an alanine side chain. Discuss the effects of this substitution. --- ### Homology Modelling ### - When a structure is available for a protein with a similar sequences, it is possible to predict the structure of a new sequence with varying degrees of confidence. - Use PSI-BLAST/DELTA-BLAST to detect sequences with similar structures. - All homology modelling procedures start from an alignment of the template and target sequences. The quality of this alignment will have a major impact on the resulting model. - Available applications include stand-alone programs (Modeller, FoldX, …) and web-based services (such as SwissModel). --- ### Exercise 5: Make a Homology Model using Swiss Model ### see []() --- ### Predict protein structures by fold recognition ### 1. Search SCOP/CATH for protein with same fold and known 3D structure 2. Align each amino acid of query sequence to a position in the template structure 3. Evaluate how well the sequence fits the fold and select best-fit fold 4. Build structural model of query based on alignment with selected fold - Phyre (http://www.sbg.bio.ic.ac.uk/phyre2/html/page.cgi?id=index) - HHpred (http://toolkit.lmb.uni-muenchen.de/hhpred) - DescFold (http://202.112.170.199/DescFold/) .left[Works because: - Number of different folds in nature is fairly small (approximately 1300) - 90% of new submissions in PDB have similar folds to those already in PDB - Not always accurate ] --- ### Guidelines to improve fold recognition results ### - Run as many methods as you can - Run each method on many sequences from your homologous protein family - After all of these runs, build up a consensus picture of the likely fold - Compare function of your protein to function of the proteins with the likely fold - Compare secondary structure of your protein to that of the likely fold --- ### Similarity searches based on 3D structure ### .left[ Similarity on structural level: aligning 3D structures Structure of query protein is known and aligned to PDB structures - VAST+ (https://www.ncbi.nlm.nih.gov/Structure/vastplus/vastplus.cgi) - DALI (http://ekhidna.biocenter.helsinki.fi/dali_server/) Compare proteins with low sequence similarity: similar structure implies homology -> same function Can help to find active sites ] --- ### Exercise 6: Study Protein-Ligand Interactions ### see []() --- ### On-Line References ### - Crystallography 101 (Bernhard Rupp): (http://www.ruppweb.org/Xray/101index.html) - Protein NMR, A Practical Guide (Vicky Higman) (http://www.protein-nmr.org.uk/) - Model validation course (http://xray.bmc.uu.se/gerard/embo2001/modval/index.html) - Assessing model quality (http://spdbv.vital-it.ch/TheMolecularLevel/ModQual/) - Lectures by Burkhard Rost on protein structure prediction (https://www.youtube.com/channel/UCU6j8BG4RbEtTgyIZJ6Vpow) ---"},{"title":"Introduction to Basic Bioinformatics Concepts, Databases and Tools","url":"/topics/basic-bioinformatics/slides/introduction.html","tags":[],"body":"### How to fill the slide decks? Please follow our [tutorial to learn how to fill the slides](/topics/contributing/tutorials/create-new-tutorial-slides/slides.html)"},{"title":"","url":"/search/search-results.html","tags":[],"body":""},{"title":"","url":"/search.html","tags":[],"body":""},{"title":"","url":"/search/search.html","tags":[],"body":""},{"title":"Introduction to Genome Assembly","url":"/topics/basic-bioinformatics/tutorials/general-introduction/slides.html","tags":[],"body":".enlarge120[ # ***De novo* Genome Assembly** ] #### With thanks to T Seemann, D Bulach, I Cooke and Simon Gladman --- .enlarge120[ # ***De novo* assembly** ] .pull-left[ **The process of reconstructing the original DNA sequence from the fragment reads alone.** * Instinctively like a jigsaw puzzle * Find reads which \"fit together\" (overlap) * Could be missing pieces (sequencing bias) * Some pieces will be dirty (sequencing errors) ] .pull-right[ ![](../../images/Humpty.jpg) ] --- # **Another View** ![](../../images/newspaper.png) --- # **Assembly: An Example** --- # **A small \"genome\"** ![](../../images/shakespear1.png) --- # **Shakespearomics** ![](../../images/shakespear2.png) --- # **Shakespearomics** ![](../../images/shakespear3.png) --- # **Shakespearomics** ![](../../images/shakespear4.png) --- # **So far, so good!** --- # **The Awful Truth** ![](../../images/notsimply.png) ## \"Genome assembly is impossible.\" - A/Prof. Mihai Pop --- .enlarge120[ # **Why is it so hard?** ] .pull-left[ * Millions of pieces * Much, much shorter than the genome * Lots of them look similar * Missing pieces * Some parts can't be sequenced easily * Dirty Pieces * Lots of errors in reads ] .pull-right[ ![](../../images/worlds_hardest.png) ] --- # **Assembly recipe** * Find all overlaps between reads * Hmm, sounds like a lot of work.. * Build a graph * A picture of the read connections * Simplify the graph * Sequencing errors will mess it up a lot * Traverse the graph * Trace a sensible path to produce a consensus --- ![](../../images/olc_pic.png) --- # **A more realistic graph** ![](../../images/real_graph.png) --- # .image-15[![](../../images/nofun.png)] **What ruins the graph?** * Read errors * Introduces false edges and nodes * Non haploid organisms * Heterozygosity causes lots of detours * Repeats * If they are longer than the read length * Causes nodes to be shared, locality confusion. --- # **Repeats** --- .enlarge120[ # **What is a repeat?** ] .pull-left[ #### ***A segment of DNA which occurs more than once in the genome sequence*** * Very common * Transposons (self replicating genes) * Satellites (repetitive adjacent patterns) * Gene duplications (paralogs) ] .pull-right[ ![](../../images/triplets.png) ] --- # **Effect on Assembly** ![](../../images/repeat_effect.png) --- .enlarge120[ # **The law of repeats** .image-15[![](../../images/repeatafterme.png)] ] ## **It is impossible to resolve repeats of length S unless you have reads longer than S** ## **It is impossible to resolve repeats of length S unless you have reads longer than S** --- # **Scaffolding** --- .enlarge120[ # **Beyond contigs** ] .pull-left[ Contig sizes are limited by: * the length of the repeats in your genome * Can't change this * the length (or \"span\") of the reads * Use long read technology * Use tricks with other technology ] --- .enlarge120[ # **Types of reads** ] .pull-left[.enlarge120[**Example fragment**]] .remark-code[.enlarge120[atcgtatgatcttgagattctctcttcccttatagctgctata]] .pull-left[.enlarge120[**\"Single-end\" read**]] .remark-code[.enlarge120[**atcgtatg**atcttgagattctctcttcccttatagctgctata]] sequence *one* end of the fragment .pull-left[.enlarge120[**\"Paired-end\" read**]] .remark-code[.enlarge120[**atcgtatg**atcttgagattctctcttcccttatag**ctgctata**]] sequence both ends of the same fragment **We can exploit this information!** --- .enlarge120[# **Scaffolding**] * **Paired end reads** * Known sequences at each end of fragment * Roughly known fragment length * **Most ends will occur in same contig** * **Some will occur in different contigs** * ***evidence that these contigs are linked*** --- .enlarge120[# **Contigs to Scaffolds**] ![](../../images/scaffolding.png) --- .enlarge120[# **Assessing assemblies**] * We desire * Total length similar to genome size * Fewer, larger contigs * Correct contigs * Metrics * No generally useful measure. (No real prior information) * Longest contigs, total base pairs in contigs, **N50**, ... --- .enlarge120[# **The \"N50\"**] .enlarge120[***The length of that contig from which 50% of the bases are in it and shorter contigs***] * Imagine we have 7 contigs with lengths: * 1, 1, 3, 5, 8, 12, 20 * Total * 1+1+3+5+8+12+20 = 50 * N50 is the \"halfway sum\" = 25 * 1+1+3+5+8+**12** = 30 (>25) so **N50 is 12** --- .enlarge120[# **2 levels of assembly**] * Draft assembly * Will contain a number of non-linked scaffolds with gaps of unknown sequence * Fairly easy to get to * Closed (finished) assembly * One sequence for each chromosome * Takes a **lot** more work * Small genomes are becoming easier with long read tech * Large genomes are the province of big consortia (e.g. Human Genome Consortium) --- .enlarge120[# **How do I do it?**] --- .enlarge120[ # **Example** * Culture your bacterium * Extract your genomic DNA * Send it to your sequencing centre for Illumina sequencing * 250bp paired end * Get back 2 files * .remark-code[MRSA_R1.fastq.gz] * .remark-code[MRSA_R2.fastq.gz] * ***Now what?*** ] --- .enlarge120[# **Assembly tools** * **Genome** * **Velvet, Velvet Optimizer, Spades,** Abyss, MIRA, Newbler, SGA, AllPaths, Ray, SOAPdenovo, ... * Meta-genome * Meta Velvet, SGA, custom scripts + above * Transcriptome * Trinity, Oases, Trans-abyss ***And many, many others...*** ] --- .enlarge120[ # **Assembly Exercise #1** * We will do a simple assembly using **Velvet** in **Galaxy** * We can do a number of different assemblies and compare some assembly metrics. ]"},{"title":"04 Data manipulation","url":"/topics/R/tutorials/Functions/tutorial.html","tags":[],"body":"Manipulation of variables General functions The big difference between R and other programming languages is that functions in R are designed to be applied to variables rather than to individual values to avoid loops e.g. if we want to log transform a whole dataset we can do this using a single operation: > v log10(v) [1] 0 1 2 3 4 The log10() function is written in such a way that it can be applied on a vector. This is true for all functions and operators in R: > v - 1 [1] 0 9 99 999 9999 R has built-in functions for virtually any standard mathematical task. Figure 1: Overview of built-in functions Arithmetic operators can be used on variables. Provided that the variables have the same dimensions, you can do element-wise addition, subtraction, multiplication and division of two vectors or tables. Element-wise means that the calculation is performed on the equivalent positions between the two variables: first element + first element, second element + second element etc. > v1 v2 z z [1] 5 7 9 If you perform operations on vectors with different lengths (not recommended) then the vector with the shorter length is recycled to the length of the longer vector so that the first element of the shorter vector is appended to the end of that vector (a way of faking that it is of equal length to the longer vector) and so forth. You will get a warning, but R does let you perform the operation: > x1 x2 x3 x3 [1] 4 6 6 hands_on Hands-on: Demo From the demo script run the Operations on variables section hands_on Hands-on: Exercise 13a Calculate log base2 of the activity in Drug_study Round the result to the nearest integer solution Solution log.act <- (log2(Drug_study$activity)) round(log.act) hands_on Hands-on: Extra exercise 13b Create vector v as the sum of newVector and threes using an arithmetic operator Print the content of v Do the same for newVector and vector x2 with elements 3,1 Join the elements of newVector and threes into 1 vector q solution Solution v <- newVector + threes v x2 <- c(3,1) newVector + x2 q <- c(newVector,threes) hands_on Hands-on: Exercise 13c Add a column called geneDensity to genomeSize containing the number of bp per gene for every organism Round the numbers to the nearest integer solution Solution dens.fl <- genomeSize$size / genomeSize$geneCount genomeSize$geneDensity <- round(dens.fl) Some functions only work on vectors. For instance sort() will sort data from smallest to largest (arguments allow other ordering) and order() returns the indices of the sorted elements: x [1] 1 3 11 1 7 sort(x) [1] 1 1 3 7 11 order(x) [1] 1 4 2 5 3 In the sorted vector the first element is also the first element of the original vector, the second element of the sorted vector has index 4 in the original vector etc. To sort a data frame use order() inside square brackets: mtcars[order(mtcars$mpg),] To sort on two columns (first on mpg, then on cyl): mtcars[order(mtcars$mpg,mtcars$wt),] To sort in descending order place a minus sign in front of the variable: mtcars[order(mtcars$mpg,-mtcars$wt),] Select the labels of a vector or table using names(). For tables rownames() and colnames() can access or set the either row or the column labels. Both functions will not work on vectors. The length() function retrieves the number of elements of a vector. Used on data frames it doesn’t throw an error but returns the number of columns instead. The same is true for match(x,y). It compares x and y and returns a vector with the same length as x containing: NA for elements of x that are not in y the index in y for elements in x that are in y On data frames it will not do an element-wise comparison but a column-wise comparison: match(D1,D2) will return a vector with length equal to the number of columns in D1 containing: NA for columns of D1 that are not in D2 the index in D2 for columns in D1 that are in D2 (so the complete column has to match, not the individual elements) Important is to see the difference between the + operator and sum(). The former works element-wise on two variables, the latter calculates the sum of all elements of one vector. There are also functions to be used only on tables, e.g. dim() returns how many rows and columns a table has, nrow() and ncol() will get these values individually t() transposes matrices (exchanges rows and columns), the output is a transposed matrix: the columns are the rows of the original matrix and vice versa Use merge() to join two data frames. Let?s say D1 has a column A with values. Data frame D2 has the same values stored in column A. Merge the two data frames on the basis of this common column: newD <- merge(D1,D2) If (some of) the values of the common column differ, merge() will ignore these values. Use argument all.x to add an extra row for every different value to the resulting data frame. All rows where the values of the two data frames don?t correspond, will be filled up with NA values. Most functions operate on numbers but there are also functions for manipulating text, e.g. paste(x,y,sep=\" \") concatenates two strings x and y (glues them together into one string) separating them by the character defined by sep. Arguments x and y can be strings but they can also be vectors. If they are vectors, they are concatenated element-wise to give a character vector result. Furthermore there are also functions specific for factors. For instance to select the names of the categories (levels) of a factor use levels() and table() to create a contingency table. table(cell_phone_data$own, cell_phone_data$grade) Figure 2: Example of a contingency table hands_on Hands-on: Exercise 13d You repeat the plant study experiment this time having the following numbers of plants developing lesions: 1, 6, 6, 5, 4 Add these data as a third column to the data frame Relabel columns to Day, Infected and Repeat Use paste() to add the word ?day? to the elements of the Day column. Look at the documentation first ! solution Solution Plant_study$repeated <- c(1,6,6,5,4) names(Plant_study) <- c(\"Day\",\"Infected\",\"Repeat\") ?paste Plant_study$Day <- paste(Plant_study$Day,\"day\",sep=\"\") question Question What will happen when you run this code ? paste(Plant_study[,\"Day\"],\"day\",sep=\"\") hands_on Hands-on: Exercise 13e Change the label of the second column of Drug_study to drug How many rows does Drug_study contain? Order the rows according to decreasing activity solution Solution colnames(Drug_study)[2] <- \"drug\" nrow(Drug_study) Drug_study[order(Drug_study$activity,decreasing=TRUE),] question Question What happens when you run this code ? colnames(Drug_study$ID) <- \"id\" question Question What happens when you run this code ? colnames(Drug_study[2]) <- \"blabla\" question Question What will happen when you run this code ? Drug_study[order(Drug_study$activity),\"ID\"] question Question What will happen when you run this code ? n <- order(Drug_study$activity,decreasing=TRUE) Drug_study[n,] hands_on Hands-on: Extra exercise 13f Sort the elements of z from smallest to largest Now use order(z). What’s the difference with the previous exercise? How many elements does z contain? solution Solution sort(z) order(z) length(z) hands_on Hands-on: Extra exercise 13g Add a new row to data frame ab containing values: 3,4,7 solution Solution d <- c(3,4,7) ab <- rbind(ab,d) hands_on Hands-on: Extra exercise 13h How many rows and columns are in the built-in data frame CO2 (data on CO2 uptake by plants) Use levels() to retrieve the names of the Treatment categories Create a contingency table with counts (number of plants) in every category of CO2 that is defined by Type and Treatment Use unique() to count how many plants were studied solution Solution dim(CO2) levels(CO2$Treatment) table(CO2$Type,CO2$Treatment) length(unique(CO2$Plant)) Functions helpful for working with large data sets Research in biology/medicine often generates very large data sets. When you work with very large data sets, it is often useful to show only a small part of the data set; head() shows the first 6 elements (vector) or rows (table) of a variable tail() prints the last 6 elements or rows hands_on Hands-on: Exercise 14a View the first 6 rows of the mtcars data frame Return TRUE if mtcars contains cars with 6 gears and FALSE if not How many cars with 3 gears are in mtcars? solution Solution head(mtcars) nrow(subset(mtcars,gear==6))!=0 nrow(subset(mtcars,gear==3)) Functions for finding indices of specific elements There are functions that help you locate specific values, the which functions: which.min(x) which.max(x) return the location (index) of the minimum, maximum or a specific value of a vector x. So max() will return the highest value in the data, which.max() will return the index of the highest value in the data. The argument of which() is a logical expression and which() will return the indices of the elements for which the logical expression is TRUE. x <- c(1,5,8,4,6) x # [1] 1 5 8 4 6 which(x == 5) # [1] 2 which(x != 5) # [1] 1 3 4 5 hands_on Hands-on: Exercise 15a Get the data of the patient with the highest activity in Drug_study solution Solution Drug_study[which.max(Drug_study$activity),] question Question What will happen when you run this code ? n <- which.max(Drug_study$activity) Drug_study[n,] hands_on Hands-on: Exercise 15b Get the index of the column called cyl in mtcars Create a data frame that contains the car with the lowest mpg for each category of cyl solution Solution which(names(mtcars)==\"cyl\") C4m <- mtcars[order(mtcars$cyl,mtcars$mpg),][1,] C6 <- subset(mtcars,cyl==6) C6m <- C6[which.min(C6$mpg),] C8m <- mtcars[order(-mtcars$cyl,mtcars$mpg),][1,] rbind(C4m,C6m,C8m) Checking and converting types of variables To check the data structure of an object you can use str() and the generic class() function: class(c(10,12,30)) # [1] \"numeric\" class(c(\"alana\",\"britt\",\"chris\")) # [1] \"character\" class(c(TRUE,TRUE,FALSE)) # [1] \"logical\" You can also use the specific is. functions e.g. is.numeric(), is.character(), is.Date(), is.vector(), is.matrix(), is.data.frame() etc. The is.na(x) function returns TRUE when an element of x is missing: x <- c(1,2,3,NA) is.na(x) # [1] FALSE FALSE FALSE TRUE To recode values to missing values you don?t need is.na(). Select the rows that contain the value you want to recode, e.g. 99, and change the value using an assignment: data$v1[data$v1==99] <- NA To exclude missing values you can use is.na() but there are alternatives. The problem with missing values is that when you apply arithmetic functions on variables that contain missing values they will return missing values and you will have no result. To circumvent this problem many functions have the na.rm argument. If you set na.rm=TRUE missing values are deleted before calculations are done. mean(x) # NA mean(x,na.rm=TRUE) # 2 The function na.omit() allows to create a new vector without missing values. If you apply this function on a data frame it will remove complete rows that contain one or more NA-values. newdata <- na.omit(x) You can convert the data type of an object by using the as. functions e.g. as.numeric(), as.character(), as.Date(), as.vector(), as.matrix(), as.data.frame() etc. hands_on Hands-on: Demo From the demo script run the Checking and converting data types section hands_on Hands-on: Exercise 16a We created a vector containing the days of the week and loaded this into a data frame called Plant_study. If we want to replace the days of the week by real dates, how should we proceed? To create a Date object in R: define the date as a string in the following format: 1970-01-01 transform the string into a date by using as.Date() Replace the days of the week by the dates of this week What type of data is Plant_study ? Convert Plant_study into a matrix called PS Did the conversion work? Look at the matrix to see if there is a problem. solution Solution Plant_study$Days <- as.Date(c(\"2019-01-09\",\"2019-01-10\",\"2019-01-11\",\"2019-01-12\",\"2019-01-13\")) class(Plant_study) PS <- as.matrix(Plant_study) PS hands_on Hands-on: Extra exercise 16b Check the data type of the second column of Drug_study. Retrieve the column using a comma. Convert the second column into a vector. What is different now? Look at the vector. solution Solution class(Drug_study[,2]) v <- as.vector(Drug_study[,2]) v hands_on Hands-on: Exercise 16c Instead of deleting missing values with na.omit() you can select the non-missing values. Create a vector with a missing value Multiply all elements with 2. What happens? Check if the 2nd element is missing Delete the missing value using is.na() and the strategy above solution Solution x <- c(1,2,3,NA) x*2 is.na(x[2]) x[!is.na(x)] hands_on Hands-on: Extra exercise 16d Check if z is a vector or a data frame Check if z contains numbers or characters Convert z into a matrix Convert the elements of z into characters solution Solution is.vector(z) is.data.frame(z) is.character(z) is.numeric(z) as.matrix(z) as.character(z) hands_on Hands-on: Extra exercise 16e Create a vector called words containing Hello, Hi Convert the words into numbers. What happens? solution Solution words <- c(\"Hello\",\"Hi\") as.numeric(words) R is smart enough to catch you if you try to do an illogical conversion, such as convert characters to numbers. It does the conversion but the data is converted to NA values."},{"title":"04 Predicting the effect of a mutation on a protein structure","url":"/topics/protein-structure-analysis/tutorials/mutate-structure/tutorial.html","tags":[],"body":"Introduction Mutations in proteins can have various origins. Natural occurring mutations are random and can have any kind of effect on the protein structure and/or function. Mutations can have no effect at all, be stabilizing of destabilizing. In the last two cases, these can lead to diseases. But we can also make mutations in the wet lab to study the effect of a single residue position on protein stability, interaction with a peptide ligand etc … Such site-directed mutagenesis in the wet lab is hard labour and costs money, I don’t have to explain that to you. So wouldn’t it be easier, cheaper and more rational if you could predict the effect of some mutations first with bioinformatics and then test the really interesting ones in the lab? FoldX is a molecular modeling tool that can quantitatively predict the change in free energy (kcal/mol) upon mutation. These values approach experimental determined values. FoldX is a non-interactive command line program. In other words, not user friendly. But the bright news is that I recently developed a YASARA plugin for FoldX, so that all predictions are just a few clicks away. And the nice thing is, it’s all free! In this tutorial, we will cover: P53 as example protein Get data What do FoldX energies mean? How to minimize the structure with FoldX How to analyze a mutation Study the effect of a second mutation P53 as example protein In this section we will let the FoldX plugin loose on some real world examples and give you step-by-step instructions on how to proceed and analyze the results. We will use the P53 tumor suppressor protein as our example molecule. In a first exercise you will make a point mutation with FoldX and determine if the mutation is stabilizing or destabilizing for the P53 structure. In a second exercise you will design a mutation in the P53 structure at the DNA binding interface and determine how the mutation affects the interaction energy of P53 with the DNA strand. Get data hands_on Hands-on: Data upload Download the file 2AC0.sce. What do FoldX energies mean? Before we start, some basic information about FoldX energies is necessary. First of all, FoldX energies are expressed in kcal/mol. The main focus of FoldX is the prediction of free energy changes, e.g. what happens to the free energy of the protein when we mutate an Asp to a Tyr? FoldX will then calculate the free energy of the wild type (WT) and the mutant (MT) and make the difference: ddG(change) = dG(MT) - dG(WT) FoldX is trained to make ddG(change) approach experimental values. It is important to realize that dG(WT) and dG(MT) are meaningless numbers as such. These do not correlate with experimental values. Only ddG(change) does. As a rule of thumb we use: ddG(change) > 0 : the mutation is destabilizing ddG(change) Load > YASARA Scene Figure 1: P53 monomer bound to DNA To Repair (or minimize) the structure with FoldX go to: Analyse > FoldX > Repair object Figure 2: Select the object for repairing And select the only object in the list. When the Repair is finished, the Repaired Object is placed in Object 2 (see top right corner) and superposed with the original Object 1. Take a look at the sidechains and see what FoldX has done while Repairing. If you feel the repair takes too long (more than 10 minutes) due to a slow computer, download and open this YASARA Scene with the Repaired Object. Because we will continue working with this Repaired Object, we can now hide the entire Object 1 by toggling the Visibility column in the top right corner head-up display (HUD). How to analyze a mutation FoldX has mutated the Ala to Trp and the structure with the Trp mutation has been loaded in the next Object (3) and is superposed with the wild type (WT, Object 2). We selected an option to show the VdW clashes in WT and mutant. The atoms that give rise to steric clashes are colored in red. Toggle the Visibility of Object 2 (WT) and Object 3 (mutant) and see how many clashes we introduced by mutating the Ala to Trp. Figure 3: Zoomed-in-view on the original Ala159, no Vander Waals clashes here Figure 4: Zoomed-in-view on the mutated Ala159Trp, lots of red Vander Waals clashes here question Questions Van der Waals clashes are red colored atoms. Do you see a difference around the mutation site between WT and mutant? solution Solution Toggle the Visibility of WT and mutant to see the differences. Open the Console by pressing the spacebar twice and see the free energy change of the mutation. Anything above a change of +0.5kcal/mol is already assumed to be destabilizing. In the console - to open press spacebar twice - we see an energy change of +29 kcal/mol. This is clearly a destabilizing mutation. Figure 5: Open the console to explore the situation. Study the effect of a second mutation Hide Object 3 by toggling its Visibility so that only Object 2 (the repaired WT) is visible. First turn on all atoms in the molecules G and H (DNA) again as you did previously, because the FoldX run has hidden it (it rearranged the view to show the VdW clashes). Show the sidechain of Arg273 of Object 2 by searching for it in the sequence selector, then right-click on it and go to: Show atoms > Sidechain and CA and zoom in on Arg273 Notice how the positively charged Arginine is making an electrostatic interaction with the negative phosphate from the DNA backbone. Figure 6: R273 makes an electrostatic interaction with the DNA phosphate groups. Let’s see what would happen to the interaction energy between the DNA and P53 when we mutate this Arginine to Alanine. Right-click on this Arg273 in the sequence selector and go to: FoldX > Mutate residue A number of menus is now presented and here is what you need to do in each menu: Select Calculate interaction energy change Select Ala ‘Move neighbours’ and ‘Show disrupted and new hydrogen bonds’ Don’t change any numerical options in the last menu Figure 7: View of the first options menu with 'Show new and disrupted hydrogen bonds' selected. question Questions What is the change in interaction energy is between P53 and DNA chain G upon mutation? And what is the reason? Why doesn’t the mutation affect the interaction with DNA chain H? solution Solution Toggle the Visibility between this mutant and the WT structure and see how the hydrogen bonding changes and check the output in the Console. Figure 8: Change in interaction energy We see that the mutation decreases the interaction with DNA strand G by approximately 1 kcal/mol since we lost 1 hydrogen bond. TODO Conclusion Instead of DNA-protein, FoldX can of course also calculate interaction energy changes in protein-protein or peptide-protein complexes."},{"title":"02 Visualize a structure","url":"/topics/protein-structure-analysis/tutorials/visualise-structures/tutorial.html","tags":[],"body":"General introduction about the topic and then an introduction of the tutorial (the questions and the objectives). It is nice also to have a scheme to sum up the pipeline used during the tutorial. The idea is to give to trainees insight into the content of the tutorial and the (theoretical and technical) key concepts they will learn. –> Agenda In this tutorial, we will cover: Install Python and PovRay Tutorial movie Scene styles Showing and hiding residues Showing and hiding secondary structure Labels Colors Saving all the beautiful work Creating high quality images Distances Hydrogen bonds Surfaces Molecular graphics exercise More coloring Get data Title of the section usually corresponding to a big step in the analysis Sub-step with My Tool Re-arrange Install Python and PovRay Python and PovRay should be installed already, so you can skip this part. The programming language Python must be installed to use some very useful YASARA features. Simply start YASARA as administrator. Right click the YASARA icon on the desktop and choose “Run as administrator”. Once the program is opened, click Help > Install program > Python PovRay is used to make high quality publication-ready images and should be downloaded first with: Help > Install program > PovRay Tutorial movie Play the movie “Working with YASARA”: Help > Play help movie > General: Working with YASARA Scene styles Open the PDB with code 1TRZ in YASARA. File > Load > PDB file from Internet If this option is not there, it means you haven’t installed Python yet. Please check above. The molecule will be loaded and presented in the ball style. Different scene styles exist to rapidly change the view: F1: Ball F2: Ball & Stick F3: Stick F4: C-alpha F5: Tube F6: Ribbon F7: Cartoon F8: Toggle sidechains on/off (press multiple times and see what happens) Be careful! If you have just made a nice close-up of e.g. an active site where you show some residues and hide others, and put some atoms in balls while others are in sticks, you will lose everything when you press one of the F-keys!!! The F-keys change the viewing style without asking. Try all the different scene styles! Showing and hiding residues The function keys F1-F3 show all atoms and residues by default. The keys F4-F7 do not explicitly show atoms and residues but are merely a impressionistic representation of the structure. The F8 keys does, to a certain extent, show atoms, but only of side chains, not main chain atoms. Mostly to do structure analysis, we want to show only the most interesting residues, the ones we want to analyze, and hide all the others. The structure of insulin was crystallized together with some water molecules. In many cases, it is no problem to permanently delete those waters. To visualize the waters, select an atom view such as F1, F2 or F3. See the red water (oxygen) atoms floating around the surface? Edit > Delete > Waters Then select the base scene style without any explicit atoms, e.g. tube style (F5). Press F5. This is our representation of the backbone. There are several ways to show the residues of interest: From the menu View > Show atoms in > Residue Select Cys7 from Molecule A and press OK From the sequence selector Figure 1: Seqselector.png Hover the mouse on the bottom of the screen, you will see the sequence selector opening. Open it permanently by pressing the blue nailpin on the left side of it. Search for Cys7 from Molecule B, right-click and select: Show > Residue Now show the atoms of His5 in Molecule B using a method of choice. And now that we’re on it, what is special about the two cysteines we just visualized? Hiding individual atoms or residues works in the same way as showing them, only now you should go to Hide atoms in the menus. Showing and hiding secondary structure Most published molecular images show a detailed active site and all the rest is hidden for clarity. From the previous exercise we show the atoms of 3 residues (let’s assume this is our active site). Now secondary structure of the rest of the molecule is also still visible. To hide all that, we do not have to hide atoms, but hide the secondary structure (the F5 tube view) from the rest of the structure. Atoms and residues in YASARA are not the same as the term ‘secondary structure’. Atoms and residues are balls and sticks, ‘secondary structure’ is an artistic impression of the structure (beta sheet arrows, helix ribbons, …). If you get this concept, you are a YASARA master. So let’s hide many of the secondary structure, but keep just a few stretches around our active site. Our active site is Cys7 (A), Cys7 (B) and His 5 (B). This can be done in several ways. Since we would have to hide almost everything, I propose to hide first everything and then show again those stretches that we want. But if you have a better idea, I would like to hear it. Hide all secondary structure: View > Hide secondary structure of > All Then show stretches of residues 2-10 in Mol B and residues 4-10 in Mol A in tube view as: View > Show secondary structure > Tube through > Residue Then select the correct stretches of residues by keeping the CTRL key pressed to select multiple residues. There are still some metal-bound histidines flying around that weren’t hidden because they are metal bound (a YASARA specific thing). Hide those histidines by clicking on one of the sidechain atoms, then right-click and select: Hide atoms > Residue The nasty dative bonds and metals can be removed simply by deleting all of them: Edit > Delete > Residue > Name In the name column select all the metals and ions you can find. Et voilà, a publication ready image! Figure 2: Insuline Labels You can put labels on the residues you want to highlight by going to the main menu or selecting an atom from a residue and right-click. In the latter case you select: Label > Residue Note that residue name and residue number is automatically selected. Change the height to 0.5 or so and select a nice color for the label. Presto! Colors You can color on all levels: atoms, residues, molecules and objects. So be careful, if you color a residue, all of its atoms will get that color. If you color a molecule, all atoms in that molecule will get that color. Let’s color the secondary structure (the backbone in our case) of our active site in orange. But the sidechains should keep their Element colors. So we shouldn’t color entire residues, but only a selected atom set. Therefore our selection will be at the atom level, not the residue level. Go to: View > Color > Atom > Belongs to or has > Backbone Then select the orange color (color code 150) and select ‘Apply unique color’. Beautiful, isn’t it? Saving all the beautiful work It would be a pitty that you spent hours creating fancy molecular graphics for that next Nature paper while you can’t continue on the work the next day. That’s why YASARA can save the entire Scene including orientations, colors, views, everything. To save the current scene, go to: File > Save as > YASARA Scene Choose a filename such as MyInsulin.sce To load the work again in YASARA go to: File > Load > YASARA Scene Careful: loading a Scene will erase everything else! Creating high quality images To save the current view to a high quality publication ready image file, go to: File > Save as > Ray-traced hires screenshot This requires that the PovRay program has been installed. See the first item on this page. Usually, you prefer to have a transparent background, so check the respective box. Distances Distances between atoms are calculated as follows: select the first atom keep CTRL pressed and select the second atom. left of the screen indicates the ‘Marked Distance’ in Angstrom. What is the distance between the C-alpha (CA) atoms of Tyr19 and Leu16? To solve the question you need to select a view that shows you atoms including C-alphas. Possible views or scene styles that show these atoms can be F1 (ball), F2 (stick), F3 (ball\\&stick) and F4 (C-alpha). The views F5-F8 won’t show you any CA’s explicitly. Try it. So you've probably noticed that pressing the CTRL button allows you to select multiple atoms. This is important for the next exercise. Hydrogen bonds To show hydrogen bonds, YASARA needs the actual hydrogens to be present. In NMR structures these are normally there. But in X-Ray structures hydrogens are missing. Luckily YASARA can add the hydrogens for you. Select tube view (F5) and toggle on the sidechains with F8. Add hydrogens with: Edit > Add > Hydrogens to all Then show the hydrogen-bonds: View > Show interactions > Hydrogen bonds of> All > OK If the view is to chaotic for you, toggle off the sidechains with F8 (press untill the sidechains are hidden). Do you see the typical helix and beta sheet pattern? Arg22 from Molecule/Chain B is making an hydrogen bonded electrostatic interaction (salt bridge) with another residue. Which residue? To remove the hydrogen bonds, you have multiple choices: View > Hide hydrogen bonds of > All or just delete all hydrogens (this will also delete all hydrogen bonds): Edit > Delete > Hydrogens Surfaces It can be very useful and informative to show the molecular surface of a protein. you can visualize cavities, ligand binding sites, etc … To show the molecular surface of one monomer of dimeric insulin, go to: View > Show surface of > Molecule Select in the Name column A and B (these are the two chains in 1 subunit). Press Continue with surface color and make sure Alpha is Any number lower than 100 will create transparency in the surface (could be nice as well). Molecular graphics exercise Try to reproduce the following image of the 1TRZ insulin structure (hints below): image:insulin.png Hints: choose the proper secondary structure scene style (F6 was used here) find the correct orientation first color all backbone atoms in gray find the residue numbers of the 2 colored helices color those residues magenta show the sidechain atoms and the CA of the two histidines and the glutamate color the sidechain atoms of all residues in the Element color label the histidines and the glutamate if you need some help how to change the parameters for the label, please have a look at Help -> Show user manual and search in Commands / Index More coloring Download GroEL via PDB code 1WE3 in YASARA. Try to reproduce (approximately) the following image (hints below): image:groel.png Hints: load the PDB as File > Load > PDB file from internet zoom out and find the correct orientation delete the ADP, DMS and Mg molecules (are treated as residues in YASARA). So Edit > Delete > Residue > Adp … color by molecule (every molecule will get another color) and color by gradient (now you need to specify 2 colors, the begin and end color). choose a first color (eg. color with code 0) choose a second color (eg. color with code 300, so you go over the entire color wheel spectrum) More exercises can be found on the basic bioinformatics exercises page. Get data hands_on Hands-on: Data upload Create a new history for this tutorial Import the files from Zenodo or from the shared data library TODO: Add the files by the ones on Zenodo here (if not added) TODO: Remove the useless files (if added) tip Tip: Importing data via links Copy the link location Open the Galaxy Upload Manager (galaxy-upload on the top-right of the tool panel) Select Paste/Fetch Data Paste the link into the text field Press Start Close the window By default, Galaxy uses the URL as the name, so rename the files with a more useful name. tip Tip: Importing data from a data library As an alternative to uploading the data from a URL or your computer, the files may also have been made available from a shared data library: Go into Shared data (top panel) then Data libraries Find the correct folder (ask your instructor) Select the desired files Click on the To History button near the top and select as Datasets from the dropdown menu In the pop-up window, select the history you want to import the files to (or create a new one) Click on Import Rename the datasets Check that the datatype tip Tip: Changing the datatype Click on the galaxy-pencil pencil icon for the dataset to edit its attributes In the central panel, click on the galaxy-chart-select-data Datatypes tab on the top Select datatypes Click the Change datatype button Add to each database a tag corresponding to … tip Tip: Adding a tag Click on the dataset Click on galaxy-tags Edit dataset tags Add a tag starting with # Tags starting with # will be automatically propagated to the outputs of tools using this dataset. Check that the tag is appearing below the dataset name Title of the section usually corresponding to a big step in the analysis It comes first a description of the step: some background and some theory. Some image can be added there to support the theory explanation: Figure 3: Legend of the image The idea is to keep the theory description before quite simple to focus more on the practical part. TODO: Consider adding a detail box to expand the theory details More details about the theory But to describe more details, it is possible to use the detail boxes which are expandable A big step can have several subsections or sub steps: Sub-step with My Tool hands_on Hands-on: Task description My Tool tool with the following parameters: param-file “Input file”: File “Parameter”: a value TODO: Check parameter descriptions TODO: Consider adding a comment or tip box comment Comment A comment about the tool or something else. This box can also be in the main text TODO: Consider adding a question to test the learners understanding of the previous exercise question Questions Question1? Question2? solution Solution Answer for question1 Answer for question2 Re-arrange To create the template, each step of the workflow had its own subsection. TODO: Re-arrange the generated subsections into sections or other subsections. Consider merging some hands-on boxes to have a meaningful flow of the analyses Conclusion Sum up the tutorial and the key takeaways here. We encourage adding an overview image of the pipeline used."},{"title":"05 Predicting a protein structure - homology modelling","url":"/topics/protein-structure-analysis/tutorials/homology-modeling/tutorial.html","tags":[],"body":"Introduction The goal of homology modeling is to predict the 3D structure of a protein that comes close to what would be achieved experimentally with X-Ray experiments. Main principles of homology modeling We predict the structure of a protein sequence on the basis of the structure of another protein with a similar sequence (the template) If the sequences are similar, the structures will have a similar fold Structure is more conserved than sequence Agenda In this tutorial, we will cover: Main ingredients for homology modelling The sequence Searching for the template structure Aligning target and template sequence and template selection Building the homology model with Swiss Model Main ingredients for homology modelling The sequence Last week my colleague sequenced a plant protein. He is not a bioinformatician. Yet, he would like to know what the structure might look like to do some rounds of rational mutagenesis. Let’s try to address the problem for him. He came up with this sequence: SVCCPSLVARTNYNVCRLPGTEAALCATFTGCIIIPGATCGGDYAN Searching for the template structure Actually, the first step is to check whether the PDB already contains the structure of this sequence. That would be easy so we don’t have to model anything. We will use Blast again to search with the sequence. TODO: add hands-on area with Blast search on PDB page hands_on Hands-on: Data upload Create a new history for this tutorial Import the files from Zenodo or from the shared data library TODO: Add the files by the ones on Zenodo here (if not added) TODO: Remove the useless files (if added) tip Tip: Importing data via links Copy the link location Open the Galaxy Upload Manager (galaxy-upload on the top-right of the tool panel) Select Paste/Fetch Data Paste the link into the text field Press Start Close the window By default, Galaxy uses the URL as the name, so rename the files with a more useful name. tip Tip: Importing data from a data library As an alternative to uploading the data from a URL or your computer, the files may also have been made available from a shared data library: Go into Shared data (top panel) then Data libraries Find the correct folder (ask your instructor) Select the desired files Click on the To History button near the top and select as Datasets from the dropdown menu In the pop-up window, select the history you want to import the files to (or create a new one) Click on Import Rename the datasets Check that the datatype tip Tip: Changing the datatype Click on the galaxy-pencil pencil icon for the dataset to edit its attributes In the central panel, click on the galaxy-chart-select-data Datatypes tab on the top Select datatypes Click the Change datatype button Add to each database a tag corresponding to … tip Tip: Adding a tag Click on the dataset Click on galaxy-tags Edit dataset tags Add a tag starting with # Tags starting with # will be automatically propagated to the outputs of tools using this dataset. Check that the tag is appearing below the dataset name A suitable template structure to make a high quality model should have following properties: The highest possible sequence identity from all structures in the PDB when aligned to the target sequence A good resolution (and R-factor): if many identical template structures exist with the same sequence, filter by resolution Is solved by X-RAY, not NMR hands_on Hands-on: Data upload Create a new history for this tutorial Import the files from Zenodo or from the shared data library TODO: Add the files by the ones on Zenodo here (if not added) TODO: Remove the useless files (if added) tip Tip: Importing data via links Copy the link location Open the Galaxy Upload Manager (galaxy-upload on the top-right of the tool panel) Select Paste/Fetch Data Paste the link into the text field Press Start Close the window By default, Galaxy uses the URL as the name, so rename the files with a more useful name. tip Tip: Importing data from a data library As an alternative to uploading the data from a URL or your computer, the files may also have been made available from a shared data library: Go into Shared data (top panel) then Data libraries Find the correct folder (ask your instructor) Select the desired files Click on the To History button near the top and select as Datasets from the dropdown menu In the pop-up window, select the history you want to import the files to (or create a new one) Click on Import Rename the datasets Check that the datatype tip Tip: Changing the datatype Click on the galaxy-pencil pencil icon for the dataset to edit its attributes In the central panel, click on the galaxy-chart-select-data Datatypes tab on the top Select datatypes Click the Change datatype button Add to each database a tag corresponding to … tip Tip: Adding a tag Click on the dataset Click on galaxy-tags Edit dataset tags Add a tag starting with # Tags starting with # will be automatically propagated to the outputs of tools using this dataset. Check that the tag is appearing below the dataset name question Questions Is there a difference in the number of identities, positives and gaps between the two remaining x-ray structures? What is the PDB ID with the highest resolution, does not have insertions or deletions and should thus be the better template structure? solution Solution TODO TODO Aligning target and template sequence and template selection The alignment is the most crucial part of homology modeling. We will not explain what an alignment is and how you make it, this should be known. In an alignment, we put homologous sequences on top of each other in a text file. The point is that amino acids that are on top of each other in the same column are assumed to have the equivalent position in the 3D structure. So if the template sequence has an Ala at position 3, where the target sequence has a Phe, homology modelling tools will use the backbone of the template structure and replace the sidechain at position 3 from Ala to Phe. hands_on Hands-on: Data upload Create a new history for this tutorial Import the files from Zenodo or from the shared data library TODO: Add the files by the ones on Zenodo here (if not added) TODO: Remove the useless files (if added) tip Tip: Importing data via links Copy the link location Open the Galaxy Upload Manager (galaxy-upload on the top-right of the tool panel) Select Paste/Fetch Data Paste the link into the text field Press Start Close the window By default, Galaxy uses the URL as the name, so rename the files with a more useful name. tip Tip: Importing data from a data library As an alternative to uploading the data from a URL or your computer, the files may also have been made available from a shared data library: Go into Shared data (top panel) then Data libraries Find the correct folder (ask your instructor) Select the desired files Click on the To History button near the top and select as Datasets from the dropdown menu In the pop-up window, select the history you want to import the files to (or create a new one) Click on Import Rename the datasets Check that the datatype tip Tip: Changing the datatype Click on the galaxy-pencil pencil icon for the dataset to edit its attributes In the central panel, click on the galaxy-chart-select-data Datatypes tab on the top Select datatypes Click the Change datatype button Add to each database a tag corresponding to … tip Tip: Adding a tag Click on the dataset Click on galaxy-tags Edit dataset tags Add a tag starting with # Tags starting with # will be automatically propagated to the outputs of tools using this dataset. Check that the tag is appearing below the dataset name To extract the sequence from the template structure, display the FASTA sequence of the template structure and paste it into the text editor on the first line. Figure 1: Legend of the image The idea is to keep the theory description before quite simple to focus more on the practical part. Figure 2: Legend of the image Building the homology model with Swiss Model Our current request for homology modelling is a rather safe one, so we can use an automatic server for homology modelling. There are many automatic tools available and many of them compete in regular competitions like lastly, the 12th Community Wide Experiment on the Critical Assessment of Techniques for Protein Structure Prediction (CASP12) - [1]. In our example, we take the Swiss Model server. SWISS-MODEL is a fully automated protein structure homology-modelling server, accessible via the ExPASy web server, or from the program DeepView (Swiss Pdb-Viewer). The purpose of this server is to make Protein Modelling accessible to all biochemists and molecular biologists worldwide. hands_on Hands-on: Template selection step with Swiss Model Browse to the Swiss Model server On the first page, paste the sequence of our unknown protein in the field ‘Target Sequence’ and give the project a name. Figure 3: Start page of Swiss Model Click ‘Search templates’ to initiate the first step. Thereafter, the server identifies structural template(s) and gives an overview list of hits which you can select the templates from. question Question Which of the 10 (at the time of writing) possible template structures would you select as template for the model building process? solution Solution TODO: add link with 10 template We suggest as template 1jxx.1.A given that it is an X-ray structure with high resolution and a very high sequence identity (X-ray, 0.9 Å, 78.26 %). hands_on Hands-on: Model Building Step and Visualisation Once you have selected the template, hit ‘Build Model’ to start the homology modelling procedure. The server will alignment of target sequence and template structure(s), build a model and evaluate it. These steps require specialized software and integrate up-to-date protein sequence and structure databases. Each of the above steps can be repeated interactively until a satisfying modelling result is achieved. Once the model has been built, you can download it. If the Swiss Model server is too busy at the moment you execute the request, you can download the model from here. Load the created model into YASARA. Perform a structural superposition with your reference e.g. 1CRN try to detect the differences through manipulating the visualisations. Figure 4: Superposition of 1CRN with the obtained model Conclusion Homology modelling evolved over the years and many online tools for homology modelling are available. You have used the Swiss Model service with a reasonable simple modelling request. Often, in research projects, homology modelling can be rather difficult and needs expert knowledge depending on the actual situation (sequence conservation, available templates, etc.)."},{"title":"03 Compare Structures","url":"/topics/protein-structure-analysis/tutorials/compare-structures/tutorial.html","tags":[],"body":"Agenda In this tutorial, we will cover: Structural comparison and RMDD Superimposing multiple structures using YASARA Structural comparison and RMDD We compare structures by superposing (or structurally align) them on top of each other. That is, we superpose structurally equivalent atoms. For now, we will only superpose CA atoms, so backbones. B ut Yasara also can superpose on any type of atom you want. You always need to specify: source object(s): the structure(s) that needs to be rotated and translated to superpose on anoth er structure target object: the structure to superpose on An optimal superposition is found when the root-mean-square deviation (RMSD) is at a minimum. The R MSD is given as: Figure 1: calculation of RMSD where R is the distance between two structurally equivalent atom pairs (CA in our case) and n is th e total number of atom pairs.Get data hands_on Hands-on: Data download Download the following adapted PDB files from Zenodo 1DKX_1.pdb 1DKY_1.pdb 1DKZ_1.pdb 3DPO_1.pdb 3DPP_1.pdb Superimposing multiple structures using YASARA Now load all of them in YASARA: File > Load > PDB File and select the CA (C-alpha) view (F4) and superpose with the MUSTANG algorithm: Analyze > Align > Objects with MUSTANG In the first window you have to select the source objects that will be repositioned. Select Objects 2 till 5. In the second window you select the target Object to superpose on. That would then be the first object. Notice that YASARA prints the RMSD of every superposition in the lower Console. Open the Console by pressing the spacebar once or twice to extend it. Color the atoms by their B-factor: View > Color > Atom > Belongs to or has > All Then choose BFactor in the next window and press 'Apply unique color'. High BFactors are yellow, low BFactors are blue. question Questions Question: Do you see a correlation between the BFactors and the variability in the structure? solution Solution A Header Yes, add explanation here TODO: add image Conclusion Superimposition of related structures is a very efficient approach to spot similarities and differences of structutally related proteins."},{"title":"06 Protein-ligand interaction - from small molecule to protein","url":"/topics/protein-structure-analysis/tutorials/protein-ligand-interaction/tutorial.html","tags":[],"body":"Introduction The goal of this exercise is appreciate how protein interactions can be studied through visual inspection and other software tools. Protein interactions can be classified into different groups regarding the molecular properties and functions of the interacting partners. (These groups are intertwined in several cases.) Some examples include: the interactions of proteins with other proteins, small molecules, carbohydrates, lipids or nucleic acids; Receptor-ligand interactions; Antigen-antibody interactions; Enzymatic interactions, enzyme-inhibitor interactions. Agenda In this tutorial, we will cover: Exploring the structure of a nanobody-stabilized active state of the β2 adrenoceptor - the ligand Exploring the structure of a nanobody-stabilized active state of the β2 adrenoceptor - the nanobody Use the tool FoldX tool AnalyseComplex Comparing the active and the inactive conformation of the β2 adrenoceptor Exploring the structure of a nanobody-stabilized active state of the β2 adrenoceptor - the ligand We will start with exploring one crystal structure of the β2 adrenoceptor. Together with the Steyaert lab from VIB, Kobilka published several crystal structures of the β2 adrenoceptor in its various activation states (Rasmussen et al. Nature 2011, 477) hands_on Get the structure Download the crystal structure 3P0G from the PDB into YASARA. File - Load - PDB file from internet As you can immediately appreciate, it is a bigger crystal structure with more than one molecule. question Questions How many molecules are present in the crystallized structures? And how many chain identifiers are used? solution Solution Answer for question1 Answer for question2 Some software routines need seperate chain identifiers for molecular entities to work correctly, so we suggest to rename the small molecule to chain L. hands_on Activate the Head-up display Select Rename Enter ‘L’ to proceed with the renaming. We first have a look whether we can find out if there are specific interactions of the small molecule ligand with the adrenoreceptor. In order to do so, we first have to add Hydrogens to all present molecules. hands_on Edit - Add - hydrogens to : All Change the display of the ligand to Sticks Select the amino acids of the binding pocket i.e. a sphere of 10 Angstrom around the ligand Select – in sphere around – Residue and drag with the mouse until the display says 10 Å View – show interactions – hydrogen bonds of - Residues select ‘Selected’ in the panel Belongs to or has and press OK in the subsequent window question Questions Between which amino acids and the ligand do you see hydrogen bonds? solution Solution Answer for question1 Given that hydrogen bonding is dependent on the definition of a hydrogen bond in the program, it is not a bad idea to use other tools to compare the analysis. There are many options to do this online if you look at published crystal structures. Next to the tools which are directly linked out from the web site of the crystal structure at the PDB database you can use the ProteinPlus server Go to the web site of ProteinPlus and enter the PDB code 3P0G into the search box. After clicking on Go, you should be presented with on overview of tools the ProteinPlus server provides. We do not go into great detail on all the tools but only mention PoseView. With this tool, you can prepare an automatic sketch of the small molecule-protein interactions. Figure 1: Overview of 3P0G Figure 2: Zoom on ligand co-crystallized with 3P0G question Questions Between which amino acids and the ligand do you see hydrogen bonds? solution Solution According to PoseView, between which amino acids and the ligand do you see hydrogen bonds? What other interactions are presented in the sketch? Inspect the visualisation in Yasara: Do you see the interactions in Yasara as well? Exploring the structure of a nanobody-stabilized active state of the β2 adrenoceptor - the nanobody In order to estimate the binding energy between the nanobody and the β2 adrenoceptor, we can use the FoldX tool AnalyseComplex. It is recommended to calculate these binding energies on energy-minimized structures. To illustrate the effect of the energy minimization, we compare the interaction energy of the current crystal structure and its minimized structure. Use the tool FoldX tool AnalyseComplex hands_on Given that energy-minimization takes a while for this rather large complex, please download the Yasara scene here Calculate the interaction energies between the chain A and B of the object 3P0G and the RepairObj1, respectively. Analyze - FoldX - Interaction energy of molecules question Questions What is the dG in the two cases? Any idea why the difference is rather hugh? solution Solution Answer for question1 Answer for question2 This command also creates a list of residues forming the interface of the two proteins. Hit space to see the list of residues in the interface. Tip: This list can also be useful if you want to make visualisations of the interaction site. Comparing the active and the inactive conformation of the β2 adrenoceptor In case, there is still time, I would recommend to try to use some of your capabilities you learned today and create a superposition of the inactive and active conformation of the β2 adrenoceptor. We take one of crystal structures which are available: 3SN6 TODO: add remark that we use the first chain A from 3P0G as target File - Load - PDB file from Internet You will be kind of overwhelmed once the structure is loaded into Yasara. In order to get a first quick overview, click on the ‘Center’ buttom in the menu of Yasara (5th buttom from the left). Then, it is time to look at the PDB entry of 3SN6 in the PDB database to have a first idea on what molecules are in the PDB file. As you see on the website 3SN6, the chain R consists of 2 molecules, the β2 adrenoceptor and lysozyme. In the corresponding article, it is stated that ‘the unstructured amino terminus of the β2AR is replaced with T4 lysozyme (T4L)’. Since this is an extra molecule in the crystal structure which disturbes our view, we will delete it. After the manipulation, the overall picture should look roughly like this. Figure 3: Overview of 3SN6 without lysozyme In the following step, we superimpose only the receptors. The rest of the structures will move along. Analyze - Align - Pairwise, based on structure - Molecules with MUSTANG First, select the first chain R from 3SN6 and second, the first chain A from 3P0G as target Figure 4: Legend of the image Conclusion Sum up the tutorial and the key takeaways here. We encourage adding an overview image of the pipeline used."},{"title":"01 Exploring the Protein Data Bank exercises","url":"/topics/protein-structure-analysis/tutorials/explore-pdb/tutorial.html","tags":[],"body":"Agenda In this tutorial, we will deal with: Search for a structure The PDB file Introduction Downloading the structure Search for a structure Via UniProt The way of searching for a specific protein structure depends on the data you already have. You might already have the PDB ID (a unique identifier), that’s an easy one. But mostly you have the protein name or you just have a sequence. In the last cases I recommend to start from the UniProt website at http://www.uniprot.org, which is the best annotated protein database in the world. Our first model protein will be the molecular chaperone DnaK from E. coli. Below is an image of the UniProt search box where you can start your search for proteins. Figure 1: Search box hands_on Explore a PDB structure on the Uniprot web site Go to the UniProt website and search for the DnaK protein The UniProt search engine returns a list of DnaK protein sequences from a variety of organisms. An entry with accession code P0A6Y8 and entry name DNAK_ECOLI should be near the top of this list. Click on the accession code (column Entry) to view the protein page of this DnaK from the model organism Escherichia coli. Click on Structure in the left-side menu and then look at the 3D structure databases table. question Guidelines which PDB structures to select Which structures (give the 4-character PDB ID) of the C-terminal domain of DnaK should preferentially be use for analysis and why? solution Solution Usually, the recommended selection criteria are using an X-ray structure with low resolution and low Rfree factor. Furthermore, the PDB database has pre-calculated a validation report for all of the structures. As an example, have a look at https://www.ebi.ac.uk/pdbe/entry/pdb/4EZX under the section ‘Experiments and Validation’. For many PDB structures, there is also a re-done structure available with a vast amount of informaton on the quality of the X-ray structure and suggested ‘better’ models e.g. (https://pdb-redo.eu/db/4ezx). In our case, we could opt for the structures 1DKX and 4EZX. This is a difficult example since there are so many high resolution structures available. So, it is recommended to study the articles and compare the available structures to find your favorite structure for further analysis. Via the Protein Data Bank by PDB ID You can find structural information directly at the PDB database. The web site of the PDB consortium is located at http://www.wwpdb.org. This web site provides links to all members of the PDB (left side). It is a question of taste which resource you start off with. For X-ray structures, it is currently PDBe, RCSB PDB, PDBj. For NMR structres, you find the BMRB. In today’s course, we focus on the PDB resources only. Below is an image of the RCSB search box http://www.rcsb.org/pdb/home/home.do where you can start your search for structures. The PDB file with ID 1DKX contains the atomic coordinates of the molecular chaperone (DnaK) from E. coli. hands_on Search a structure on the RCSB web site Go to the PDB website and type 1DKX in the search box This will lead you to the same page we got earlier through UniProt. Via the Protein Data Bank by sequence In lots of cases we only have a sequence of which we would like to find out if there is structural information. The PDB can be searched using a sequence as input. Here is the sequence of the C-terminal substrate binding domain of DnaK: DVKDVLLLDVTPLSLGIETMGGVMTTLIAKNTTIPTKHSQVFSTAEDNQSAVTIHVLQGE RKRAADNKSLGQFNLDGINPAPRGMPQIEVTFDIDADGILHVSAKDKNSGKEQKITIKAS SGLNEDEIQKMVRDAEANAEADRKFEELVQTRNQGDHLLHSTRKQVEEAGDKLPADDKTA IESALTALETALKGEDKAAIEAKMQELAQVSQKLMEIAQQQHAQQQTAGADASANNAKDD DVVDAEFEEVKDKK The PDB allows sequence searches through the same search box we used before. There is also an Advanced Search section, with a Blast/Fasta option in the Sequence Features section. hands_on Hands-on: BLAST search for PDB structure Go to the Advanced Search section Please select ‘Sequence BLAST/PSI-BLAST’ in the Query type drop down. This method allows you to change some parameters for the search. Copy and paste the sequence in the ‘‘Sequence’’ field Press ‘‘Submit query’’. You should see the same structures popping up as you saw in the UniProt page of DnaK. The PDB file Introduction A PDB (Protein Data Bank) file is a plain text file that contains the atom coordinates of a solved 3D structure of a protein or even DNA. Such coordinate files can be obtained at the Protein Data Bank at http://www.rcsb.org/pdb. Each PDB file has a unique identifier (ID) consisting of 4 characters, the first one is always a number. Note: It has been announced that the 4 character code will change in the future https://www.wwpdb.org/news/news?year=2017\\#5910c8d8d3b1d333029d4ea8. The PDB file with ID 1DKX contains the atomic coordinates of the molecular chaperone (DnaK) from E coli. hands_on Hands-on: BLAST search for PDB structure Go to the PDB website at http://www.rcsb.org/pdb Type 1DKX in the search and try to answer the following questions. question Questions How many molecules were solved in this PDB file? What kind of molecules are these (proteins, peptides, DNA, …)? Does the structure represent the full protein? If not, how many residues are missing? Hint: Click on the UniProt KB link in the Sequence tab to see the full sequence. Was this structure solved by X-Ray or NMR? What is the atomic resolution and R-factor? solution Solution Two, called polymers or chains: they are polypeptides To answer this question you can go to the sequence tab at the top: Summary: a large chunk of the N-terminus is missing from the structure, the C-terminus is virtually complete. X-RAY diffraction, as shown by Experimental Details Atomic resolution: 2.00 Ångstrom and R-factor of 0.206 Downloading the structure The file that holds the 3D coordinates can be downloaded by clicking on Download files in the top right corner and then choosing PDB file (text). For convenience, save this file on your desktop. The filename is the 4-character unique PDB ID. hands_on Hands-on: Open downloaded PDB file in an editor Open this file with a text editor, e.g. WordPad is an excellent tool for that. Do you see the different sections in the PDB file? Analyse some ATOM lines and try to explain what kind of data is in each column. Additional exercises on searching PDB can be found on the basic bioinformatics exercises page."},{"title":"Share: Data selection and preservation","url":"/topics/data-management-plans/tutorials/share-preserve/tutorial.html","tags":[],"body":"Introduction Introduction to data selection and preservation Research should be transparent and you should always be able to revert back to your data if necessary and be able to show others how you came to your results. Therefore, your research data with all information reasonably necessary for verification needs to be preserved. With well-managed and preserved research data, you can defend yourself against allegations of mistakes. You can also prevent wrong conclusions from further spreading into the scientific community if there really are mistakes. Long term data preservation Research data can be preserved for different reasons such as verification and/or possible reuse. It can be your own wish or that of your university, funder or journal. Verification TODO: adapt this part The Netherlands Code of Conduct for Academic Practice (VSNU) states that raw data from research must be kept available for a minimum of ten years. This statement is also included in the Utrecht University Policy framework for research data: “Archived research data are to be retained for a minimum of ten years, commencing from the date that the research results are published.” Reuse It may be worthwhile to make (part of) your data available for a longer period of time and/or for a wider audience. Data which are suitable to keep for reuse are interpretable data on which new research can be based, independent of the publication. On the one hand, making research data reusable will need extra effort. On the other hand, possible reuse, even by your future self, might bring you lots of benefits and credits. Consider if your data is worth the effort of making it reusable or if preserving and archiving for verification is enough. Reuse is explained more in depth in the next part of this course: ‘Availability for reuse’. In this part we will focus on selection and preservation of research data for verification purposes. Data package Keeping data for verification serves the specific goal of having transparent, reproducible research. Alternatives to preserving raw data If preserving your raw data poses problems, alternatives can also ensure verfication. For instance, transcripts of recorded interviews could hold all important information and may be less privacy-sensitive, so it is reasonable to preserve those instead of the recordings themselves. Also, if raw data is very large, preserving your data only in some processed form could be an alternative. Combined with, for instance, a demonstrable quality check on the processing. The contents of your data package TODO: add image for illustration/zenodo? Others should be able to understand what you did. It is not enough to just provide data. Instead you should preserve a package with everything included that is necessary to reproduce your results. Think of including the following: Primary (raw) data; Secondary (processed) data; Protocols; Computer code/scripts; Lab journals; Metadata and/or codebooks describing the data; An overview of what the contents of the data package stating what file contains what information, and how these are related. The data should contain a reference to any publication which is based on the data. To make understanding your data less dependent on information in the publication, you can also add information on: Collection methods; Procedures; Experimental protocol; Your research question; Stimuli used; Sample descriptions. This is especially practical if the data package can be found and used on its own account. This is the case if it is published in a data repository or data journal as a data package for reuse. Do not forget to explicitly state who is responsible for the content of the data package, who is to be contacted in case of a request for access, and under what conditions access is granted. Where to preserve what type of data? During your research, you generate research results that can be made available for others. A paper or publication is the most traditional way of making results available, but it is by no means the only way. A relatively new way of making results available is using a public data repository. As you have just learned, preserving your data may serve the purpose of verification or reuse. Public data repositories cater to both needs. In addition, they handle requests to view or use your data which means you do not have to take care of such requests yourself. In the example below, you find a workflow for experimental research. What information can be made available in what place? Drag the items on the right to the correct place in the figure. Please note that some items can be used more than once. TODO: add H5P quiz and PDF solution? Accounting for data of others If you are permitted to use data from other parties, you will have to account for those as well if your research is to be verifiable and reproducible by others. You may recognise this from chapter 1 of this course: Data collection: Discover existing data, weblecture ‘Assessing usefulness of research data of others’ (5 of 10). You have the following options: If the used data is preserved correctly somewhere for the coming ten years, refer to the data repository in question. If it is not taken care of, contact the responsible persons, negotiate correct preservation in a data repository for ten years, and refer to that repository. If this isn’t possible, try to arrange a local copy that you preserve yourself; If this isn’t allowed, you will not be able to present the data in case of questions. Therefore, you should question yourself whether you can actually use the data. Figure 1: Preserve for 10 years Accounting for data of others on websites If you find interesting information on a website that you want to refer to, it is possible that this information will not be future proof. The link or web address might change over time (link rot). Or the information on a website is updated, changed or replaced with other content (content drift). It is possible to archive web pages on a web archive like the Internet Archive. You can capture a web page as it appears now for use as a trusted citation in the future (save a page). You will get an alternative link, pointing to the archived, static version of the page. Use this alternative link as a reference to the online information. How to preserve your data correctly In order for the data to survive for the long term, an active preservation regime has to be applied. The bad news is, data automatically gets lost over time. There are five main ways your data can be lost: Digital sources degrade over time (‘bit rot’); File formats and software become outdated; The media on which your data is stored becomes outdated or defective; Disaster strikes the storage location; The person that understands the data finds another job or data simply becomes forgotten. In this video below you will learn how to minimise the risk of losing data. You are also given good preservation practices. Match the solutions to the data loss From the weblecture you learned how to prevent data loss. Can you recall all applicable active regimes, as explained in the weblecture? Below you see a list of solutions to prevent data loss. Underneath that list you see a list of risks for data loss. Please add the number of each solution to the correct risk. Solutions to prevent data loss Have multiple copies. Use a checksum to identify faulty copies Use preferred file formats that can be opened by a wide range of software. Update the file format to a current one. Move data to fresh media well before the media’s expiration date. Have multiple copies. Move data to fresh media well before the media’s expiration date. Document your data well. Advertise the content in a data catalogue. TODO: add quiz text solution Write your data management plan for your data preservation Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module on data selection and preservation. You should be able to complete the following questions in the section ‘Data selection and preservation’: Which data should be preserved and/or shared? How and where will you keep your data for the long term?"},{"title":"Round up","url":"/topics/data-management-plans/tutorials/roundup/tutorial.html","tags":[],"body":"Introduction Click to expand! *Heading* 1. A 2. list * With some * Sub bullets Figure 1: Seqselector.png Introduction to rounding up You have almost reached the end of this course on research data management. You have learned about data collection, data documentation, data storage and security, selection and preservation and making data available for reuse. We are very curious to know if this course has helped you write your Data Management Plan (DMP). To round up: We want to remind you of the DMP review service of RDM Support; We want to share some good practices of data management with you; We invite you to fill out the evaluation of this online training. This will help us to further develop this training and future learners can benefit from this. Thank you very much! DMP review service You can have your data management plan (DMP) checked by the specialists of Research Data Management Support. You can get in touch if you are unsure about sections in your DMP or when you doubt whether your plan fits the requirements of your research funder. When you are in the process of writing a proposal for a research funder and you want a check on the data section, you can also contact the Research Support Office (RSO) of your faculty. Researchers sharing their experiences TODO: add stories if available or links to resources TODO: merge with experiences More data stories Challenges in irreproducible research special issue in Nature, 7 Oct 2015 There is growing alarm about results that cannot be reproduced. Explanations include increased levels of scrutiny, complexity of experiments and statistics, and pressures on researchers. Journals, scientists, institutions and funders all have a part in tackling reproducibility. Nature has taken substantive steps to improve the transparency and robustness in what they publish, and to promote awareness within the scientific community. Data stories in environmental science collected by DataONE Success stories and cautionary tales from researchers related to their experiences with managing and sharing scientific research data as collected by DataONE. Advantages of data sharing by John-Alan Pascoe of Delft University of Technology John-Alan Pascoe, researcher at the Faculty of Aerospace Engineering at Delft University of Technology, explains the advantages he experienced after sharing his raw and derived data in the data archive of 4TU.ResearchData. Evaluation of training TODO: link to questionnaire"},{"title":"03 Ecology Analysis using vegan","url":"/topics/metagenomics/tutorials/Ecology_Analysis_using_vegan/tutorial.html","tags":[],"body":"Ecology Analysis using vegan In this exercise we will look at a data matrix of 16S rRNA counts in 74 samples. This dataset is the microbiota composition of 74 mice from 5 different mice strains. The original research aim was to define the effect that the mouse genome has on the microbiota and what the effect of living in the same cage would be. However, we found much stronger trends in the data, and these we will look at in this exercise. The 454 data was already compiled into a matrix with genus abundance per sample in a previous step. This matrix is called a feature abundance matrix, or abundance matrix for short. We will do an ecology-oriented analysis of the data, in later steps also taking metadata (experimental, environmental or clinical data that was collected for each sample, independent of the DNA) into account. The aim of this tutorial is to get an idea of the very basic steps of ecological data analysis using the programming language R. The gene abundance table (Genus.txt) can be found in the folder /home/VIBTrainingX/NGS/metagenomics/higherLvl folder on the server. Those who are working on their own laptop can download it from the lotus website. Set the folder with the provided files as your working directory in R using setw. This way required files can be easily loaded. To find out how to use this command, you can type ?setwd() to open the help. If there are other R-commands that you want to know more about, you can open the R-help for that command by entering in the R-prompt ?command. This will be very useful when working with R, make sure to use this a lot as you can only learn more :o). hands_on Hands-on: Exercise 1 question Question How to set the working directory in R solution Solution setwd(\"dir_to_data\") Load the provided data into the matrix M (Genus.txt, actual genus abundance data), using the read.delim command, saving the loaded table as M. Make sure, the row names are correctly read in. As R reads the matrix as an object of class data.frame, we convert M from a data.frame to a matrix M=as.matrix(M). This is important for some of the following calculations, where we need a matrix class object. hands_on Hands-on: Exercise 1 question Question How to read in data as matrix ? solution Solution # read in data as matrix M = read.delim(file=\"Genus.txt\",row.names=1) M = as.matrix(M) The matrix you loaded represents the number of 16S sequences assignable to each genus, which we could find in the samples. Also note that not all genera are real genera, but partly assigned unknown sequences. With these groups we do not know if this is a single genus or in fact several genera or in extreme cases even several classes, that just all fall under the same phylum tag. What are the advantages and disadvantages of keeping such undefined groups in the data? Use the function edit(M) to better view the abundance matrix. Let’s look at some basic features of the abundance matrix. The summary(M) command is a good start, but also look at total row and column counts (colSums, rowSums command). To see how the genera are distributed within each sample, we will plot a sample-wise density plot.We will be using a combination of the density, lines and lapply functions, to draw the densities of values found in each sample. Let’s start with looking at the density of the first sample. In R you can access specific columns by writing the matrix coordinates in square brackets. For example M[1,] shows the first row of a matrix, M[,7] shows the 7th column etc: hands_on Hands-on: Exercise 3 question Question How to estimate density of first sample ? solution Solution # estimate density of first sample densityOfSample1 = density(M[,1]) Look at the object densityOfSample1 by simply entering the object name into the command prompt. Next try to visualize it with plot(densityOfSample1). In this plot you see that most genera are at 0 abundance, some genera have an abundance 0 The sum of each column in this matrix will tell us how many species were detected in total within the respective sample, use the apply and sum functions , saving the result in rich1. hands_on Hands-on: Exercise 11 question Question How to calculate the sum of each column ? solution Solution # Calculate sum of each column rich1 = apply(OnceOrMoreOftenPresent,2,sum) Compare the richness values in rich1 to the richness obtained on the rarefied matrix M2, calculated with a shortened command: # Calculate number of present species in each sample using the rarefied data rich2 = apply(M2>0,2,sum) Compare rich1 and rich2 in a matrix value by value. We use the cbind command to bind two vectors column wise together, so we get a matrix with 2 columns. Order this matrix by the richness values in rich1, using the order command and accessing the vector representation with [] square brackets. # Create new matrix with two columns: rich1 and rich2 and order rows according to rich1 values cbind(rich1,rich2)[order(rich1),] What does the second part of the formula do? What happens if you change that to order(rich2)? Discuss which richness values have the highest value to the researcher and why the order is very different between these two richness estimates. Is one way clearly wrong? Why did we choose 1000 as cutoff for the sequences per sample? What is the maximum value we could choose? First samples are clustered to see underlying data structures. For this tutorial we will choose a hierarchical clustering, based on a bray-curtis distance between samples, using the function vegdist. Make sure the distances are calculated between Samples and not Genera. Next, use the function hclust on the distance matrix, saving the output in variable cluster, and subsequently plot the clustering of the samples (using plot). Take a guess of how many groups there might be in this clustering? hands_on Hands-on: Exercise 11 question Question How to cluster samples and plot results ? solution Solution # cluster samples and plot results BCD = vegdist(t(M1), dist=\"bray\") cluster = hclust(BCD) plot(cluster) To visualize the samples and their relatedness to each other in a two-dimensional space, we can use an ordination to visualize the data in a low dimensional space. The dimensionality of the original matrix (73 genera=73 dimensions) is reduced to two dimensions. If you know what a PCA (Principal component analysis) is, this step will use a conceptually similar, but methodologically quite different technique to perform an ordination of the data, NMDS (non-metric multidimensional scaling). Start by calculating a 2-dimensional NMDS of the data using M1, using the Bray-Curtis distance in the function metaMDS, saving the result to nmds. Again, make sure that samples are being ordinated and not Genera. hands_on Hands-on: Exercise 11 question Question How to calculate the NMDS ? solution Solution # calculate NMDS nmds = metaMDS(t(M1),distance = \"bray\") #actual NMDS command, matrix needs to be transformed to conform with vegan’s standards Take a look at the nmds object and explore some of its features (e.g. type str(nmds) to see what variables are stored within the NMDS object). Try to find out what the stress of your ordination is. What does stress stand for (tip: go to the R help on metaMDS)? Next we can visualize the NMDS, similar to what you get out of PCA’s, displaying samples only: # plot NMDS plot(nmds,display =\"sites\") The important difference of NMDS compared to PCA is, that NMDS works with any kind of distance metric, while PCA can only use Euclidean distances between samples. A second important feature of NMDS is, that this method finds non-parametric, monotonic relationships between objects; in short: it doesn’t assume a specific data distribution. Why might these two features be important for ecologists? You might have noticed that you see two clusters, similar to the hierarchical clustering of the data. We can get for each sample the identity within the two clusters using the cutree commands, specifying k=2 (2 clusters). This can be plotted into the NMDS with the following command: # identify clusters memb = cutree(cluster, k = 2) ordispider(nmds,memb) Congratulations, you have just visualized the mouse enterotypes. Next we are going to look closer at these. If you want to know the exact methods to detect enterotypes in your data visit [http://enterotype.embl.de/enterotypes.html http://enterotype.embl.de/enterotypes.html] In the last step, we will test for all the genera in the matrix whether they show significant differences between two clusters. The scientific question we are posing here is: what are the significant differences in the gut microbiota of between enterotypes? We will use a non-parametric test (kruskal-wallis) to do the tests, as ecological data is in most cases not normally distributed. This test is very similar to the student t-test, and the interpretation works just the same way. Use the function kruskal.test to test the first genera (M[1,]) for significant differences between the two cluster groups (in object memb). Save the output of this command in variable Kt. hands_on Hands-on: Exercise 12 question Question How to test if there is a difference between the two clusters for the first genus ? solution Solution # Test if there is a difference between the two clusters for the first genus Kt = kruskal.test(M1[1,],memb) Look at the output of this function. This will show you a human readable summary of the test and the result. You can access elements of a list (Kt is a list in this case) using the $ operator. Try to extract the p-value from the Kt object. Once you know how, we can start to calculate the significance for every genus in the M1 matrix,. These p-values we will store in a newly created vector pvals. Let’s add the first 2 p-values to the vector: # Test if there is a difference between the two clusters for the first and second genera. Store p-values in a vector. pvals = c() pvals[1] = kruskal.test(M1[1,], memb)$p.value pvals[2] = kruskal.test(M1[2,], memb)$p.value Since doing this 73 times takes a long time, we will be using a for-loop to loop over the matrix and do this for us. We could as well use the apply function, but the syntax would get a little more complicated, since we are only interested in a subpart of the result, the $p.value part. Try to write a for-loop, to calculate the p-value 73 times. hands_on Hands-on: Exercise 13 question Question How to test if there is a difference between the two clusters for all genera ? solution Solution # Test if there is a difference between the two clusters for all genera for (i in 1:dim(M1)[1]) { pvals[i] = kruskal.test(M1[i,], memb)$p.value } As an additional help, you can add the name of the taxa to the pvals vector using the names command (that will name a vector): # Add names to the vector names(pvals) = dimnames(M1)[[1]] Which taxa are significantly different? In this case we will use the normalized M1 matrix, can you explain why we do not use the M or M2 matrix? Would either be wrong to use? In total we were testing in 73 genera, if their p-value was below a threshold of 0.05. What is the chance of observing data with a p-value >0.05 by random chance? How many genera do you expect to be below this threshold by random chance? To avoid statistical errors of this kind, we will use a Benjamini-Hochberg multiple testing correction, implemented in the R function p.adjust. Save the result as qvals. hands_on Hands-on: Exercise 14 question Question How to perform multiple testing correction of p-values using Benjamini-Hochberg method ? solution Solution # Multiple testing correction of p-values using Benjamini-Hochberg method qvals = p.adjust(pvals,method = \"hochberg\") What do you see in this test? What would you report on this dataset, based on these values? Try sorting the q-values to see the most significant differences first: # Sorting q-values sort(qvals) Now that you have finished the tutorials, you should be able to analyze any new dataset of amplicon data, using the LotuS pipeline and performing a basic analysis with R, including Data normalization Clustering analysis Ordination Univariate statistics You can always expand upon these concepts, using this tutorial as starting point. Just remember that R is a very flexible language, and all these commands can be expanded for new purposes and visualizations. Data sources All the material provided in this tutorial are from metagenomic study on mice knockouts. Further analysis of the data can be found in the reference below. Reference Hildebrand, F., Nguyen, A. T. L., Brinkman, B., Yunta, R. G., Cauwe, B., Vandenabeele, P., … Raes, J. (2013). Inflammation-associated enterotypes, host genotype, cage and inter-individual effects drive gut microbiota variation in common laboratory mice. Genome Biology, 14(1), R4. doi:10.1186/gb-2013-14-1-r4"},{"title":"01 Tools and Data files","url":"/topics/metagenomics/tutorials/Introduction/tutorial.html","tags":[],"body":"Tools Lotus pipeline LotuS offers a lightweight complete 16S/18S/ITS pipeline to Demultiplex and filter fasta or fastq sequences Denoise, remove chimeric sequences and cluster sequences into very high quality OTUs that perform at a similar level to mothur / dada2 Determine taxonomic origin of each OTU using >5 spezialized and general purpose database or statistical algorithms Construct OTU, genus, family, class, order and phylum abundance tables in .txt or .biom format Reconstruct OTU phylogenetic tree More information at LotuS home page usearch Download usearch version 8 and copy the executable in a folder e.g. /usr.bin/tools/ which you can reach (you might to be superuser for this) Make executable: sudo chmod +x /usr/bin/tools/usearch8.1.1861_i86linux32 Create a symbolic link into the folder where Lotus will search for it: sudo ln -s /usr/bin/tools/usearch8.1.1861_i86linux32 /usr/bin/tools/lotus_pipeline/bin/usearch_bin ` R package You also need R with the vegan package installed."},{"title":"02 OTU creation using LotuS","url":"/topics/metagenomics/tutorials/OTU_creation/tutorial.html","tags":[],"body":"OTU creation using LotuS In this tutorial we will create a genus abundance table from two 454 sequencer runs using a pipeline called LotuS. A genus abundance table contains counts of different genera in a several of samples – Rows are the different genera and columns the samples. As a simple example, take a look at this table:             Genus bl10 bl11 bl12 bl128 bl13 Bacteria 24 52 39 63 181 Bacteroides 169 27 7 42 6 Porphyromonadacea 370 346 621 565 224 This table tells us how often we observe unclassified Bacteria, Bacteroides and unclassified Porphyromonadaceae in the 5 samples bl10, bl11, bl12, bl128 and bl13. A matrix like this will be used for the next tutorial on numerical ecology and created from raw sequence data within this tutorial. The data In a recent experiment, we sequenced 73 samples in two 454 runs, the raw fasta and quality files are in /home/VIBTrainingX/metagenomics/ on the bits server. For each run we have a fasta (.fna) and quality (.qual) file. Go to this directory using the command cd and become aware of the files required from the experimenter (command ls). You can always take a look at files and their contents using viewing commands like less. The sequence files were multiplexed before the experiment, that is a small nucleotide sequence – the barcode - was attached to each read, specific for each experiment. A mapping file is typically used, containing the link between a sequence barcode and the name of the experiment and is essential to demultiplex the fasta files. The tools LotuS is actually a set of tools that were installed in the /opt/ folder. First go to the lotus website and familiarize yourself with the basic documentation. To start the exercises, go to the directory where Lotus is installed. cd /opt/lotus-1.62/lotus_pipeline/ From this directory you can run all the tools. To reach all data files (e.g. input files) you have to provide the path to the files: ~/metagenomics/ The analysis Creation of Mapping file. An Excel is provided, with some basic experiment annotation. The fwd primer is given as ACTYAAAKGAATTGACGG, but if you search for the primer sequence in the reads (in one of the .fna files) you will not find it because you need to reverse translate the primer sequence first using [http://www.bioinformatics.org/sms/rev_comp.html this tool]. So you see annotation provided by the provider is not always correct. Lotus needs experiment annotation to map input files to barcodes. Based on the documentation on [http://psbweb05.psb.ugent.be/lotus/documentation.html#MapFile the Lotus website], create a mapping file for this experiment. This means that you need to replace the column headers of the Excel file to terms that are accepted by Lotus and that you have to indicate that there is a .fna and a .qual file for each run. The header line should be preceeded by a #. The mapping file should at least contain four columns with the following headers: SampleID BarcodeSequence fnaFile qualFile Save the file as a tab-delimited text file. You can always test the validity of your mapping file with the command ./lotus.pl -check_map [your mapping file] If you have fastq files as input the fnaFile and qualFile columns would be replaced by one fastqFile column. Changing the data format of the input files. Sometimes you need to transcribe data from one format to another. For instance we could transform the fasta files (.fna) to fastq files (.fq). This can be done with the program sdm, that is part of the LotuS pipeline. Take a look at the sdm help by typing ./sdm and exploring the options, e.g. ./sdm -help_commands Then, using command line arguments, transcribe the fasta + qual files of the Anh experiment into fastq files. Take a look at output and log files created by sdm. hands_on Hands-on: Exercise 1 question Question How to transform fasta + qual files into fastq files ? solution Solution sudo ./sdm -i_fna ~/metagenomics/Anh.1.fna -i_qual ~/metagenomics/Anh.1.qual -o_fastq t1.fq In the lotus_pipeline folder the fastq file t1.fq was generated, to take a look at the file use head t1.fq Do the same for the t1.log file: you see that sdm is not only used to transform fasta into fastq files but it is also capable of doing quality filtering on the raw reads files. Setting up a quality filter of the input sequence files. Since we want to make sure the quality filtering of the input file is strict, LotuS offers several quality filtering options. Quality settings are different for different data formats, that´s why Lotus offers a file with specific settings for each platform. Since we have 454 data we will take a look at the file sdm_454.txt. less sdm_454.txt Read the comments (line starting with “#”) to each option and think which quality filtering options might be important in order to create OTUs from the raw sequences. (Hint: an OTU is a clustering of similar sequences with the aim to have one cluster of sequences for each species that was originally present in the samples. Take into account that sequencing machines make errors and that PCR amplification of the 16S rDNA is similarly with errors). Think about a set of parameters, including the statistical information from step 2, and save these in your copy of sdm_options.txt for later use. Check the sdm quality filter settings. Some of the default filter settings are: MinSeqLength=250 : Only use reads of at least 250 nt long after processing (remember we are working with long reads from 454 sequencing) TruncateSequenceLength = 250 : Cut all reads after 250 nt QualWindowWidth = 50 and QualWindowThreshold = 25 : Remove all reads where average quality is = 0,5 maxHomonucleotide = 8 : Remove all reads with a homonucleotide run (repeat of same nt) >= 8 RejectSeqWithoutFwdPrim = TRUE : Remove all reads that do not contain the forward primer Demultiplexing and quality filter the input files. For this step you will need the mapping file from Step 1 and the file with the quality filtering settings for 454 data. Use the sdm command to demultiplex and filter all input files at the same time into a local folder ‘‘demultDir’’. First create the folder where the demultiplexed files will be written in ~/metagenomics/: mkdir ~/metagenomics/demultDir Since the mapping file contains information on all files, you have to provide an input path to the folder that contains the input files (.fna + .qual) to sdm. hands_on Hands-on: Exercise 2 question Question How to demultiplex and quality filter files ? solution Solution ./sdm -i_path ~/metagenomics/ -o_fastq t1.fq -o_demultiplex ~/metagenomics/demultDir/ -map ~/metagenomics/map.txt -options sdm_454.txt Input is the folder containing the .fna and .qual files. The demultiplexing will fill the demultDir folder with fastq files. Discuss the output files and what each of these represents. In this experiment multiple samples were sequenced in the same lane. Two lanes were used, each containing 37 samples. After sequencing, this results in two files with reads. To know which sample a read comes from, unique bar codes are incorporated into the adapter sequences. One specific bar code for each sample. In this step reads from different samples (but from the same lane thus in the same fasta file) are split over separate fastq files, one for each sample. Mapping file creation when sequence provider provides demultiplexed files. Now that you have demultiplexed the files into a single folder, you might be aware that sequence providers often deliver files in this format: already demultiplexed into single files. In this case slight modifications to the mapping file are enough to change the input from non-demultiplexed large file(s) to demultiplexed-many-small-files. Note that lotus has a special script that creates the mapping file for you in this case. The script is autoMap.pl. It is used to link SampleIDs to demultiplexed files. Run autoMap. ./autoMap.pl ~/metagenomics/demultDir/ ~/metagenomics/automap.txt 1,1 Running Lotus. We will run Lotus on the demultiplexed files. Use the mapping file you generated in Step 5 and the settings file sdm_454.txt. Use the utax taxonomy to assign a phylogeny to the derived OTUs. Run lotus from out the /opt/lotus_pipeline/ and save the output in the folder ‘‘testR’’ hands_on Hands-on: Exercise 3 question Question How to run lotus solution Solution sudo ./lotus.pl -i ~/metagenomics/demultDir/ -o ~/metagenomics/testR/ -m ~/metagenomics/automap.txt Input is the folder containing the .fna and .qual files. The demultiplexing will fill the demultDir folder with fastq files. In case you haven’t done any quality filtering yet, you can still do it now. The command would then be: sudo ./lotus.pl -i ~/metagenomics/demultDir/ -o ~/metagenomics/testR/ -m ~/metagenomics/automap.txt -s sdm_454.txt Peek at the file hiera_RDP (using head). The file maps eachg OTU to a genus. Peek at the file OTU.txt (using head). The first line contains the number of reads that represent OTU_1 in each sample. Peek at the file otus.fa (using head). It contains the reads representing each OTU. You can use this file to blast the sequences to check if they are really from the OTU they were assigned to. Go to the folder higherLvl. This contains the data that we are going to use in the Ecology analysis. Go to the folder LotuSLogs. This contains the settings of the analysis. For instance, peek a the file demulti.log: it shows how many reads were rejected… The file citations.txt contains the references for reporting your LotuS results. Using a different taxonomy assignment on a finished run. In this step we want to reassign the taxonomy to a LotuS run, but keep exactly the same OTUs. In this exercise, assign the OTUs to the Silva taxonomy. This option is useful, if e.g. you want to keep your work on a given OTU set (as well as the phylogenetic tree), but want to try out what would have happened if you had used e.g. Silva as reference database instead of utax. hands_on Hands-on: Exercise 4 question Question How to reassign the taxonomy with Silva as reference database? solution Solution sudo ./lotus.pl -i ~/metagenomics/demultDir/ -o ~/metagenomics/testR2/ -m ~/metagenomics/automap.txt -s sdm_454.txt -refDB SLV -redoTaxOnly 1 Input is the folder containing the .fna and .qual files. The demultiplexing will fill the demultDir folder with fastq files. Using a custom database. The research of honey bee gut communities have very specific taxonomic names for already known bacteria. In order to accomodate for their naming sheme, we will use a very specific database that contains 16S sequences of bacteria mostly found in the honey bee gut. Download the bee taxonomy in tax format and [http://psbweb05.psb.ugent.be/lotus/packs/DB/beeTax/beeTax.fna bee taxonomy in fna format]. Use the two provided files (fna, tax) to again redo the taxonomy, but this time assigning first using the honey bee DB and secondly everything with low hit should be assigned with the SILVA database. hands_on Hands-on: Exercise 5 question Question Use honey bee taxonomy database ? solution Solution sudo ./lotus.pl -i XX -o ~/metagenomics/testR3/ -redoTaxOnly 1 \\ -m ~/metagenomics/LASI_Spring_2_bees_barn_3_map_lts_5.txt \\ -refDB ~/metagenomics/beeTax.fna,SLV -tax4refDB ~/metagenomics/beeTax.tax Input is the folder containing the .fna and .qual files. The demultiplexing will fill the demultDir folder with fastq files. Get everything assigned! In this step we want to assign every OTU sequence to a database target – and we don’t care about false positive assignments! Of course this is per se wrong, but in some cases you just want to know what the best hit would be, even if it is only 90% similar to your OTU sequence. LotuS provides several options that allow tweaking towards more lenient assignments. Find all options related to this and try to create the most extreme case with these options, by reassigning the taxonomy again as in the previous step. Try a different sequence clustering algorithm. Now rerun lotus, but try to optimize for a lot of small, hard defined OTUs (that might correspond to something like strain level). Which clustering algorithm might be suitable? Which clustering cutoffs make sense? For this specific run, use the first mapping file you created (step 1) and the non-demultiplexed input files. Save this output in the folder ‘‘testR4’’ Your own best run. Now that you have run LotuS with various databases and options, go back and look at the output folder of the different runs, look at the statistics provided in the ‘‘LotuSLogS’’ subfolder. Based on this, tune the sdm filtering parameter file from step 3 (again), choose the database you think best suited/most interesting, and choose a clustering algorithm. With this create run the sample set again, saving the output in folder ‘‘testrun1.3’’. This output folder you can use in the following session on numerical ecology. If LotuS run has finished, go to the specified output folder and copy the genus.txt from the output folder to your home folder: cp testrun1.3/ higherLvl/genus.txt ~ Using Illumina data as input. In all the analysis before we were using 2 x 454 runs from an outdated next generation sequencing technology. For the next exercise we will look at the output of an Illumina miSeq sequencing platform, that is still being used a lot nowadays. Set up the mapping file, using [http://data.bits.vib.be/pub/trainingen/metagenomics/Miseq.xlsx the provided Miseq.xlsx file]. Run LotuS, after you set up a custom sdm configuration file and using a combination of parameters you learned about in previous steps. This run might take some time longer to finish. Be sure you set it to use all the cores of your computer and let it run over the lunch break. Congratulations, now you know how to process raw sequence files to meaningful summary tables, that can be directly analyzed in R or even Excel! In the next tutorial this matrix will be analyzed with the help of R, after the lunch break."},{"title":"Share: Data availability for reuse","url":"/topics/data-management-plans/tutorials/share-reuse/tutorial.html","tags":[],"body":"Introduction to data availability for reuse Thanks to information and communication technology and globalisation new opportunities arise to exchange results of scientific research - publications and research data - and even of scientific methods and practices. This new way of practising science is called ‘open science’. Open data is a part of this movement towards open science. It is the ambition of universities, governments, funders and publishers to make research data optimally suited for reuse. There are different reasons why you may not be able to share your research data. Thinking about these issues and challenges when developing your data management plan will help you reflect on such reasons in an early stage. How frustrating a data request can be Not being prepared to share your data can lead to problems in using the data. In this short video, you see what shouldn’t happen when a researcher makes a data sharing request! Topics include storage, documentation, and file formats. A made up, yet not unrealistic story. Introduction to data repositories In order to preserve, manage, and provide access to your research data, you can deposit your data in a data repository. Data repositories allow permanent access to datasets in a trustworthy environment and enable search, discovery, and reuse of the data they host. Click on the topics below to find out more about data repositories. TODO: add repositories from Elixir A wide variety There is a wide variety of data repositories. Most have the option to publish your dataset using a persistent identifier and some provide the service of long-term preservation. Some repositories host data from various disciplines and others are domain- or discipline specific. Choosing a data repository When choosing a repository for your data be sure to check if the repository meets your criteria or the criteria set by your funder or journal editors. Criteria to select a certain repository can be: Is the repository certified with a CoreTrustSeal or Data Seal of Approval? Repositories with a Data Seal of Approval are recognised in the community as a trustworthy source of data. Is long term archiving guaranteed or not? Some repositories will guarantee the legibility of the data, even if the hardware and software become obsolete. What are the costs per dataset or gigabyte? Repositories differ in their cost model, some allow free deposits up to a certain amount of storage What is the physical storage location of data? The location of your data determines under which data protection law it falls. Some repositories store data in the US and others in the EU. What is the default license? Some repositories allow for open or restricted access, or you can specify which license for use you want for your data. You can use this repository selection tool to help you select a suitable repository. Registry of research data repositories You can browse or search for a data repository in re3data.org. This is a global registry of research data repositories covering different academic disciplines. You can search or browse by subject, content type or country. You can filter the search and browse results on criteria for choosing a data repository as described above. https://www.re3data.org/ Some well-known and more generic repositories Zenodo – a repository that enables researchers, scientists, EU projects and institutions to share and showcase multidisciplinary research results (data and publications) that are not part of the existing institutional or subject-based repositories of the research communities; Dryad – a curated general-purpose repository that makes the data underlying scientific publications discoverable, freely reusable and citable. Dryad has integrated data submission for a growing list of journals; Open Science Framework (OSF) - a scholarly commons to connect the entire research cycle. It is part network of research materials, part version control system, and part collaboration software; Figshare – a repository that allows researchers to publish all of their research outputs in an easily citable, sharable and discoverable manner. Explore data repositories You have just learned about the existence of a global registry of research data repositories that covers repositories from different academic disciplines. Re3data.org makes it possible to search for a repository that meets your criteria. Go to www.re3data.org/search and find a repository that meets all three of the following criteria: Certificate → CoreTrustSeal Data licenses → CC0 (Creative Commons 0) Persistent identifier (PID systems) → DOI (Digital Object Identifier) Make use of the filters offered on the left side of the screen, as visualized here: TODO: quiz with ELIXIR resources Give clarity with (Creative Commons) licenses In order to publish your data and make it reusable, you require a license. A license creates clarity and certainty for potential users of your data. A license is not an option for all data; some of it may be too confidential or privacy-sensitive to be published. Creative Commons licenses Licenses such as the Creative Commons (CC) licenses replace ‘all rights reserved’ copyright with ‘some rights reserved’. There are seven standard CC licenses. CC-BY is the most commonly used license, in which attribution is mandatory when using data. You can also choose restrictions like non-commercial, no derivatives, or share alike. Creative Commons offers a guide to help you determine your preferred license. Figure 1: Creative Commons Assigning a license to your data Assigning licenses to data can also have disadvantages. Licenses are static and do not change with the quick developments in the field of research data. Therefore, some data repositories work with a CC0 license whereby no rights are reserved. Instructions regarding use are completed with codes of conduct, which may be adapted more easily. A short movie explaining the different Creative Commons elements is shown below. Remember that sharing without a license can still lead to conflicts. TODO: add video on CC licenses? Question We are very interested to know what license you would choose if you were to share the underlying research data of your most recent publication. An explanation for each license can be found by clicking on the links below. CC BY: Attribution CC BY-SA: Attribution ShareAlike CC BY-ND: Attribution-NoDerivs CC BY-NC: Attribution-NonCommercial CC BY-NC-SA: Attribution-NonCommercial-ShareAlike CC BY-NC-ND: Attribution-NonCommercial-NoDerivs CC0: Public Domain Publishing in a data journal Data journals are publications whose primary purpose is to publish datasets. They enable you as an author to focus on the data itself, rather than producing an extensive analysis of the data which occurs in the traditional journal model. Fundamentally, data journals seek to: Promote scientific accreditation and reuse; Improve transparency of scientific methods and results; Support good data management practices; Provide an accessible and permanent route to the dataset. The benefits of publishing in a data journal Publishing in a data journal may be of interest to researchers and data producers for whom data is a primary research output. In some cases, the publication cycle may be quicker than that of traditional journals, and where there is a requirement to deposit data in an “approved repository”, long-term curation and access to the data is assured. Publishing a data paper may be regarded as best practice in data management as it: Includes an element of peer review of the dataset; Maximises opportunities for reuse of the dataset; Provides academic accreditation for data scientists as well as for front-line researchers. (source: ANDS Guide) General and disciplinary data journals There are data journals for various disciplines and also more general data journals exist. A widespread standard PID is the DOI. DOI stands for ‘Digital Object Identifier’. A DOI is an alphanumeric string assigned to an object which allows for an object to be identified over time. Often a DOI will be presented as a link which looks like: https://doi.org/10.1109/5.771073. There are other identifiers available which some repositories may use instead. If you are depositing in a reputable repository then you should be given some type of persistent identifier which you can use to cite and link to your data. Examples of generic data journals: Scientific Data Data in Brief Data Science Journal Examples of disciplinary data journals: TODO: check for life science additions Open archaeology data; Earth System Science Data; Research Data Journal for the Humanities and Social Sciences. How to cite a dataset Citations to your data can add to your academic impact. A citation should include enough information so that the exact version of the data being cited can be located. Including a Persistent Identifier (PID) in the citation ensures that even if the location of the data changes, the PID will always link to the data that were used. You can indicate in your (Creative Commons) license or user agreement that you want your data cited when reused. Data citations work just like book or journal article citations and can include the following information: Author; Year; Dataset title; Repository; Version; Persistent IDentifier (PID), often works as a functional link/URL. Examples A widespread standard PID is the DOI. DOI stands for ‘Digital Object Identifier’. A DOI is an alphanumeric string assigned to an object which allows for an object to be identified over time. Often a DOI will be presented as a link which looks like: https://doi.org/10.1109/5.771073. There are other identifiers available which some repositories may use instead. If you are depositing in a reputable repository then you should be given some type of persistent identifier which you can use to cite and link to your data. Irino, T; Tada, R (2009): Chemical and mineral compositions of sediments from ODP Site 127‐797. Geological Institute, University of Tokyo. http://dx.doi.org/10.1594/PANGAEA.726855 Tips Tip1: Get a PID at the data repository of your choice. Tip2: Is your PID a DOI and do you want to cite it in the format of a specific journal? Use the DOI formatter from CrossCite. TODO: add short quiz FAIR data FAIR stands for ‘Findable, Accessible, Interoperable, and Reusable’. The FAIR data principles act as an international guideline for the result of high-quality data management. With the increase in volume, complexity and creation speed of data, humans are more and more relying on computational support for dealing with data. The principles were defined with the focus on machine-actionability, i.e. the capacity of computational systems to find, access, interoperate and reuse data with none or minimal human intervention. F – Findable By using correct metadata to describe the data, it will be findable. By using a persistent identifier the data can be found by computer systems automatically. A – Accessible The data should be accessible for the long term. Even when underlying data is not accessible, the describing metadata should remain available. I – Interoperable The data can be used and combined with other datasets. To achieve this, the data should be stored in generic file types, not in software specific file types. R – Reusable The options for reuse should be stated clearly in a license. Without a license there is no certainty about the options for reuse and creator rights are implicit. How to achieve FAIR data In general, having a good data management plan will lead to FAIR data. In the case of privacy-sensitive data, it is possible to meet the criteria, but not to share the data openly. In this case you can make sure that a well-described dataset can be found online, while preventing the underlying data to be downloaded and used without permission. If you anonymise your data, presuming the data is of limited sensitivity and you are very sure the data cannot lead back to the persons involved, you can share your data openly. The FAIR Guiding Principles were put together and published in Scientific Data (Mark D. Wilkinson et al., “The FAIR Guiding Principles for Scientific Data Management and Stewardship,” Scientific Data 3 (March 15, 2016): 160018.). TODO: add question H5P quiz? Open science “Open Science is the practice of science in such a way that others can collaborate and contribute, where research data, lab notes and other research processes are freely available, under terms that enable reuse, redistribution and reproduction of the research and its underlying data and methods.” (Source: FOSTER). You have learned that good data management contributes to the findability, accessibility, interoperability and reusability of your research data. This does not necessarily mean that you should make your data openly available. But to open up data, you do need good data management from the earliest possible stage of your research project. TODO: add links to ORION course or other relevant elements Flemish open science plan? Write your data management plan for your data reuse Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module on data sharing and availability for reuse. You should be able to complete the following questions in the section ‘Data availability for reuse’: What secondary use of your data is intended or foreseeable? Where will you make your data available? What access and usage conditions will apply?"},{"title":"Introduction to Genome Assembly","url":"/topics/basic-bioinformatics/tutorials/general-introduction/tutorial.html","tags":[],"body":"Genome assembly with Velvet: Background Velvet is one of a number of de novo assemblers that use short read sets as input (e.g. Illumina Reads). The assembly method is based on the manipulation of de Bruijn graphs, via the removal of errors and the simplication of repeated regions. comment Comment For information about Velvet, you can check its (nice) Wikipedia page. For this tutorial, we have a set of reads from an imaginary Staphylococcus aureus bacterium with a miniature genome (197,394 bp). Our mutant strain read set was sequenced with the whole genome shotgun method, using an Illumina DNA sequencing instrument. From these reads, we would like to rebuild our imaginary Staphylococcus aureus bacterium via a de novo assembly of a short read set using the Velvet assembler. Agenda In this tutorial, we will deal with: Get the data Evaluate the input reads Assemble reads with Velvet Collect some statistics on the contigs Discussion Get the data We will now import the data that we will use for the tutorial. hands_on Hands-on: Getting the data Create and name a new history for this tutorial. Import from Zenodo or from the data library the files: mutant_R1.fastq mutant_R2.fastq tip Tip: Importing data via links Copy the link location (Right-click on the filename then “Copy Link Address”) Open the Galaxy Upload Manager Select Paste/Fetch Data Paste the link into the text field Change the data-type to fastqsanger Press Start Change the name of the files to mutant_R1 and mutant_R2. As a default, Galaxy uses the link as the name of the new dataset. It also does not link the dataset to a database or a reference genome. tip Tip: Renaming a dataset Click on the galaxy-pencil pencil icon for the dataset to edit its attributes In the central panel, change the Name field Click the Save button Inspect the content of a file. tip Tip: Inspecting the content of a dataset Click on the galaxy-eye (eye) icon next to the relevant history entry View the content of the file in the central panel question Questions What are four key features of a FASTQ file? What is the main difference between a FASTQ and a FASTA file? solution Solution Each sequence in a FASTQ file is represented by 4 lines: 1st line is the id, 2nd line is the sequence, 3rd line is not used, and 4th line is the quality of sequencing per nucleotide In a FASTQ file, not only are the sequences present, but information about the quality of sequencing is also included. The reads have been sequenced from an imaginary Staphylococcus aureus bacterium using an Illumina DNA sequencing instrument. We obtained the 2 files we imported (mutant_R1 and mutant_R2) question Question Why do we have 2 files here if we only sequenced the bacteria once? solution Solution The bacteria has been sequenced using paired-end sequencing. The first file corresponds to forward reads and the second file to reverse reads. Evaluate the input reads Before doing any assembly, the first questions you should ask about your input reads include: What is the coverage of my genome? How good is my read set? Do I need to ask for a new sequencing run? Is it suitable for the analysis I need to do? We will evaluate the input reads using the FastQC tool. This tool runs a standard series of tests on your read set and returns a relatively easy-to-interpret report. We will use it to evaluate the quality of our FASTQ files and combine the results with MultiQC. hands_on Hands-on: FastQC on a fastq file FastQC tool with the following parameters “Short read data from your current history” to (Multiple datasets) mutant_R1.fastq and mutant_R2.fastq MultiQC tool with the following parameters “Software name” to FastQC “Result file” to the raw data files generated by FastQC MultiQC generates a webpage combining reports for FastQC on both datasets. It includes these graphs and tables: General statistics This is important in setting maximum k-mer size for an assembly. comment Getting the length of sequences Click on Configure Columns Check Length Close the window question Questions How long are the sequences? What is the average coverage of the genome, given our imaginary Staphylococcus aureus bacterium has a genome of 197,394 bp? solution Solution The sequences are 150 bp long We have 2 x 12,480 sequences of 150 bp, so the average genome coverage is: 2 * 12480 * 150 / 197394, or approximately 19 X coverage. Sequence Quality Histograms Dips in quality near the beginning, middle or end of the reads may determine the trimming/cleanup methods and parameters to be used, or may indicate technical problems with the sequencing process/machine run. Figure 1: The mean quality value across each base position in the read question Questions What does the y-axis represent? Why is the quality score decreasing across the length of the reads? solution Solution The y-axis represents the quality score for each base (an estimate of the error during sequencing). The quality score is decreasing accross the length of the reads because the sequencing become less and less reliable at the end of the reads. Per Sequence GC Content High GC organisms tend not to assemble well and may have an uneven read coverage distribution. Per Base N Content The presence of large numbers of Ns in reads may point to a poor quality sequencing run. You will need to trim these reads to remove Ns. k-mer content The presence of highly recurring k-mers may point to contamination of reads with barcodes or adapter sequences. comment Comment For a fuller discussion of FastQC outputs and warnings, see the FastQC website link, including the section on each of the output reports, and examples of “good” and “bad” Illumina data. We won’t be doing anything to these data to clean it up as there isn’t much need. Therefore we will get on with the assembly! Assemble reads with Velvet Now, we want to assemble our reads to find the sequence of our imaginary Staphylococcus aureus bacterium. We will perform a de novo assembly of the reads into long contiguous sequences using the Velvet short read assembler. The first step of the assembler is to build a de Bruijn graph. For that, it will break our reads into k-mers, i.e. fragments of length k. Velvet requires the user to input a value of k (k-mer size) for the assembly process. Small k-mers will give greater connectivity, but large k-mers will give better specificity. hands_on Hands-on: Assemble the reads FASTQ interlacer tool with the following parameters “Type of paired-end datasets” to 2 separate datasets “Left-hand mates” to mutant_R1.fastq “Right-hand mates” to mutant_R2.fastq Currently our paired-end reads are in 2 files (one with the forward reads and one with the reverse reads), but Velvet requires only one file, where each read is next to its mate read. In other words, if the reads are indexed from 0, then reads 0 and 1 are paired, 2 and 3, 4 and 5, etc. Before doing the assembly per se, we need to prepare the files by combining them. velveth tool with the following parameters “Hash Length” to 29 “Input Files”: click on Insert Input Files “file format” to fastq “read type” to shortPaired reads “Dataset” to the pairs output of FASTQ interlacer The tool takes our reads and break them into k-mers. velvetg tool with the following parameters “Velvet Dataset” to the output of velveth “Using Paired Reads” to Yes This last tool actually does the assembly. Two files are generated: A “Contigs” file This file contains the sequences of the contigs longer than 2k. In the header of each contig, a bit of information is added: the k-mer length (called “length”): For the value of k chosen in the assembly, a measure of how many k-mers overlap (by 1 bp each overlap) to give this length the k-mer coverage (called “coverage”): For the value of k chosen in the assembly, a measure of how many k-mers overlap each base position (in the assembly). A “Stats” file This is a tabular file giving for each contig the k-mer lengths, k-mer coverages and other measures. Collect some statistics on the contigs question Question How many contigs have been built? What is the mean, min and max length of the contigs? solution Solution 190 To compute this information, we can use the Datamash tool on the 2nd columns (length). Be careful with the first line, the header. As a result, we obtain: 597.82 as mean, 1 as min and 12904 as max. It would mean that the smallest contig has a length of 1 bp, even smaller than k. The length on the 2nd column corresponds to length of the contig in k-mers. This means that the smallest contig has a length of 1k = 29. So to obtain the real length, we need to add k-1 to the length. We then obtain a mean contig length of 625.82 bp, a min contig of 29 bp and a max contig of 12,932 bp. This table is limitted, but we will now collect more basic statistics on our assembly. hands_on Hands-on: Collect fasta statistics on our contigs Quast tool with “Contigs/scaffolds output file” to the output of velvetg “Type of data” to contig “Reference File” to wildtype.fna “Type of organism” to Prokaryotes “Lower Threshold” to 500 “Thresholds” to 0,1000 This tool generates 5 output files, but we will focus on the HTML report and the Icarus viewer. question Question What is represented in the Icarus viewer? solution Solution Icarus is a novel genome visualizer for accurate assessment and analysis of genomic draft assemblies. It draws contigs ordered from longest to shortest, highlights N50, N75 (NG50, NG75) and long contigs larger than a user-specified threshold The HTML report reports many statistics computed by QUAST to assess the quality of the assembly: Statistics about the quality of the assembly when compared to the reference (fraction of the genome, duplication ratio, etc) Misassembly statistics, including the number of misassemblies A misassembly is a position in the contigs (breakpoints) that satisfy one of the following criteria: the left flanking sequence aligns over 1 kbp away from the right flanking sequence on the reference; flanking sequences overlap on more than 1 kbp flanking sequences align to different strands or different chromosomes Unaligned regions in the assembly Mismatches compared to the reference genomes Statistics about the assembly per se, such as the number of contigs and the length of the largest contig question Question How many contigs have been constructed? Which proportion of the reference genome do they represent? How many misassemblies have been found? Has the assembly introduced mismatches and indels? What are N50 and L50? Is there a bias in GC percentage induced by the assembly? solution Solution 190 contigs have been constructed, but only 47 have a length > 500 bp. The contigs represents 87.965% of the reference genome. 1 misassembly has been found: it corresponds to a relocation, i.e. a misassembly event (breakpoint) where the left flanking sequence aligns over 1 kbp away from the right flanking sequence on the reference genome. 8.06 mismatches per 100 kbp and 4.03 indels per 100 kbp are found. N50 is the length for which the collection of all contigs of that length or longer covers at least half an assembly. In other words, if contigs were ordered from small to large, half of all the nucleotides will be in contigs this size or larger. And L50 is the number of contigs equal to or longer than N50: L50 is the minimal number of contigs that cover half the assembly. The GC % in the assembly is 33.64%, really similar to the one of the reference genome (33.43%). Discussion hands_on (Optional) Hands-on: Rerun for values k ranging from 31 to 101 velveth tool with the same parameters as before except “Hash Length” to a value between 31 and 101 velvetg tool with the same parameters as before Quast tool with the same parameters as before We have completed an assembly on this data set for a number of k values ranging from 29 to 101. A few of the assembly metrics appear below. Figure 2: Number of contigs in the assembly for various k-mer sizes Figure 3: Largest contig in each of the assemblies by k-mer size Figure 4: Total number of base pairs in all the contigs for each assembly by k-mer size Figure 5: N50 metric for each of the assemblies by k-mer size question Questions Are there any distinct features in the charts? Does it look like one assembly might be better than some of the others? The reasons for these patterns will be discussed in detail in the De Bruijn graph assembly slides and tutorial."},{"title":"Handle: Data storage","url":"/topics/data-management-plans/tutorials/handle-store/tutorial.html","tags":[],"body":"Introduction Click to expand! *Heading* 1. A 2. list * With some * Sub bullets Figure 1: Seqselector.png TODO: specific chapter on storage Write your data management plan for your data storage Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module on data storage. You should be able to complete the following questions in the section ‘Data documentation’: Where will you store your data? How will the data be backed up? After finishing this part in DMPonline, please return to the learning environment and click on [Complete]. This takes you back to the course overview. Continue with the next learning unit. You can ask your faculty or project data manager or RDM Support for a review of your DMP once you have finished writing all or parts of your DMP."},{"title":"Prepare: Data collection","url":"/topics/data-management-plans/tutorials/prepare-collect/tutorial.html","tags":[],"body":"Introduction Introduction to data collection By now you will have obtained some idea of what research data management is all about. Now we will have a more in-depth look into the different phases of your research by starting with data collection. Data collection involves understanding the different types of data you collect. Depending on the nature of your research, there are different methods of collecting data and thus different types of data. Your data may be physical (paper records or archival forms) or digital (database contents or Excel data). The source of your data may be external, you collect it yourself or you generate it from a machine. When you write your data management plan you will need to take into account the type of data you collect, the source of the data, and how you will process and analyse your data. You can watch the video below, provided by TU Delft, about data collection. The video stops at 1:12. Preferred formats for your research data This part is based on the online Research Data Management training ‘MANTRA’ of The University of Edinburgh (CC BY: https://mantra.edina.ac.uk/) and Managing Data @ Melbourne. Figure 1: Learning objectives The file formats you use to generate your research data will influence how you can manage them over time, i.e. a program or application must be able to recognise the file format in order to access your data within the file. For example, a web browser is able to process and display a file in the HTML file format so that it appears as a web page. If the browser encounters another file type, it may need to call on a special plug-in to view it. Or it may simply let you download the file to view if it can recognise it in another program. To identify the file format, files usually have a file name extension, or suffix that follows a full stop in the file name and contains three or four letters, like for example: TODO: add PDF with links to preferred file formats .txt text .pdf portable document format .jpg joint photographic experts group .csv comma separated values .html hypertext markup language .xml extensible markup language .rtf rich text format Figure 2: Background on proprietary and open formats Figure 3: Background on proprietary and open formats question Question Determine which format is proprietary and which is an open format .xml .pdf .psd .odf .ppt .docx .csv .xls Check your answers! Proprietary: .psd, .docx, .xls, .ppt Open format: .csv, .xml, .odf, .pdf TODO: list of preferred formats Figure 4: Risks of file conversion question Question While file conversion or migration sometimes has to be done, there are also risks. Which ones can you think of? Check your answers! file size may change and even become surprisingly large blanks used as missing data code special characters and end of line returns may change relation among items in a table and among tables may be lost layers, color fidelity and resolution may be lost or changed in image files fonts, footnotes and links to other documents may change frame rate, sound quality, codecs and wrappers may be altered in multimedia files last characters in rows (due to row size limitations) may be altered hands_on Hands On Open the following .docx file to the preferred format .txt: PreferredFormatsExcersizePenguinDOC.docx Convert this docx file to the preferred format .txt Open the text file in an editor Is all formatting preserved OK? Check your answers! No, the format Microsoft Word creates saves the layout together with the textual and other elements. The .txt format created by Word is only the textual information in your file. hands_on Hands On Open the following .docx file to the preferred format .txt: PreferredFormatsExcersizePenguinDOC.docx Convert this docx file to the preferred format .odt Open the .odt file Is all formatting preserved OK? Check your answers! No, ODT files are formatted using the OASIS OpenDocument XML-based standard. When you open an OpenDocument Text file in Word, it might not have the same formatting as it did in the original application it was created in. This is because of the differences between applications that use the OpenDocument Format. Discovering existing data Where to discover existing data? Watch the screencast below. In this screencast, you will be guided through different ways to find data. hands_on Hands On You have just learned that there are different places to find data. By actively searching the different places, you will get an understanding of the differences. Look at the different portals below. Some of them have been showcased in the screencast, some of them are additional. Google - add “database OR registry OR dataset OR archive OR statistics” to your search Registry of Research Data Repositories re3data - find appropriate repositories holding interesting data ZanRan - search engine for tables and graphes within .pdf or .html on the internet Elsevier Data Search - try out chip-seq drosophila Google Dataset Search - try out chip-seq drosophila. Google Dataset Search indexes OmicsDI, an index providing a knowledge discovery framework across heterogeneous omics data (genomics, proteomics, transcriptomics and metabolomics). Assess the usefullness of existing data How useful is a dataset? Follow this short tutorial. Assess the usefullness of existing data yourself In the previous activity, the lecture described four main points to check if you want to reuse existing data: Condition for reuse Context Trustworthiness Persistence In the following quizzes, take a closer look at the description or metadata of some datasets and assess the usefulness of these datasets yourself. As the description or metadata of datasets can be lacking in several different areas at the same time, it will be indicated per assignment on which of the four main points your focus should be. hands_on Hands On Can you re-use this dataset on Spatial Patterns of Water-dispersed Seed Deposition along Stream Riparian Gradients in DataverseNL? Maybe Yes No Check your answer! Yes, the Terms of use indicate that there is a Creative Commons license ‘Public Domain Dedication’, which means you can copy, modify, distribute and perform thge work, even for commercial purposes, all without asking permission. hands_on Hands On Can you re-use this weather dataset? Maybe Yes No Check your answer! Maybe, although the website states ‘We hope that you will enjoy using ClimaTempss as much as we have enjoyed developing it!”, there is no clear license or use agreement and directions on how to cite the data are lacking. The use has not been defined nor explained. In this case of re-use you should simply contact the creators. hands_on Hands On Given the follwing description of a dataset: can you assess the usefulness of this dataset to establish cholestasis (an unhealthy condition of the liver) parameters in livers in the age group of puberty through adulthood? Please focus on the context. Description: “We measured livers for several parameters of cholestasis. The subjects were in advanced stages of liver cancer.” Maybe Yes No Check your answer! No, the dataset is not useful because the subjets have cancer. This should affect the values of parameters for cholestasis. You would rather have a dataset of healthy subjects. hands_on Hands On Would you trust the following dataset on heart rate under severe physical stress? Heart rate (beats per minute): 124, 160, 240, 0, 120, 400, 198, 156, 167 Please focus on the trustworthiness. Maybe Yes No Check your answer! No, there are weird values in the dataset, a value of zero is unlikely. And overall, the values are on the high side. hands_on Hands On Is your research likely to be reproducible when you use the following the following infrastructure? The datasets is created during a PhD. Conditions for use state that it is a dataset stored and shared by the PhD student on his university account. Maybe Yes No Check your answer! No, it is unlikely that the dataset can be reused since you do not have certainty that the files stored on the university file drives are availble for at least 10 years which is the current rule for data availablity. Describe what kind of data you will generate Having a clear view of what data you will generate will enable you to plan its management. You can create an overview of the data you produce or collect by drawing the data in a workflow, or noting down in a table. Please watch the video below. Tessa Pronk will explain to you how to describe your data. Order elements in your data flow TODO: add H5P quiz Copyright and Intellectual Property Rights (IPR) issues Copyright is a form of intellectual property right which arises automatically if an original work is created. Copyright may affect the way data may be stored, shared and reused. You should ask yourself who the copyright holder of your datasets is, especially when you use existing data or when you collaborate with external parties. Using someone else’s research data SURF provides a brief guide to determining what consent is necessary to reuse someone else’s data (see “A brief guide … someone else’s data” in the resources below) Clarifying the ownership of your research data TODO: change accordingly for VIB Officially VIB, as your employer, is considered the rights holder to the research data you create. You, as a researcher, have the primary responsibility for taking care of the data. Questions on data exploitation may be even more important than those of ownership. Who can use the data? Who can publish it? Who can provide it to third parties? We strongly recommend that you deal with the issues around data exploitation at an early stage of your research project. Write down agreements between yourself, your supervisor, project members and other interested parties in your Data Management Plan. TODO: change accordingly RDM Support offers you a Guide to legal instruments and agreements for research data management (see the Guide ‘Legal instruments and agreements’) Confidential or privacy-sensitive data When your research project has received data under confidentiality or under legal privacy restrictions, you will have to identify and explain how you will deal with these restrictions in your data management plan (also see ‘Learning Unit: Handle - Data security’). Costs involved with managing your data TODO: https://www.uu.nl/en/research/research-data-management/guides/costs-of-data-management The costs of data management and sharing activities must be included into your research, in terms of time and resources needed. 1. Data Management Cost Guide When you plan your research you may not be able to oversee all costs involved. Nevertheless, it is useful to have an idea of possible costs at an early stage. You can use the Guide ‘Costs of Data Management’, which is a practical overview of possible costs per activity within each phase of the research process. Note: The Cost Guide offers cost indications and examples. These are not real prices. 2. Budget your data management costs You are advised to budget the data management costs as separate data management costs. These costs are eligible for funding with funders like NWO and the European Commission, as long as the costs are invoiced before the end of the project. 3. Planning can save time and money Planning an early start for certain activities within your research project can lower the costs for data management in the run of your project. You can save time by: Properly describing your data while collecting it, instead of doing it afterwards Choosing the right file format so that file conversion afterwards is not necessary Hiring an experienced data manager Spending time to think about data activities beforehand can help prevent unexpected extra efforts and costs later on in your research project. Check the current and expected costs for your research data You have just learned that in many parts of a research project there are data related costs. These costs depend on the type and volume of data you produce, analyse and store. TODO: link to file (calculation) https://lll-platform.uu.nl/pluginfile.php/4907/format_elevated/resource/0/Cost%20overview.docx Write your data management plan for your data collection Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module Data collection. You should be able to complete the following questions in the section Data collection: Will you use existing data? What data will you collect or create? How will the data be collected or created? How will you manage rights issues? What are the costs involved in managing and storing your data?"},{"title":"Overview of other training resources collected by UGent RDM team","url":"/topics/data-management-plans/tutorials/other-material/tutorial.html","tags":[],"body":"Useful information & training resources on Research Data Management UGent RDM webpages in Dutch UGent RDM webpages in English Australian National Data Service (esp. “23 (research data) things”) Coursera Mooc “Research Data Management and Sharing” Data Management Training Clearinghouse (registry of RDM learning resources) DataOne (esp. education modules) Digital Curation Centre (esp. How-to Guides & Checklists) Essentials for Data Support EUDAT (esp. training materials) FOSTER training portal MANTRA – Research Data Management Training OpenAIRE webinars RDM open training materials on Zenodo UK Data Service (esp. “Prepare & Manage Data pages) UK Data Service webinars FAIRDOM Knowledge Hub Data4LifeSciences Handbook for Adquate Natural Data Stewardship"},{"title":"01 R basics: installation, help, set up working directory, variables, syntax, scripts","url":"/topics/R/tutorials/Rbasics/tutorial.html","tags":[],"body":"What is R ? R is many things: a project, a language… As a project, R is part of the GNU free software project. The development of R is done under the philosophy that software should be free of charge. This is good for the user, although there are some disadvantages: R comes with ABSOLUTELY NO WARRANTY. This statement comes up on the screen every time you start R. There is no company regulating R as a product. The R project is largely an academic endeavor, and most of the contributors are statisticians, hence the sometimes incomprehensible documentation. As a computer language it was created to allow manipulation of data, statistical analysis and visualization. It is not easy to learn the language if you haven’t done any programming before but it is worth taking the time as it can be a very useful tool. An enormous variety of statistical analyses are available and R allows you to produce graphs exactly as you want them with publication quality. Good things about R It’s free It works on Windows, Mac and Linux It can deal with very large datasets (compared to Excel) A lot of freedom: graphs can be produced to your own taste Supports all statistical analyses: from basic to very complex Bad things about R It can struggle with extremely large datasets Difficult if you don’t have any programming experience Open source: many people contribute thus consistency can be low Open source: documentation can be poor or written by/for experts Can contain bugs and errors: packages that are widely used are probably correct, niche packages can contain errors, there is no central team assessing the quality of the code Installing R R is available on the CRAN website (Comprehensive R Archive Network]. It can be installed on Linux, Mac and Windows. On the top of the CRAN page is a section with Precompiled Binary Distribution: R versions you can download as an .exe file (for Windows users) and are easy to install. What you download is the basic R installation: it contains the base package and other packages considered essential enough to include in the main installation. Exact content may vary with different versions of R. As R is constantly being updated and new versions are constantly released, it is recommended to regularly install the newest version of R. Installing RStudio Although you can work directly in the R editor, most people find it easier to use RStudio on top of R. RStudio is free and available for Windows, Mac and Linux. You need to have R installed to run Rstudio. RStudio user interface Watch this video tutorial on the different components of the RStudio user interface and this video tutorial on how to use the RStudio user interface. The script editor A script is a text file that contains all the commands you want to run. You can write and run scripts and you can also save them so next time you need to do a similar analysis you can change and re-run the script with minimal effort. An R project can contain multiple scripts. The script editor highlights syntax in scripts making it easy to find and prevent errors. It has many features that will help you write scripts e.g. autocompletion, find/replace, commenting. Autocompletion It supports the automatic completion of code, e.g. if you have an object named relfreq in your workspace, type rel in the script editor and it will show a list of possibilities to complete the name. Figure 1: Example for autocompletion Find and replace Find and replace can be opened using Ctrl+F. Adding comments to scripts In scripts you must include comments to help you remember or tell collaborators what you did. Comments are lines that start with a # symbol. This symbol tells R to ignore this line. Comments are displayed in green. You can comment and uncomment large selections of code using: Comment/Uncomment Lines Figure 2: Menu Comment/Uncomment Lines Adding section headings to scripts Add section headings to your scripts using the following format: #Heading Name#### Figure 3: Define section headings At the bottom of the script editor you can quickly navigate to sections in your script. Especially in long scripts this is very useful. Creating a new script Click File in the top menu and select New File > R Script. Figure 4: File Menu / New File Besides a simple R script, there are many other file types you can create: R markdown file: incorporate R-code and its results in a report R Notebook: R Markdown file with chunks of code that can be executed interactively, with output visible beneath the code R Sweave file: incorporate R-code and its results in a Latex report Opening an existing script Click File in the top menu and select Open File. Scripts are opened as a tab in the script editor. You can open several scripts at the same time in RStudio. Running a script To run a script you select the code that you want to execute in the script editor and click the Run button at the top right of the script editor. The code will be executed in the console. Saving a script If there are unsaved changes in a script, the name of the script will be red and followed by an asterisk. To save the script click the Save button: R scripts should have the extension .R Once it is saved the asterisk disappears and the name becomes black. The console The > symbol in the console shows that R is ready to execute code e.g. type 10+3 and press return > 10 + 3 [1] 13 > The result is printed in the console. It is recommended to write commands in a script rather than typing them directly into the console. Creating a script makes it easier to reproduce, repeat and describe the analysis. If you select commands in the script editor and press the Run button, you will see the commands appearing in the console as they are executed. If the > symbol does not reappear upon execution of a command it means that R has crashed or is still calculating. To terminate a command press Esc. The console also has many features that make life easier like autocompletion, retrieving previous commands. Environment A list of all variables (numbers, vectors, plots, models…) that have been imported or generated. The variables that R creates and manipulates are called objects. To remove all variables that have been generated in the RStudio session: > rm(list=ls()) ls() lists the objects in the current workspace and rm() removes them. History An overview of the last 500 commands that were run in the console: see how to use the history. Connections An interface to easily connect to databases in R. Files The list of files and folders in the working directory. RStudio has a default working directory, typically your home folder. Changing the working directory Often you want to work in the folder that contains the data. In that case you can change the working directory. Check which folder R is using as a working directory: > getwd() Change the working directory: > setwd(\"D:/trainingen/zelfgegeven/R/\") comment Comment You need to use / or \\ in paths. Either will work but \\ will not since R sees it as the character that represents a division. Changing your working directory will make relative file references in your code invalid so you type this in the console at the start of the analysis. Alternatively you can change the working directory in the Files tab, expand More and select Set As Working Directory. hands_on Hands-on: Demo Download the demo script for this lesson and open it in RStudio Demo_1.R From the demo script run the Set working directory section hands_on Hands-on: Exercise 1 Set the working directory to the folder that contains the demo script that you have downloaded and check if it was changed. To list the files in the working directory: > list.files() Plots Plots that are generated by the code you run will appear here. To save a plot click the Export button: Packages R is popular because of the enormous diversity of packages. R is essentially a modular environment and you install and load the modules (packages) you need. Packages are available at the CRAN and Bioconductor websites. Installing a package means that a copy of the package is downloaded and unzipped on your computer. If you want to know in what directory R stores the packages, type: >.libPaths() [1] \"D:/R-3.6.0/library\" > to see the default path where R stores packages. If you want to change this folder use the destdir argument of the install.packages() function: > install.packages(\"car\",destdir=\"C:/Users/Janick/R\") You only need to install a package once, as it is saved on your computer. Installing R packages Watch this video tutorial on how to install CRAN packages. When you have made changes to the right side of the Rstudio user interface (packages, files tab…), R is sometimes slow to show these changes. In that case hit the refresh button: Some packages are not available on the CRAN site. Download in compressed format (as a .zip or .tar.gz file) from the source site. To install: select Install from Package Archive File (.zip; .tar.gz) in the Install Packages window and R will put it in the appropriate directory. Figure 5: Installing packages downloaded from their source site Installing Bioconductor packages Bioconductor is a set of R packages that provides tools for the analysis of high-throughput data, e.g. NGS data. Make sure you have the BiocManager package installed: > if (!requireNamespace(\"BiocManager\")) install.packages(\"BiocManager\") The if statement is checking if you already have the BiocManager package installed, if not then install.packages() will install it. BiocManager is a package to install and update Bioconductor packages. Once BiocManager is installed, you can install the Bioconductor core packages: > BiocManager::install() To install additional Bioconductor packages e.g. GenomicFeatures you type the following command: > BiocManager::install(\"GenomicFeatures\") Overview of all available Bioconductor packages and workflows. Installing packages from GitHub Git is a free and open source version control system. Version control helps software developers manage changes to code by keeping track of every change in a special database. If a mistake is made, the developer can turn back the clock and compare earlier versions of the code to fix the mistake. There is an install_github() function in the devtools packageto install R packages hosted on GitHub: > install.packages(\"devtools\") > library(devtools) > devtools::install_github(\"statOmics/MSqRob&copy;MSqRob0.7.6\") Loading packages Each time you want to use a package you have to load it (activate its functions). Loading a package is done by selecting it in the list of installed packages or by typing the following command: > library(\"name_of_package\") If R responds: Error in library(car) : there is no package called 'car' or similar, it means that the car package needs to be installed first. hands_on Hands-on: Demo Run commands of the Installation section of the demo script Help You can find a lot of documentation online: e.g. the getting help section of the R website. R documentation is not easily accessible nor well-structured so it can be a challenge to consult the help files of R packages online. By far the most user-friendly interface for searching the R documentation is the Rdocumentation website. Additional useful links: Documentation of RStudio Quick R by DataCamp: loads of basic and advanced tutorials R-bloggers: R-news and tutorials contributed by bloggers Rseek: Google specifically for R. Google’s R style guide: Programming rules for R designed in collaboration with the entire R user community at Google to make R code easier to read, share, and verify. Access the R documentation in RStudio using commands: help() or ? hands_on Hands-on: Demo From the demo script run the Get help section Viewer Views HTML files that are located on your computer. All RStudio keyboard shortcuts Expressions in R R can handle any kind of data: numerical, character, logical… Character data Character data like “green”, “cytoplasm” must be typed in between single or double quotes: > x x x = 1 > y = 2 > z = x > y is x larger than y? > z FALSE > u = TRUE > v = FALSE > u & v u AND v: FALSE > u | v u OR v: TRUE > !u NOT u: FALSE hands_on Hands-on: Exercise 2a question Question What’s the difference between x=2 and x==2 ? solution Solution The = operator attributes a value to a variable (see next section), x becomes 2. The == is a logical operator, testing whether the logical expression x equals 2 is TRUE or FALSE. hands_on Hands-on: Exercise 2b Check if the words UseR and user are equal. comment R is case sensitive As exercise 2b showed R is indeed case sensitive. Assigning variables A variable allows you to save a value or an object (a plot, a table, a list of values) in R. A value or object is assigned to a variable by the assignment operator v v v = 4 give the same result: a variable called v with value 4 After R has performed the assignment you will not see any output, as the value 4 has been saved to variable v. You can access and use this variable at any time and print its value in the console by running its name: > v [1] 4 You can now use v in expressions instead of 4 > v * v [1] 16 You can re-assign a new value to a variable at any time: > v v [1] \"a cool variable\" R is not very fussy as far as syntax goes. Variable names can be anything, though they cannot begin with a number or symbol. Informative names often involve using more than one word. Providing there are no spaces between these words you can join them using dots, underscores and capital letters though the Google R style guide recommends that names are joined with a dot. Using operators to create variables You can combine variables into a new one using operators (like + or /). Using functions to create variables A function is a piece of code that performs a specific task. Functions are called by another line of code that sends a request to the function to do something or return a variable. The call may pass arguments (inputs) to the function. In other words a function allows you to combine variables (arguments) into a new variable (returned variable). There are lots of built in functions in R and you can also write your own. Even the base package supplies a large number of pre-written functions to use. Other packages are filled with additional functions for related tasks. Calling a function in R has a certain syntax: output p ? ggplot > help(ggplot) This opens the documentation of the function in the Help tab including an overview of the arguments of the function. At the bottom of the documentation page you find examples on how to use the function. The function generates a plot so the plot p is the output of the function. hands_on Hands-on: Demo From the demo script run the Assigning variables section hands_on Hands-on: Exercise 3a Create a variable called patients with value 42 Print the value of patients divided by 2 Create a variable called patients_gr2 with value 24 Print the total number of patients solution Solution patients example(min) hands_on Hands-on: Exercise 3c Calculate and print the sum of patients and patients_gr2 using the sum() function. solution solution: answer sum(patients,patients_gr2) question Question Replace the sum() function with the mean() function. What happens ? solution solution: answer Look at the help of the sum() function. What’s the first argument ? Compare with the first argument of the mean() function question Question Will the code below work ? sum (patients,patients_gr2) question Question Will the code below work ? sum ( patients , patients_gr2 ) Sometimes functions from different packages have the same name. In that case use package::function to specify the package you want to use, e.g. ggplot2::ggplot() where ggplot2 is the name of the package and ggplot() is the name of the function. hands_on Hands-on: Extra exercise 3d Create a variable patients_gr3 with value “twenty” and print the total number of patients solution Solution patients_gr3 <- \"twenty\" patients + patients_gr3 hands_on Hands-on: Extra exercise 3e Create variable x with value 5 Create variable y with value 2 Create variable z as the sum of x and y and print the value of z Print x - y Print the product of x and y and add 2 to it solution Solution x <- 5 y <- 2 z <- x+y z x-y x*y+2 hands_on Hands-on: Extra exercise 3f What is the difference between: correctLogic <- TRUE incorrectLogic <- \"TRUE\" hands_on Hands-on: Extra exercise 3g Is there a difference between: name <- \"Janick\" name <- 'Janick' name <- Janick"},{"title":"Handle: Data Security","url":"/topics/data-management-plans/tutorials/handle-security/tutorial.html","tags":[],"body":"Introduction to data security By now you know more about how to manage your data collection, how to organise and document your research data and where and how to store your data. Now we will take you into the world of keeping data safe and secure. Loss of data, loss of academic career The loss of scientific data can have a devastating impact on careers. Imagine that you loose all of the research data you’ve been diligently collecting for four years. Now imagine the knock-on effect: you won’t get the PhD you’ve been working towards, affecting your future career. This nightmare happened to Billy Hinchen, a biologist at Cambridge University. Listen to his story. Data breaches There are several examples of (mainly online) data storage going wrong, leading to leaks of sensitive and personal information. The picture below shows the biggest cases of data breaches in the past 10 years. They involve some well-known, highly regarded and trusted companies as well as some practices from the academic world. Read about the story Figure 1: Biggest data breaches Prevent unauthorised access Data security may be needed to protect intellectual property rights, commercial interests, or to keep personal or sensitive information safe. Data security involves security of data files, computer system security and physical data security. All three need to be considered to ensure the security of your data files and to prevent unauthorised access, changes, disclosure or even destruction. Data security arrangements need to be proportionate to the nature of the data and the risks involved. Attention to security is also needed when data are to be destroyed. If data destruction is in order, you need to make sure that the destruction process is irreversible. Learn about different measures depending on the kind of security you need. Security of data files The information in data files can be protected by: Controlling access to restricted materials with encryption. By coding your data, your files will become unreadable to anyone who does not have the correct encryption key. You may code an individual file, but also (part of) a hard disk or USB stick Procedural arrangements like imposing non-disclosure agreements for managers or users of confidential data Not sending personal or confidential data via email or through File Transfer Protocol (FTP), but rather by transmitting it as encrypted data e.g. FileSender Destroying data in a consistent and reliable manner when needed Authorisation and authentication: for personal data you have to give very selective access rights to specified individuals. Computer security systems The computer you use to consult, process and store your data, must be secured: Use a firewall Install anti-virus software Install updates for your operating system and software Only use secured wireless networks Use passwords and do not share them with anyone. Do not use passwords on your UU computer only, but also on your laptop or home computer. If necessary, secure individual files with a password. Encrypt your devices (laptop, smartphone, USB stick/disk). Physical data security With a number of simple measures, you can ensure the physical security of your research data: Lock your computer when leaving it for just a moment (Windows key + L) Lock your door if you are not in your room Keep an eye on your laptop Transport your USB stick or external hard disk in such a way that you cannot lose it Keep non-digital material which should not be seen by others, in a locked cupboard or drawer. Data classification TODO: what to do with classified data Data that contain personal information These data should be treated with higher levels of security than data which do not. You will learn more about privacy-sensitive data in the e-module. What is your experience with unauthorised access to your research data? TODO: implementation form widget We are interested to know if you have ever experienced unauthorized access to any of your research data. When you give your reply, we will show you an overview with the responses of other researchers in this course. All responses will be processed anonymously. [(1)] No, I am sure about that [(2)] Not that I am aware of [(3)] Yes, without much consequences [(0)] Yes, with severe consequences Legal agreements and contracts Often other people are required to handle your data, or you might be the person that handles other people’s data. To arrange the security of the research data you work with, in many cases you have to make a (legal) agreement with other people involved. These agreements will make explicit permitted uses, retention time, and agreed upon security measures. Find out what legal contracts you can use by studying the figure below. TODO: Visit the Guide ‘Legal instruments and agreements’ for more information For tailored advice and templates, contact Legal Affairs via your faculty Research Support Officer (RSO) TODO: add link Figure 2: Agreements types for data When to use which legal contract? You have been acquainted with the different flavors of legal agreements. Is it clear to you when you need which agreement? Please answer the following questions by choosing the right kind of agreement. TODO: add quiz or H5P quiz Privacy-sensitive data Figure 3: Personal data - learning objectives Privacy in a nutshell Privacy is a fundamental right. With regards to privacy, we all have two perspectives: How is your privacy protected? How can we, as a researcher, protect the privacy of the people involved in our research (the data subjects)? TODO: add link to document and image screenshot Figure 4: Privacy reference card Six principles from the European General Data Protection Regulation 1/2 The European General Data Protection Regulation (GDPR) outlines how we should work with privacy-sensitive data. TODO: create working infographics with images see http://gdprcoalition.ie/infographics Six principles from the European General Data Protection Regulation 2/2 According to the GDPR processing of personal data must be done according to 6 principles. TODO: create HP5 document The GDPR outlines six data protection principles you must comply with when processing personal data. These principles relate to: Lawfulness, fairness and transparency - you must process personal data lawfully, fairly and in a transparent manner in relation to the data subject. Purpose limitation - you must only collect personal data for a specific, explicit and legitimate purpose. You must clearly state what this purpose is, and only collect data for as long as necessary to complete that purpose. Data minimisation - you must ensure that personal data you process is adequate, relevant and limited to what is necessary in relation to your processing purpose. Accuracy - you must take every reasonable step to update or remove data that is inaccurate or incomplete. Individuals have the right to request that you erase or rectify erroneous data that relates to them, and you must do so within a month. Storage limitation - You must delete personal data when you no longer need it. The timescales in most cases aren’t set. They will depend on your business’ circumstances and the reasons why you collect this data. Integrity and confidentiality - You must keep personal data safe and protected against unauthorised or unlawful processing and against accidental loss, destruction or damage, using appropriate technical or organisational measures. Privacy by design To comply with the six principles from the GDPR, you can implement privacy by design. This means that you design a data management plan with measures on both IT and procedural level. Which data breach is breached? Can you recognise the principles that are breached in the different ways personal data is processed? TODO: H5P quiz 7 cases Storing personal data 1/2 Figure 5: Storing personal data Storing personal data 2/2 Only if the access can be unambiguously be restricted to authorised persons, can data be stored without such measures. Should you want an elaborate visualisation of what is considered identifiable data, check out the information sheet at the Future Privacy Forum. Download the visual guide to practical data de-identification Can you recognize identifiable data? question Can you recognize identifiable data? a collection of GPS data of daily routines a list of households sizes associated with number of pets MRI scans without identifying metadata. audio recordings with no metadata and no names of the recorded persons transcripts of interviews without any directly identifying information a list of gender and grades for a de-identified course Check the answers. Answer 1,3, and 4 are correct! GPS data holds information on where people go. In a daily routine, the track ends at a particular location which is likely the home of the subject. AN MRI scan from the profile of the head can be identifiable. Audio recordings can be identifiable from the tone of the voice. A list of surnames in itself is not identifying nor personal information. Access to privacy-sensitive data If and how you can make personal data available, depends n the level of sensitivity of your data. The more sensitive, the more restrictions and safeguards need to be put in place to make sure the data does not fall into the hands of unauthorised persons both during and after research. To determine where the privacy risks lie for your data you will have to do a Data Privacy Impact Assessment (DPIA). For more information: TODO: link to: https://www.uu.nl/en/research/research-data-management/guides/handling-personal-data Towards the data subjects, you need to be transparent regarding the possible reuse, or retaining of the data for verification requirements, and get their prior consent. Cases on how to make personal data accessible Case 1: YOUth cohort study YOUTH COHORT STUDY YOUth (Youth Of Utrecht) is a large-scale, longitudinal cohort following children in their development from pregnancy until early adulthood. A total of 6,000 babies and children from Utrecht and its surrounding areas will be included in two different age groups and followed at regular intervals. The YOUth data enables researchers to look for answers to all sorts of scientific questions on child development. A few examples of YOUth data: human bodily material, hours of videos, MRI images, questionnaires, ultrasounds and IQ scores. YOUth encourages and facilitates data sharing. It is one of the leading human cohorts in FAIR and open data in the Netherlands. More information at: https://www.uu.nl/en/research/youth-cohort-study Case 2: TODO: other example from Wings? An introduction to informed consent In the module ‘Legal agreements and contracts’ you learned about informed consent. Informed consent is very important when working with data which is in any way related to people. TODO: add graphics on informed consent One thing to arrange in your informed consent is the possibility for future use, for verification or reuse. In your informed consent, it is important to be clear on future use of data. Informed consent for data sharing One thing to arrange and to be crystal clear about in your informed consent is the possibility for future use of your data, for verification or reuse. question Question Check the sentences that do permit data sharing if used as a single statement. Any personal information that reasonably could identify you will be removed or changed before files are shared with other researchers or results are made public. Other genuine researchers (may) have acces to tis data only if they agree to preserve the confidentiality on the information as requested in this form. Any data that could identify you will be accessible only to the researchers responsible for performing this study. All personally identifying information collected about you will be destroyed after the study. Check the answers. Answer 1 and 2 are both correct! Sharing of research data that relates to people can often be achieved using a combination of obtaining consent, anonymizing data and regulating data access. If the statement towards the data only mentions the current study, sharing is not explicitly possible. You should add some sentence to make it clear to participants that the data could be used for further research, deidentified where possible, or identifiable with enough safeguards and security measures, if it is not. Write your data management plan for your data security Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module on data security. You should be able to complete the following questions in the section ‘Data security’: Will you use or collect any confidential or privacy-sensitive data? What measures will you take to ensure the security of any confidential or privacy-sensitive data? What measures will you take to comply with security requirements and mitigate risks? To whom will access be granted/restricted?"},{"title":"05 Reading and writing data","url":"/topics/R/tutorials/Reading_Writing/tutorial.html","tags":[],"body":"Reading and writing files Reading files Entering data in R can be done by typing the values when you create a variable. In most cases, however, you will have a file with data that was created by an instrument in your lab. How to import such a file into R? There is a manual available in the R documentation called R Data Import/Export. It’s accessible using help.start() and covers in detail the functionality R has to import and export data. Reading this is highly recommended. This manual covers importing data from spreadsheets, text files, and networks. Reading text files Most instruments put out data in text format: tab-delimited text (.txt) or comma-separated value files (.csv). Both can be easily opened in R. The most convenient method to import data into R is to use the read functions, like read.table(). These functions can read data in a text file. In Notepad you can save such a file as a regular text file (extension .txt). Many spreadsheet programs can save data in this format. Reading means opening the file and storing its content into a data frame. read.table(file,header=FALSE,sep=\"\",dec=?.?,skip=0,comment.char=\"#\") This function has a long list of arguments, the most important ones are: file: path on your computer to the file e.g. D:/trainingen/Hormone.csv If it is stored in the working directory, you can simply use its name. You can also use file=file.choose() to browse to the file and select it. File can be replaced by a url to load a file with data from the internet. header: does the first line of the file contain column names? dec: symbol used as decimal separator sep symbol used as column separator, default is a whitespace or tab skip: number of lines to skip in the file before starting to read data comment.char: symbol to define lines that must be ignored during reading See the documentation for an overview of all the arguments. The output of every read function is a data frame. There are functions to read specific file formats like .csv or tab-delimited .txt files. In the documentation of read.table() you see that these functions are called read.csv() and read.delim(). Both functions call read.table(), but with a bunch of arguments already set. Specifically they set up sep to be a tab or a comma, and they set header=TRUE. read.delim(file,header=TRUE,sep=\"\\t\") On the documentation page, you see that these functions each have two variants that have different default settings for the arguments they take: read.csv( file,header=TRUE,sep= \",\",dec=\".\", ...) read.csv2( file,header=TRUE,sep= \";\",dec=\",\", ...) read.delim( file,header=TRUE,sep=\"\\t\",dec=\".\", ...) read.delim2(file,header=TRUE,sep=\"\\t\",dec=\",\", ...) Originally the CSV format was designed to hold data values separated by commas. In .csv files that are made on American computers this is the case. However, in Europe the comma was already used as a decimal separator. This is why .csv files that are made on a European computer use the semicolon as a separator. For instance, the file below contains a header row and three columns, separated by semicolons. It uses the comma as decimal separator. Patient;Drug;Hormone 1;A;58,6 2;A;57,1 3;B;40,6 Obviously, the file is a European CSV file, to open it use read.csv2() Reading Excel files To import Excel files via a command the easiest way is to let Excel save the file in .csv or tab delimited text format and use the read functions. An easy way to import Excel files is to use the RStudio interface although I prefer to use commands. To use the interface go to the Environment tab and click the Import Dataset button. RStudio can import 3 categories of files: text files, Excel files and files generated by other statistical software. To read .xls or .xlsx files select From Excel. A dialog opens with options on the import. You can import data from your computer (Browse) or from the internet (provide a url and click Update). Click Browse, locate the Excel file and click Open. The Data Preview section shows what the data will look like in R. The Import Options section allows you to specify the import parameters. Name: name of the data frame that will hold the imported data. The default is the name of the file that you are opening. Skip: number of rows at the top of the file to skip during import. Some data formats contain a number of header rows with general info like parameter settings, sample names etc. These rows are followed by the actual data. Skip allows you to skip over the header rows and import the actual data. If the first row of the file contains column names, select First Row as Names Open data viewer shows the data in the script editor upon import Click Import. Behind the scenes RStudio uses the readxl package that comes with the tidyverse package. You can also use the functions of this package directly in commands. Compared to other packages for reading Excel files (gdata, xlsx, xlsReadWrite) readxl has no external dependencies, so it?s easy to install and use on all operating systems. It supports the .xls format and the .xlsx format. The easiest way to install it from CRAN is to install the whole tidyverse package but you have to load readxl explicitly, since it is not a core tidyverse package. Once imported into RStudio the data is stored in a data frame and you can use it as input of commands. The data frame appears in the list of Data in the Environment tab. Figure 1: Inspect Variables and Data Frames in the Environment tab If you want to view the data frame you can click its name in the Environment tab and it will appear in a separate tab in the script editor. Figure 2: View file content hands_on Hands-on: Demo From the demo script run the Reading files section hands_on Hands-on: Exercise 17a Import the file GeneEx.csv into a data frame called GeneEx Rename the two last columns Ct1 and Ct2 Create a new column containing the average Ct: (Ct1+Ct2)/2 solution Solution GeneEx 0) and a table of downregulated genes and store them in data frames called up and down. How many up- and downregulated genes are there? What is the gene with the highest log2 fold change? What is the data of the gene with the lowest adjusted p-value (= padj)? Write the Ensembl IDs (= row names) of the upregulated genes to a file called up.txt. You will use this file for functional enrichment analysis using online tools like ToppGene,EnrichR? These tools want a file with only Ensembl IDs as input (one per line, no double quotes, no column headers, no row names). solution Solution DE 0,] down 0) question Question What’s the difference between these 2 commands ? which.max(up$log2FoldChange) max(up$log2FoldChange) question Question Will this command write Ensembl IDs and log fold changes ? toprint <- as.data.frame(up$log2FoldChange) write.table(toprint,file=\"up.txt\",quote=FALSE,col.names=FALSE) hands_on Hands-on: Extra exercise 17c Which type of files are imported by read.delim ? solution Solution Check the documentation and look at the default for sep hands_on Hands-on: Extra exercise 17d Read the file ALLphenoData.tsv into a variable called pdata using one of the read functions What type of data structure is pdata ? What are the names of the columns of pdata ? How many rows and columns are in pdata ? solution Solution pdata <- read.delim(\"Rdata/ALLphenoData.tsv\") class(pdata) colnames(pdata) dim(pdata)"},{"title":"02 Matrices, data frames and lists","url":"/topics/R/tutorials/Matrices_Dataframes_Lists/tutorial.html","tags":[],"body":"Data structures in R The power of R lies not in its ability to work with simple numbers but in its ability to work with large datasets. R has a wide variety of data structures including scalars, vectors, matrices, data frames, and lists. Matrices A matrix is a table, the columns are vectors of equal length. All columns in a matrix must contain the same type of data. The top row, called the header, contains column labels. Rows can also have labels. Data values are called elements. Indices are often used as column and row labels. Creating a matrix To create a matrix M use the matrix() function M 3] Data frames Just like a matrix, a data frame is a table where each column is a vector. But a data frame is more general than a matrix: they are used when columns contain different data types, while matrices are used when all data is of the same type. comment Comment R has a number of built-in data frames like mtcars. Creating a data frame To create a data frame D use the function data.frame() with the vectors we want to use as columns: D Plant_study[Plant_study$Plants > 2,\"Days\"] > subset(Plant_study,Plants > 2,Days) question Question What will happen when you run this code ? Plant_study[Plant_study[\"Plants\"] > 2,\"Days\"] hands_on Hands-on: Extra exercise 10d Create vector q by extracting the a column of data frame ab (exercise 9) with and without subset(). Retrieve the second element of column a of data frame ab Add column c with elements 2,1,4,7 to data frame ab solution Solution q <- ab$a subset(q,select=a) ab$a[2] ab$c <- c(2,1,4,7) Removing elements from a data frame To remove elements from a data frame use negative indices just as in a vector e.g. to remove the second row from data frame D use: D <- D[-2,] comment Comment The minus sign only works with numbers not with column labels. To remove columns based on labels assign them to NULL: D$genome <- NULL comment Comment Setting a column to NULL is done via an assignment so the removal is permanent. comment Comment Insteading of removing elements you can also define the elements you want to keep. hands_on Hands-on: Demo From the demo script run the Data removal: data frames section Reordering columns in a data frame Reordering columns is a special case of retrieving columns, e.g. for a data frame that has 4 columns you can switch the position of the second and third column as follows: D2 <- D[ ,c(1,3,2,4)] comment Comment The first comma means keep all the rows, and the 1,3,2,4 refer to column indices. You can use indices or labels to refer to the columns. You can also use subset(): D2 <- subset(D,select=c(1,3,2,4)) hands_on Hands-on: Demo From the demo script run the Column reordering: data frames section hands_on Hands-on: Exercise 11a Switch the position of the second and the third column of Drug_study solution Solution Drug_study[,c(1,3,2)] question Question What will happen when you run this code ? subset(Drug_study,select=c(1,3,2)) Lists A list is an ordered collection of objects (of any data type: string, numbers, vectors, matrices, data frames). Lists can even contain other lists as objects! A list allows you to gather a variety of objects under one name. It is not mandatory but very useful to give each object in a list a label. Creating a list To create a list L use the list() function: L <- list(label1=object1,label2=object2,label3=object3) hands_on Hands-on: Extra exercise 12a Create a list called myList with the following objects: 5, 6, the word seven, the matrix mat. Print the list. solution Solution myList<-list(5,6,\"seven\",mat) question Question What will happen when you run this code ? subset(Drug_study,select=c(1,3,2)) Referring to the elements of a list Referring to the elements of a list can be done in exactly the same way as for data frames, using row and column indices or labels in between square brackets. However, since a list can contain other lists or data frames you have to use double square brackets [[ ]] to retrieve elements. comment Comment The $ operator also works to access the objects of a list."},{"title":"03 Vectors and factors","url":"/topics/R/tutorials/Vectors_Factors/tutorial.html","tags":[],"body":"Data structures in R The power of R lies not in its ability to work with simple numbers but in its ability to work with large datasets. R has a wide variety of data structures including scalars, vectors, matrices, data frames, and lists. Vectors The simplest data structure is the vector, a single row consisting of data values of the same type, e.g. all numbers, characters, Booleans… Creating a vector The function c() (short for “combine values” in a vector) is used to create vectors. The only arguments that need to be passed to c() are the values that you want to combine into a vector. You can create a numeric (a), character (b) or logical (c) vector: a Plants_with_lesions days to then subtract increment, if from 5 & x x > 5] question Question What will happen when you run this code ? x(x > 5 & x 5] & x[x 2] question Question What will happen when you run this code ? days[4,5] question Question What will happen when you run this code ? days[4:5] question Question What will happen when you run this code ? days(4:5) hands_on Hands-on: Extra exercise 5c Create vector y with elements 9,2,4 and retrieve the second element of y. solution Solution y 100] hands_on Hands-on: Demo From the demo script run the Logical and arithmetic operations on variables section hands_on Hands-on: Extra exercise 5h Retrieve elements from newVector (exercise 4b) that are larger than the corresponding elements of vector threes (exercise 4d). solution Solution newVector[newVector > threes] Removing, changing or adding elements in a vector To remove an element from a vector use a negative index: ?-? indicates ?NOT? followed by the index of the element you want to remove, e.g. to remove the second element of vector z use: z <- z[-2] Change or add elements by assigning a new value to that element . hands_on Hands-on: Demo From the demo script run the Data removal vectors section hands_on Hands-on: Exercise 6a From vector x (exercise 5a) remove the first 8 elements and store the result in x2. solution Solution x2 <- x[-(1:8)] x2 question Question What will happen when you run this code ? x2 <- x[-1:8] hands_on Hands-on: Extra exercise 6b Retrieve the same elements from z as in exercise 5d2 but first replace the 3rd element by 7. solution Solution z[3] <- 7 z[3:7] Factors You can tell R that a variable is categorical (= text labels representing categories although sometimes numbers are also used) by making it a factor. The difference between a categorical variable and a continuous variable is that a categorical variable represents a limited number of categories. A continuous variable is the result of a measurement and can correspond to an infinite number of values. In most cases categorical data is used to describe other data, it is not used in calculations e.g. which group does a measurement belong to. Storing data as factors ensures that the graphing and statistical functions in R will treat such data correctly. There are two types of categorical data: unranked categorical data do not have an implied order ranked categorical data do have a natural ordering R will treat factors by default as unranked but you can create ordered (ranked) factors. To create a factor, first create a vector and then convert it to a factor using the factor() function: v <- c(1,4,4,4,3,5,4,4,5,3,2,5,4,3,1,3,1,5,3,4) v #[1] 1 4 4 4 3 5 4 4 5 3 2 5 4 3 1 3 1 5 3 4 f <- factor(v,ordered=TRUE) f #[1] 1 4 4 4 3 5 4 4 5 3 2 5 4 3 1 3 1 5 3 4 #Levels: 1 < 2 < 3 < 4 < 5 comment Comment The factor() function creates “Levels”: these are the labels of the categories. The only required argument of the factor() function is a vector of values which will be factorized. Both numeric and character vectors can be made into factors but you will use factor() typically for numerical data that represents categories. When you create a vector containing text values in R you have to factorize it but if you store the vector as a column in a data frame, text data is automatically converted to a factor. When you import data into R using read.() functions, the data is automatically stored in a data frame so text will be automatically converted into a factor. So in reality (since you mostly import data into R) you use factor() mainly to factorize numbers that represent categories. By default, factor() transforms a vector into an unordered factor, as does the automated factorization of the read.() functions. Unordered means that the categories are processed in alphabetical order: High will be plotted before Low since H comes first in the alphabet. If the categories are ranked, you have to create an ordered factor, you have to add two additional arguments: Set ordered to TRUE to indicate that the factor is ordered levels: a vector of category labels (as strings) in the correct order hands_on Hands-on: Demo From the demo script run the Data creation: factors section hands_on Hands-on: Extra exercise 7a Create a vector gender with the following elements: Male, Female, male. Convert gender into a factor with levels: Male and Female Print the content of the factor. What happens? solution Solution gender <- c(\"Male\",\"Female\",\"male\") gender <- factor(gender,levels=c(\"Male\",\"Female\")) gender"},{"title":"Prepare: Data documentation","url":"/topics/data-management-plans/tutorials/prepare-document/tutorial.html","tags":[],"body":"Introduction to documentation By now you understand how to describe your data collection in terms of, for example, type, size, and format. You have identified this for your own research data. Now we will look into the documentation and metadata which will accompany your data. Documentation and metadata are essential to understand what a dataset means and to make it reusable in the future. Figure 1: Why document your data: learning objectives Tips for data documentation - John MacInnes, professor of Sociology of the University of Edinburgh, explains why it is necessary to document each step of your research and how this will benefit you in the long term. Examples of data documentation Since there is a wide variety of types of data and types of research, there are many different ways of documenting data. A few examples of data documentation are: Laboratory notebooks and experimental procedures Questionnaires, codebooks, data dictionaries Software syntax and outout files; Information about equipment settings & instrument calibrations Database schemes Methodology reports Provenance information about sources of derived or digitised data question Question What data documentation will you use and why? Feedback on your reflections Data documentation has as goal to be used by people to understand the dataset. Such as specific conditions in which it was collected, what each column means and which methods were used to collect the data. When creating documentation, you need to ask yourself, can others (or I, myself) understand my dataset if I give them this information. There are many different ways to set up and organise your documentation. Project level Project level documentation documents what the study sets out to do; how it contributes to new knowledge in the field, what research questions/hypotheses are, what methodologies are used, what samples are used, what intruments and measures are used, etc. A complete academic thesis normally contains this information in details, but a published article may not. If a dataset is shared, a detailed technical report needs to be included for the user to understand how the data were collected and processed. You should also provide a sample bibliographic citation to indicate how you would like secondary users of your data to cite it in any publication. File or database level File or database level documentation documents how all the files (or tables in a database) that make up the dataset relate to each other, what format they are in, whether they supersede or are superseded by previous files, etc. A readme.txt file is the classic way of accounting for all the files and folders in a project. Variable or item level Variable or item level documentation documents how an object of analysis came about. For example, it does not just document a variable name at the top of a spreadsheet file, but also the full label explaining the meaning of that variable in terms of how it was operationalised. John MacInnes, professor of Sociology of the University of Edinburgh, speaks about how data documentation can help to find a way in often voluminous data collections of different copies, routings, syntaxes, samplings, etc. On the necessity of data documentation in secondary data analysis question Question Looking back at your previous research project: Did you ever have problems reusing other people’s data because of lack of documentation? Never tried Successfully reused Had to ask clarification Had to abandon the reuse attempt Feedback on your reflections Data documentation always provides advantages for yourself and for others such as better understandability, sharability and reusability in the future. Figure 2: Laboratory Notebooks for documentation Thorough and effective management of laboratory data and the routine documentation of all lab procedures is a highly important responsibility for all researchers. If you want to learn more about the electronic lab notebook system at VIB, please see these tutorials An introduction to metadata Watch this web lecture to learn about the different types of metadata and how metadata can help make your research data better findable. You are pointed to useful sources for metadata standards. identify different types of metadata TODO: HP5 quiz or matrix quiz Metadata for different disciplines Different disciplines like biology, earth sciences, physical sciences and social sciences and humanities have their own standards. By choosing a well-supported standard, you will maximise the chance that your data can be re)used and understood by other researchers. Metadata for different disciplines Useful links to metadata standards: Biology General Sciences A community-maintained directory of metadata schemas which has been set up under the auspices of the Research Data Alliance. A list of metadata standards and other standards developed by FairSharing. Controlled vocabulary Improve a record description question Question Take a look at the record descriptions n the table below and answer the question below and in the following pages. Soil Sample Condition Length Classx A1 low $458 III A2 low $391 II A3 medium $422 IV x according to the classification from last experiment Is the value of in the Soil sample column clear? Click your answers! Yes, it is sufficient to say this is a sample. The identifier for the sample needs to be unique, the content of the sample comes from the other metadata fields and their values. question Question Take a look at the record descriptions n the table below and answer the question below and in the following pages. Soil Sample Condition Length Classx A1 low $458 III A2 low $391 II A3 medium $422 IV x according to the classification from last experiment Is the value in the COndition column clear? Click your answers! No! It is not clear what low or medium as condition means. question Question Take a look at the record descriptions n the table below and answer the question below and in the following pages. Soil Sample Condition Length Classx A1 low $458 III A2 low $391 II A3 medium $422 IV x according to the classification from last experiment Is the value in the Length column clear? Click your answers! No, it is not clear what is meant by length. Also a unit for the values is missing. Is it meters, centimeters, or seconds? question Question Take a look at the record descriptions n the table below and answer the question below and in the following pages. Soil Sample Condition Length Classx A1 low $458 III A2 low $391 II A3 medium $422 IV x according to the classification from last experiment Is the value in the Class column clear? Click your answers! No! There is a reference that the classes are explained somewhere. But no link to the document is given. Data standards explained Your dataset can be standardised in various aspects. Standardisation, in general, makes data comparable and interpretable. In other words, your data becomes interoperable by applying standards. Datasets can be combined, compared or are simply easier to reuse. You have to plan standardisation, as it is for many aspects hard or impossible to apply afterwards. Standardise as much as possible between you and your collaborators or research group. If there are standards established and used in your field of research you are advised to use these. Here is a list of things you can standardise in your research. Standardise how, what and when you measure things by standardising your protocol, or methods and materials For instance, is there a standard set of questions for ‘quality of life’? Is there a standard procedure to house mice for your purpose? What aspects do you measure? At what parameter values (age, concentration, etc.)? When do you measure (every two hours, every gram of weight gain, etc.)? Standardise your file formats so you can easily exchange results without technical difficulties. Check for standard taxonomies or coding systems within your research discipline. Standardise the units in which you note down your results. For instance, do you use mm, cm, m? It is extra work to transform units between experiments. Standardise the metadata you use to describe your records or study. What fields will fill in by default, and according to what standard do you define the fields’ names? Will you design a metadata spreadsheet where you specify all things that you will note down? Standardise the vocabulary you use. If everyone has the same terminology, it can avoid confusion or misinterpretation. Check for standard taxonomies or coding systems within your research discipline. Check your knowledge on standards Follow the links below for examples of standards. What type of standardisation do the links refer to? Demographic market research Find via Google: “general morphology score (GMS)” Marine Geoscience Data International Union of crystallography The Cultural Objects Name Authority) SI Units UK data service TODO: add H5P exercise Folder structure and file naming Figure 3: Folder structure - learning objectives CC BY: https://mantra.edina.ac.uk/ Figure 4: Introduction to good file management Trying to find a data file that you need which has been stored or named incorrectly or inaccurately can be both frustrating and a waste of valuable time. In this short video Jeff Haywood, professor at the University of Edinburg, explains his experiences with good and bad file management. Project level Project level documentation documents what the study sets out to do; how it contributes to new knowledge in the field, what research questions/hypotheses are, what methodologies are used, what samples are used, what intruments and measures are used, etc. A complete academic thesis normally contains this information in details, but a published article may not. If a dataset is shared, a detailed technical report needs to be included for the user to understand how the data were collected and processed. You should also provide a sample bibliographic citation to indicate how you would like secondary users of your data to cite it in any publication. File or database level File or database level documentation documents how all the files (or tables in a database) that make up the dataset relate to each other, what format they are in, whether they supersede or are superseded by previous files, etc. A readme.txt file is the classic way of accounting for all the files and folders in a project. Variable or item level Variable or item level documentation documents how an object of analysis came about. For example, it does not just document a variable name at the top of a spreadsheet file, but also the full label explaining the meaning of that variable in terms of how it was operationalised. question Choose the best chronological file name Which of the file names below is the most appropriate? 2019-03-24_Attachment 24 March 2006 Attachment 240306attach Click your answers! 2019-03-24_Attachment is correct! Using a date in the format Year-Month-Day will maintain the chronological order of your files. question Choose the best descriptive file name Which of the file names below is the most appropriate? labtox_recent_110810_old_version.sps 2010-08-11_bioasssay_tox_V1.sps FFTX_3776438656.sps Click your answers! 2010-08-11_bioasssay_tox_V1.sps is correct! Keep the file names short and relevant while using sufficient characters to capture information. Do not name files recent or final or definitive_final, a date or version number will suffice. Figure 5: Batch renaming Figure 6: Suggestions for version control How would you treat your data question Choose the best descriptive file name Why should you discard or delete obsolete versions of data? The most current version is the only relevant version. You have several versions of files in a state between versions You are exceeding the storage space available to you. Click your answers! Correct answer: You have several versions of files in a state between versions! Too many similar or related files may be confusing to yourself and to anyone else wanting to access or use your data. You may think that you know which data file is which but that may not always be the case as time passes and the number of different versions increases. It is easier to maintain a manageable number of versions with a clear naming structure. As long as the original raw or definitive copy is retained and processing is well documented, the intermediate working files can and should be discarded. Fill the blanks TODO: add H5P Write your data management plan for your data documentation Go to DMPonline and open your draft data management plan created in the Introduction. You have now completed the module Data documentation. You should be able to complete the following questions in the section Data documentation: How will you structure your data? How will the data be described and documented? What standards will you use?"},{"title":"05 Analyzing data from different qPCR experiments over time","url":"/topics/qbase-plus/tutorials/multiple-experiments/tutorial.html","tags":[],"body":"You need to do inter-run calibration if you want to compare samples from different runs e.g.: when it is not possible to get all samples for the same gene on the same plate when you do additional runs weeks or months after your initial experiment Of course there is a lot of variability between runs on a qPCR instrument: thermal block is not always heating uniformously quality of the lamp, the filters and the detector decreases over time data analysis settings on the qPCR instrument (baseline correction and threshold) can be slightly different efficiency of reagents (polymerase, fluorophores) is variable optical properties of the plastic plates vary Fortunately, inter-run calibration allows you to eliminate most of this variability. In this experiment we will analyze the data from the gene expression experiment (see Analyzing gene expression data in qbase+) together with data from 2 runs (Run4 and Run5) that were done weeks after the initial gene expression experiment. Because the data comes from two different experiments spread over time, we have included three inter-run calibrators on the plates: Sample01, Sample02 and Sample03. The principle of the IRCs is very similar to that of the reference genes: In theory, the IRCs should have the same NRQ in each run. In practice, the difference in NRQ between two runs is a measure of the inter-run variation and can be used to adjust the NRQs to remove the inter-run variation. Creating a new Experiment | Import Run1, Run2, Run3(all three in CFX format), Run4 and Run5 (the latter two are in qBase format). | :—————————– | | Since the data is in files of two different format, you have to do a separate import for each format. So first import Run1, Run2 and Run3, then import Run4 and Run5. You can find the details on how to import CFX files in Loading data into qbase+. The details of importing qBase files are in Analyzing data from a geNorm pilot experiment in qbase+ Analyzing the data Use assay specific amplification efficiencies. You can find the details on how to convert the targets in the Taking into account amplification efficiencies section of Analyzing gene expression data in qbase+ In Analyzing gene expression data in qbase+ we have already checked the stability of the reference genes (see Normalization section). We determined that Flexible did not show stable expression. Convert Stable and Nonregulated to Reference targets. You can find the details on how to convert the targets in the Normalization section of Analyzing gene expression data in qbase+ Appoint Sample01, Sample02 and Sample03 as IRCs. Leave the Analysis wizard by clicking the Close wizard button in the top menu. Expand Intermediate results (red) in the Project Explorer Double click Interrun calibration (green) This opens the Interrun calibration window: Click the New button (blue) to create a IRC Once the IRC is created you have to appoint samples to it: select Sample01 in the list of Other samples Click the Add Sample button (purple) Remember that you cannot give IRCs the same name in different runs: the software would think that they are technical replicates spread over different plates (which is not allowed). Therefore, in Run4 and Run5 we have given Sample01 another name: Sample01_2. Select Sample01_2 in the list of Other samples Click the Add Sample button (purple) You have appointed the first IRC (grey), now do the same for the other two IRCs. Remember that for each target the variability of the normalized expression levels of the IRCs between different runs will be used to adjust the other normalized expression levels of that target gene. The adjustment is done by amplifying the normalized expression levels with a calibration factor that is calculated based on the normalized expression levels of the IRCs. Since variability between runs is the same for each IRC, you expect that all IRCs measure the variability between the runs to the same extent, hence leading to similar calibration factors. Do these IRCs generate similar calibration factors ? Open the Calibration Factors tab (red) of the Interrun calibration window and look at the result for Duvel: You see that IRC2 returns a substantially different calibration factor in Run5 (green) so the validity of this IRC should be interpreted with care. For Leffe the IRCs also gives inconsistent results in Run5. Switch to the results for Leffe by selecting Leffe in the Targets list (blue) | Do you still see the same expression pattern for Palm as you did in the first three runs ? | :—————————– | | Open the target bar chart for Palm. You see that the pattern Palm showed in the first three runs (sample01 to sample16): high expression in the odd and low expression in the even samples is reversed in the samples from Run4 and Run5 (sample17 to sample25). In the latter runs you see high expression in the even and low expression in the odd samples. However, without annotation for Run4 and Run5 (which samples are treated and which not) it’s impossible to interpret the bar chart. Link"},{"title":"Introduction to Data Management Plans","url":"/topics/data-management-plans/tutorials/Introduction/tutorial.html","tags":[],"body":"Why manage your research data? In this video Katarzyna Biernacka explains what data in a research context is. CC-BY-4.0: Katarzyna Biernacka, HU Berlin & Discipline Workshops 2019 Managing your data effectively is crucial to the success of your research. This doesn’t only apply to the immediate context of your thesis or publications. Managing your data is a practice that will benefit you throughout your research career. The following list gives an overview of what benefits are evident. Access, Re-use & Recognition Facilitating future research by allowing others to build on or add to your research data. Increased citations of research data and of publications based on that data. Efficiency Increasing your research efficiency by saving time and resources. Preventing duplication of effort by enabling others to use your data. Quality & Security Ensuring the integrity and reproducibility of your research. Ensuring that research data and records are accurate, complete, authentic and reliable. Enhancing data security and minimising the risk of data loss. Compliance Meeting legal obligations, restrictions and codes of conduct. Meeting the University policy for research data requirements. Meeting funding body grant requirements. Meeting publisher requirements for data access. A case to consider Marleen is an early career researcher. She completed her PhD about four years ago and is now a postdoctoral research fellow at a different university. Since she obtained her PhD, she has published a number of journal articles based on her doctoral research. Her papers have been cited widely in the literature of her field. But just recently a fellow researcher has questioned her findings. He has gone so far as to suggest that the data on which her research was based is inaccurate. One implication is that the data could even have been falsified. Marleen is confident that her research is valid and that her data is accurate. What steps could Marleen take to verify her research findings? What evidence would she need to demonstrate that she hasn’t falsified her data? Think about your own research. If someone accused you of research misconduct, would you be in a position to defend your research and reputation? List some strategies you could implement right now that would assist you, should you ever find yourself in Marleen’s situation. Data disasters – postcards from the edge The following are real examples where researchers or data centers have lost crucial data. Could any of these ever happen to you? With good planning you could avoid or reduce the impact of such occurrences. TODO: add H5P University policy framework for research data For the Flemish universities, it is important that all researchers honour scientific standards, including the meticulous and ethical treatment of research data. This policy is intended to set out parameters to safeguard the quality, availability and accessibility of research data within any Flemish university. It provides a basis for evaluating compliance with laws, regulations and codes of conduct. The policy also clarifies the various roles and responsibilities of university staff in managing research data. The highlights of the policy are: Archive (relevant and valuable) research data for a minimum of ten years; Store data in a structure that is suitable for long-term preservation and later consultation; Provide metadata to describe the data with sufficient clarity to ensure they are findable for further research; Make archived research data available for access and reuse at and outside VIB insofar as is reasonably possible; Each individual researcher / research leader is responsible to draw up a Data Management Plan (DMP) at the start of the research project and to follow up the agreements made in this plan; Scientific directors are responsible for the implementation and monitoring of the University policy framework and for drawing up additional faculty guidelines to this end if needed. Links to the Policy Frameworks of the Flemish Universities Policy Framework from Ghent University Policy Framework from KU Leuven Policy Framework from UHasselt Policy Framework from VUB Policy in Practise In this short video Prof. dr. Chantal Kemner explains the importance of good data management for Utrecht University. Chantal is full professor of Biological Developmental Psychology in Utrecht at the faculty of social sciences and since 2013 also at the UMCU. Funder requirements More and more research funders explicitly require you to consider the management and publication of your research data, both during and after your research project. The European Commission and the Flemish funders FWO have explicit policies on research data management. European Commission - Horizon 2020 The European Commission wants “Horizon 2020 beneficiaries to make their research data findable, accessible, interoperable and reusable (FAIR), to ensure it is soundly managed. Good research data management is not a goal in itself, but rather the key conduit leading to knowledge discovery and innovation, and to subsequent data and knowledge integration and reuse.” Horizon 2020 is the biggest research and innovation program of the European Commission. FWO FWO states that “FWO has made data management a key element of its policy for all support channels provided by the FWO. The FWO expects researchers to pay due attention to this dimension before, during and for at least five years after their research.” FWO Overview Data Management Plan Funder guidelines and templates Most funders require you to write a Data Management Plan. A DMP outlines all key aspects of collecting, storing and managing research data during and after a project. For this they provide you with guidelines, forms, templates and examples. For more information you can download the documents under Resources or check out the websites. You can also contact your faculty Research Support Office: EC – Horizon 2020: guidelines FWO template Writing a data management plan By now it should be clear that data needs to be properly managed throughout its lifecycle. The most effective way to do this is to create a Data Management Plan (DMP). This will take into account all the stages of the research data lifecycle. As outlined earlier, each individual researcher or research leader is responsible to draw up a data management plan. He or she should do this at the start of the research project. And during the research you should actively follow up on the agreements made in this plan. Think about our early career researcher Sasha (introduced in ‘Why manage your research materials and data?’) who needs to defend herself against accusations of researcher misconduct. As well as defending against misconduct accusations, some additional benefits of creating a data management plan include: Accessing your data more easily; Prioritising and balancing activities relating to research data collection and storage; Mitigating data loss; Reaching agreement between stakeholders about ownership of data; Reducing time and effort in the long term. The good news is that this online training will take you through the necessary steps to create a plan during the subsequent modules. Getting started with DMPonline We offer you DMPonline to create your Data Management Plan. DMPonline is an international online service that guides you in creating a DMP by answering a series of questions about your research project. It allows you to create, share, store, and revise your data management plans online. You will be asked to complete different sections of your DMP as we go through the other modules. As a result you will have written your own data management plan at the end of this course. With DMPonline you can: Write your plan and keep it up-to-date You can easily update your DMP throughout the lifecycle of a project Share plans online DMPonline allows collaborative access, so you can share your DMP with other researchers, within and outside of your university. Create multiple plans You can store different DMPs for different projects. And you can make a copy of a previous plan as the basis for writing a new one. Download plans You can download your DMP in a variety of formats. We recommend that graduate researchers share their data management plans with their supervisor(s). About RDM Support RDM Support provides all kinds of research data management assistance to researchers of VIB in all stages of their research. This can range from one-off individual advice to large-scale infrastructure coordination. You can find an overview of the contact details of the main host institutions for DMP related questions and guidance are as follows: AMS: Bart Cambré (bart.cambre@ams.ac.be) Hogere Zeevaartschool: Marc Vervoort (marc.vervoort@hzs.be) ITG: Ann Verlinden (averlinden@itg.be) KU Leuven: RDM.ub@kuleuven.be UAntwerpen: RDM-support@uantwerpen.be UGent: Myriam Mertens and Annik Leyman (rdm.support@ugent.be) UHasselt: Sadia Vancauwenbergh (rdm@uhasselt.be) Vlerick: Eva Cools (eva.cools@vlerick.com) VUB: dmp@vub.be VIB: bits@vib.be"},{"title":"04 Gene expression analysis","url":"/topics/qbase-plus/tutorials/gene-expression/tutorial.html","tags":[],"body":"Create a project When you use qbase+ for the first time, you can’t do anything unless you create a project to store your experiments in. Create a new Project When you double click the qbase+ icon, the software starts up automatically opens the Start page where you can create a new project by clicking the Create new project button : This will create a new project with a default name like Project 1 . Create an experiment To open actual data (one/more runs) in qbase+, creating a project is not sufficient. You need to create an experiment in this project to hold the run data. Create a new Experiment called GeneExpression in the new project. Select the Create a new qbase+ experiment option in the Start page. Type a name for th new experiment . Click the Next button at the bottom of the page . This will create the experiment. When you leave the Start page, the Import run page is automatically opened allowing you to import the actual qPCR data into qbase+. Loading the data First a few quick words about the data set. We’ll be working with data coming from 3 runs (plates in the qPCR instrument): Run1, Run2 and Run3 The data consist of Cq values for: 3 reference target genes: Stable, Nonregulated, and Flexible 3 target genes of interest: Duvel, Leffe, and Palm each measured twice (= technical replicates) in 16 different samples. Half of the samples have undergone a treatment, half of them are untreated control samples. The data set also contains a series of standard samples consisting of a four-fold dilution series of cDNA for each target gene. These measurements allow to generate a standard curve from which target-specific amplification efficiencies can be calculated. Finally, negative controls (No Template Controls) have been measured. The goal of the analysis is to identify target genes of interest that have different expression levels in the treated samples compared to the untreated control samples. | In GeneExpression load CFX run files Run1, Run2 and Run3. | :—————————- | | Click the Import runs button to open the Import Run window Click the Browse button to go to the directory that stores the files containing the qPCR data Select the 3 run files simultaneously by holding the Ctrl key on your keyboard during the selection in Windows or the command button in MacOSX. Click the Open button Now you go back to the Import Run window, click the Next button (purple) qbase+ tries to recognize the format of the selected import files. If only one format matches the files (as in our case CFX), it is selected and the quick import option is enabled. Click the Finish button. In the Imported run names area on the Import run page you should now see the names of the 3 run files. If these are the correct files, click the Next button at the bottom of the page. Adding annotation to the data When you leave the Import run page, you are redirected to the Sample target list page, which gives you an overview of the targets (= genes) and samples qbase+ detected when reading in the datafiles. Take a look at the data. You see that the list of samples and targets matches the description of the qPCR experiment at the top of this page. The samples in this experiment are divided into two groups: samples that received some kind of treatment and untreated control samples. This information was not included in the run files so qbase+ does not know which sample belongs to which group. However, this is relevant information: in our analysis we are going to compare the expression of our genes of interest between treated and untreated samples. This means that qbase+ needs the grouping annotation to be able to perform the analysis we want to do. So we have to give qbase+ this annotation: we can do this by adding a custom sample property. To do this we need to create a sample properties file with a specific format that is described in the tutorial. You can find the file in the qbase+ folder on the BITS laptops or you can download the file here. How to add the grouping annotation ? To import the file containing to grouping annotation: select Add samples and targets click Import sample list browse to the folder that contains the samples file select the file and click Open click Next In the Importing samples window, you have to tell qbase+ which sample annotation you want to import from the sample properties file In our case we could import Quantities (this annnotation is available in the sample properties file) but the quantities of the standard samples were included in the run files so qbase+ has already imported this annotation from the run files during data import. We definitely need to import the Custom properties since they were not a part of the run files. The Treatment property will tell qbase+ which samples belong to the group of control samples and which samples belong to the group of treated samples. Click the Next button at the bottom of the page to finish the import. At this point you don’t see the custom annotation that you have imported, you will see it later in the analysis during scaling Leaving the Sample target list page takes you to the Run annotation page, where you have to confirm again that the sample and gene names are ok. If this is not the case you can adjust the annotation here. Click the Next button at the bottom of the page Our data file contains all required annotation: Cq values sample and target names sample types quantities for the standard samples grouping of the samples Once runs are imported, you can start analyzing the data. Data consist of Cq values for all the wells. Specifying the aim of the experiment On the Aim page you tell the software what type of analysis you want to do. Different types of analyses require different parameters, parameter settings and different calculations. By selecting the proper analysis type, qbase+ will only show the relevant parameters and parameter settings. Since we are doing a gene expression analysis in this exercise, this the option we should select. Click the Next button on the bottom of the page to go to the Technical quality control page. Checking the quality of technical replicates and controls The Technical quality control page handles the settings of the requirements that the data have to meet to be considered high quality. For instance the maximum difference between technical replicates is defined on this page. If there are technical replicates in the data set, qbase+ will detect them automatically (they have the same sample and target name) and calculate the average Cq value. In theory, technical replicates should generate more or less identical signals. How to set the maximum difference in Cq values for technical replicates ? The quality criterium that the replicates must meet to be included for further analysis is one of the parameters in qbase+. You can set it on the Technical quality control page: The default maximum allowed difference in Cq values between technical replicates is 0.5 Additionally, you can do quality checks based on the data of the positive and negative controls. | How to set quality requirements for the control samples ? | :—————————- | | On the same Technical quality control page you can define the minimum requirements for a well to be included in the calculations: Negative control threshold : minimum allowed difference in Cq value between the sample with the highest Cq value and the negative control with the lowest Cq value: the default is 5 which means that negative controls should be more than 5 cycles away from the sample of interest. Lower and upper boundary : allowed range of Cq values for positive controls. Excluded means that the data are ignored in the calculations. How to check if there are wells that do not meet these criteria ? You can see flagged and excluded data by ticking the Show details… options on the Technical quality control page and clicking the Next button (purple) at the bottom of the page. Qbase+ will open the results of the quality checks for the replicates and the controls on two different tabs. These tabs show lists of samples that failed the quality control criteria. When you open the replicates tab you can get an overview of the flagged or the excluded (purple) wells. Select the failing wells. When the difference in Cq between technical replicates exceeds 0.5, the wells end up in the flagged or failing list. They are included in calculations unless you exclude them by unticking them. You see that the two replicates of Palm in Sample05 have very different Cq values. All other bad replicates are coming from standard samples. If you are finished checking the data quality, click Next to go to the Amplification efficiencies page. Taking into account amplification efficiencies Qbase+ calculates an amplification efficiency (E) for each primer pair (= gene). Genes have different amplification efficiencies because: some primer pairs anneal better than others the presence of inhibitors in the reaction mix (salts, detergents…) decreases the amplification efficiency inaccurate pipetting Qbase+ has a parameter that allows you to specify how you want to handle amplification efficiencies on the Amplification efficiencies page. How to specify the amplification efficiencies strategy you want to use ? Since we have included a dilution series for creating a standard curve in our qPCR experiment, we will select Use assay specific amplification efficiencies Calculate efficiencies from included standard curves Amplification efficiencies are calculated based on the Cq values of a serial dilution of representative template, preferably a mixture of cDNAs from all your samples. Since you know the quantity of the template in each dilution, you can plot Cq values against template quantities for each primer pair. Linear regression will fit a standard curve to the data of each gene, and the slope of this curve is used to calculate the amplification efficiency. How to check the amplification efficiencies of the genes ? Once you have made this selection, qbase+ starts calculating the efficiencies and the results are immediately shown in the calculation efficiencies table. In this way, one amplification efficiency (E) for each gene is calculated and used to calculate Relative Quantities (RQ): ∆Cq is calculated for each well by subtracting the Cq of that well from the average Cq across all samples for the gene that is measured in the well. So ∆Cq is the difference between the Cq value of a gene in a given sample and the average Cq value of that gene across all samples. Cq is subtracted from the average because in this way high expression will result in a positive ∆Cq and low expression in a negative ∆Cq. So at this point the data set contains one RQ value for each gene in each sample. Click Next to go to the Normalization page. Normalization Differences in amplification efficiency are not the only source of variability in a qPCR experiment. Several factors are responsible for noise in qPCR experiments e.g. differences in: amount of template cDNA between wells RNA integrity of samples efficiency of enzymes used in the PCR or in the reverse transcription Normalization will eliminate this noise as much as possible. In this way it is possible to make a distinction between genes that are really upregulated and genes with high expression levels in one group of samples simply because higher cDNA concentrations were used in these samples. In qPCR analysis, normalization is done based on housekeeping genes. Housekeeping genes are measured in all samples along with the genes of interest. In theory, a housekeeping gene should have identical RQ values in all samples. In reality, noise generates variation in the expression levels of the housekeeping genes. This variation is a direct measure of the noise and is used to calculate a normalization factor for each sample. These normalization factors are used to adjust the RQ values of the genes of interest accordingly so that the variability is eliminated. These adjusted RQ values are called Normalized Relative Quantities (NRQs). In qbase+ housekeeping genes are called reference genes. In our data set there are three reference genes: Stable, Non-regulated and Flexible. On the Normalization page we can define the normalization strategy we are going to use, appoint the reference genes and check their stability of expression. How to specify the normalization strategy you want to use ? You can specify the normalization strategy you want to use on the Normalization method page: Reference genes normalization is based on the RQ values of the housekeeping genes Global mean normalization calculates normalization factors based on the RQ values of all genes instead of only using the reference genes. This strategy is recommended for experiments with more than 50 random genes. Random means that the genes are randomly distributed over all biological pathways. Custom value normalization is used for specific study types. This strategy allows users to provide custom normalization factors such as for example the cell count. None means that you choose to do no normalization at all. This option should only be used for single cell qPCR. We have incorporated 3 housekeeping genes in our experiment so we select the Reference genes strategy. How to appoint reference targets ? You have to indicate which targets should be used as reference genes since qbase+ treats all genes as targets of interest unless you explicitly mark them as reference genes on the Normalization method page: We have measured 3 housekeeping genes: Stable, Flexible and Non-regulated so we tick the boxes in front of their names. It’s not because you have appointed genes as reference genes that they necessarily are good reference genes. They should have stable expression values over all samples in your study. Fortunately, qbase+ checks the quality of the reference genes. For each appointed reference gene, qbase+ calculates two indicators of expression stability M (geNorm expression stability value): calculated based on the pairwise variations of the reference genes. CV (coefficient of variation): the ratio of the standard deviation of the NRQs of a reference gene over all samples to the mean NRQ of that reference gene. It is considered that the higher these indicators the less stable the reference gene. Are Flexible, Stable and Nonregulated good reference targets ? M and CV values of the appointed reference genes are automatically calculated by qbase+ and shown on the Normalization method page: The default limits for M and CV were determined by checking M-values and CVs for established reference genes in a pilot experiment that was done by Biogazelle. Based on the results of this pilot experiment, the threshold for CV and M was set to 0.2 and 0.5 respectively. If a reference gene does not meet these criteria it is displayed in red. As you can see the M and CV values of all our reference exceed the limits and are displayed in red. If the quality of the reference genes is not good enough, it is advised to remove the reference gene with the worst M and CV values and re-evaluate the remaining reference genes. Which reference target are you going to remove ? Both the M-value and the CV are measures of variability. The higher these values the more variable the expression values are. So we will remove the gene with the highest M and CV. You can remove a reference gene simply by unticking the box in front of its name. Are the two remaining reference genes good references ? After removing Flexible as a reference gene the M and CV values of the two remaining reference genes decrease drastically to values that do meet the quality criteria. M and CV values that meet the criteria are displayed in green. This exercise shows the importance of using a minimum of three reference genes. If one of the reference genes does not produce stable expression values as is the case for Flexible, you always have two remaining reference genes to do the normalization. See how to select reference genes for your qPCR experiment. So after normalization you have one NRQ value for each gene in each sample. Click Next to go to the Scaling page. Scaling Rescaling means that you calculate NRQ values relative to a specified reference level. Qbase+ allows you to rescale the NRQ values using one of the following as a reference: the sample with the minimal expression the average expression level of a gene across all samples the sample with the maximal expression a specific sample (e.g. untreated control) the average of a certain group (e.g. all control samples): this is often how people want to visualize their results positive control: only to be used for copy number analysis After scaling, the expression values of the choice you make here will be set to 1 e.g. when you choose average the average expression level across all samples will be set to 1 and the expression levels of the individual samples will be scaled accordingly. How to scale to the average of the untreated samples ? You can specify the scaling strategy on the Scaling page. Select Scale to group and set the Scaling group to the untreated samples . This is one of the reasons why you need the grouping annotation. Rescaling to the average of a group is typically used to compare results between 2 groups, e.g. treated samples against untreated controls. After rescaling, the average of the NRQs across all untreated samples is 1 and the NRQs of the treated samples are scaled accordingly. Click Next to go to the Analysis page. Visualization of the results One of the things you can select to do on the Analysis page is viewing the relative expression levels (= scaled NRQs) of each of the genes in a bar chart per gene. It is recommended to visualize your results like this. It is possible to view the relative expression levels of all genes of interest on the same bar chart. You can use this view to see if these genes show the same expression pattern but you cannot directly compare the heights of the different genes because each gene is independently rescaled! How to visualize single gene expression bar charts ? Select Visually inspect results For individual targets on the Analysis page and click Finish How to visualize the expression levels of Palm in each sample ? Select Visually inspect results For individual targets on the Analysis page and click Finish The Target select box allows you to select the gene you want to view the expression levels of. Relative expression levels are shown for each sample. Error bars are shown and represent the technical variation in your experiment (variation generated by differences in amounts pipetted, efficiency of enzymes, purity of the samples…). You see that Palm has a low expression level and a very large error bar in Sample05 because the two replicates of this sample had very different Cq values. You can group and colour the bars according to a property. How to group the bars of Palm according to treatment (so treated at one side and untreated at the other side) In the Grouping section you can specify the property you want to group by. How to view average expression levels in each group ? In the Grouping section you can choose to plot individual samples as shown above but you can also choose to plot group average expression levels. The error bars that you see here represent biological variation and will be used later on in the statistical analysis. The error bars are 95% confidence intervals which means that they represent the range that will contain with 95% certainty the real average expression level in that group of samples. The nice characteristic of 95% confidence intervals is the following: if they do not overlap you are sure that the expression levels in the two groups are significantly different, in other words the gene is differentially expressed if they do overlap you cannot say that you are sure that the expression levels are the same. You simply don’t know if the gene is differentially expressed or not. | Assess the effect of switching the Y-axis to a logarithmic scale for Palm. | :—————————- | | In the Y axis section you can specify if you want a linear or logarithmic axis. As you can see you do not change the expression values, you just change the scale of the Y axis. Switching the Y-axis to a logarithmic scale can be helpful if you have large differences in NRQs between different samples Assess the effect of switching the Y-axis to a logarithmic scale for Flexible. Switch to the bar charts of Flexible. By switching the Y-axis to logarithmic you can now see more clearly the differences between samples with small NRQs. Statistical analysis Once you generate target bar charts you leave the Analysis wizard and you go to the regular qbase+ interface. Suppose that you want to perform a statistical test to prove that the difference in expression that you see in the target chart is significant. At some point, qbase+ will ask you if your data is coming from a normal distribution. If you don’t know, you can select I don’t know and qbase+ will assume the data are not coming from a normal distribution and perform a stringent non-parametric test. However, when you have 7 or more replicates per group, you can check if the data is normally distributed using a statistical test. If it is, qbase+ will perform a regular t-test. The upside is that the t-test is less stringent than the non-parametric tests and will find more DE genes. However, you may only perform it on normally distributed data. If you perform the t-test on data that is not normally distributed you will generate false positives i.e. qbase+ will say that genes are DE while in fact they are not. Performing a non-parametric test on normally distributed data will generate false negatives i.e. you will miss DE genes. Checking if the data is normally distributed can be easily done in GraphPad Prism. To this end you have to export the data. | How to export the data ? | :—————————- | | To export the results click the upward pointing arrow in the qbase+ toolbar: You want to export the normalized data so select Export Result Table (CNRQ): You will be given the choice to export results only (CNRQs) or to include the errors (standard error of the mean) as well . We don’t need the errors in Prism so we do not select this option. The scale of the Result table can be linear or logarithmic (base 10) . Without user intervention, qbase+ will automatically log10 transform the CNRQs prior to doing statistics. So we need to check in Prism if the log transformed data are normally distributed. Additionally, you need to tell qbase+ where to store the file containing the exported data. Click the Browse button for this . Exporting will generate an Excel file in the location that you specified. However, the file contains the results for all samples and we need to check the two groups (treated and untreated) separately. The sample properties show that the even samples belong to the treated group and the odd samples to the untreated group. This means we have to generate two files: a file containing the data of the untreated samples a file containing the data of the treated samples Now we can open these files in Prism to check if the data is normally distributed. | How to import the data of the untreated samples in Prism ? | :—————————- | | Open Prism Expand File in the top menu Select New Click New Project File In the left menu select to create a Column table. Data representing different groups (in our case measurements for different genes) should always be loaded into a column table. Select Enter replicate values, stacked into columns (this is normally the default selection) since the replicates (measurements for the same gene) are stacked in the columns. Click Create Prism has now created a table to hold the data of the untreated samples but at this point the table is still empty. To load the data: Expand File in the top menu Select Import Browse to the resultslog.csv file, select it and click Open In the Source tab select Insert data only Since this is a European csv file commas are used as decimal separators so in contrast to what its name might imply, semicolons and not commas are used to separate the columns in the csv file (you can open the file in a text editor to take a look). In American csv files dots are used as decimal separator and the comma is used to separate the columns. Prism doesn’t know the format of your csv file so you have to tell him the role of the comma in your file. Select Separate decimals Go to the Filter tab and specify the rows you want to import (the last rows are these of the standard and the water samples, you don’t want to include them) Click Import As the file is opened in Prism you see that the first column containing the sample names is treated as a data column. Right click the header of the first column and select Delete | How to check if the data of the untreated samples comes from a normal distribution ? | :—————————- | | Click the Analyze button in the top menu Select to do the Column statistics analysis in the Column analyses section of the left menu In the right menu, deselect Flexible. It’s a bad reference gene so you will not include it in the qbase+ analysis so there’s no point checking its normality (it is probably not normally distributed). In that respect you could also deselect the other two reference genes since you will do the DE test on the target genes and not on the reference genes. Click OK In the Descriptive statistics and the Confidence intervals section deselect everything except Mean, SD, SEM. These statistics is not what we are interested in: we want to know if the data comes from a normal distribution. The only reason we select Mean, SD, SEM is because if we make no selection here Prism throws an error. In the Test if the values come from a Gaussian distribution section select the D’agostino-Pearson omnibus test to test if the data are drawn from a normal distribution. Although Prism offers three tests for this, the D’Agostino-Pearson test is the safest option. Click OK Prism now generates a table to hold the results of the statistical analysis: As you can see, the data for Palm are not normally distributed. Since we found that there’s one group of data that does not follow a normal distribution, it’s no longer necessary to check if the treated data are normally distributed but you can do it if you want to. We will now proceed with the statistical analysis in qbase+. Statistical analyses can be performed via the Statistics wizard. How to open the Statistics wizard ? You can open it in the Project Explorer (window at the left): expand Project1 if it’s not yet expanded expand the Experiments folder in the project if it’s not yet expanded expand the GeneExpression experiment if it’s not yet expanded expand the Analysis section if it’s not yet expanded expand the Statistics section double click Stat wizard This opens the Statistics wizard that allows you to perform various kinds of statistical analyses. Which kind of analysis are you going to do ? On the Goal page: Select Mean comparison since you want to compare expression between two groups of samples so what you want to do is comparing the mean expression of each gene in the treated samples with its mean expression level in the untreated samples. Click Next. How to define the groups that you are going to compare ? On the Groups page: specify how to define the two groups of samples that you want to compare. Select Treatment as the grouping variable to compare treated and untreated samples. Click Next. How to define the genes that you want to analyze ? On the Targets page: specify for which targets of interest you want to do the test. Deselect Flexible since you do not want to include it in the analysis. It’s just a bad reference gene. Click Next. On the Settings page you have to describe the characteristics of your data set, allowing qbase+ to choose the appropriate test for your data. The first thing you need to tell qbase+ is whether the data was drawn from a normal or a non-normal distribution. Since we have 8 biological replicates per group we can do a test in Prism to check if the data are normally distributed. Which gene(s) is/are differentially expressed ? On the Settings page you describe the characteristics of your data set so that qbase+ can choose the ideal test for your data. For our data set we can use the default settings. Click Next. In the results Table you can see that the p-value for Palm is below 0.05 so Palm is differentially expressed. In this example we will analyze data from another expression study with the following characteristics: All samples fit in a single run: Run7 We have the following samples: 5 control samples: control1, control2… 5 treated samples: treated1, treated2… 1 no template control: NTC The expression of the following genes was measured: 2 reference genes: refgene1 and refgene2 2 genes of interest: gene1 and gene2 There are two technical replicates per reaction Creating a new experiment Create a new Experiment called GeneExpression2 in Project1 You can find the details on how to create a new experiment in Creating a project and an experiment Loading the data Import Run7. This file is in qBase format. You can find the details on how to import the data file in the Loading the data into qbase+ section of Analyzing data from a geNorm pilot experiment in qbase+ Adding sample annotation Download the the sample properties file. | Add a custom sample property called Treatment. | :—————————- | | You can find the details on how to add a custom sample property in the Adding annotation to the data section of Loading data into qbase+ Analyzing the data Choose the type of analysis you want to perform.   | Check controls and replicates. | :—————————- | | First set the minimum requirements for controls and replicates You see that 6 replicates do not meet these requirements . Select to Show details and manually exclude bad replicates All negative controls pass the test . Positive controls were not included in this analysis. Qbase+ will now open the results for the failing replicates: as you can see the difference in Cq values between these replicates is not that big. They fail to meet the requirement just slightly. Which amplification efficiencies strategy are you going to use ? You don’t have data of serial dilutions of representative template to build standard curves so the only choice you have is to use the default amplification efficiency (E = 2) for all the genes. Appoint the reference genes as reference targets. You can find the details on how to appoint reference targets in the Normalization section of Analyzing gene expression data in qbase+ Is the stability of the reference genes ok ? In the Reference target stability window the M and CV values of the reference genes are shown in green so the stability of the reference genes is ok. You can find the details on how to check reference target stability in the Normalization section of Analyzing gene expression data in qbase+ Which scaling strategy are you going to use ? Since you have a treated and a control group, it seems logical to use the average of the control group for scaling. You can find the details on how to specify the scaling strategy in the Scaling section of Analyzing gene expression data in qbase+ Look at the target bar charts. In the target bar charts group the samples according to treatment. You can find the details on how to group the samples in the Visualization of the results section of Analyzing gene expression data in qbase+ The samples of each group are biological replicates so you might want to generate a plot that compares the average expression of the treated samples with the average expression of the untreated samples. In the target bar charts plot the group averages instead of the individual samples. In the Grouping section at the bottom of the chart you can select Plot group average: Are there any genes for which you see a clear difference in expression between the two groups ? For gene 1, the mean expression levels in the two groups are almost the same and the error bars completely overlap. When you look at the title of the Y-axis, you see that 95% confidence levels are used as error bars. In case of 95% confidence intervakls you can use the following rules: if they do not overlap: you are certain that the difference between the means of the two groups is significant if they do not overlap: you know nothing with certainty: the means can be different or they can be the same So for gene 1 the means are very close but just based on the plot we may not make any conclusions with certainty. For gene 2, the mean expression levels in the two groups are very different and the error bars do not overlap. So the 95% confidence intervals do not overlap meaning that we can be certain that the difference between the means of the two groups is significant. Use a statistical test to compare the expression levels between the two groups of samples ? You only have 5 replicates per group so you cannot test if the data comes from a normal distribution. Qbase+ will assume they’re not normally distributed and perform a non-parametric Mann-Whitney test. The p-value of gene2 is smaller than 0.05 so it has a statistically significant difference in expression levels in treated samples compared to untreated samples. For gene1 the p-value is 1 so we have no evidence to conclude that the expression of gene1 is different in treated compared to untreated samples. You can find the details on how to compare the means of the two groups in the Statistical analysis section of Analyzing gene expression data in qbase+"},{"title":"01 qbase+ introduction","url":"/topics/qbase-plus/tutorials/qbaseplus-introduction/tutorial.html","tags":[],"body":"qbase+ is software to visualize and analyze qPCR data. It allows you to perform various types of analyses: statistical analysis of gene expression advanced copy number analysis miRNA profiling ChIP-qPCR analysis Installation and licensing You can find the installation instructions on VIB qbase+ support page VIB only offers qbase+ to VIB scientists, you need a valid VIB email address to run the software. Biogazelle (the company who has developed the software) have written a manual with instructions on how to use the software. Download Biogazelle’s user manual. Before you can download the manual you have to log on to the qbase+ website using your qbase+ account. Use your VIB email address for setting up this account. Training material slides Extra clean log10 transformed CNRQs for checking normality in Prism clean untransformed CNRQs for visualization in Prism R script for analysis and visualization log10 transformed CNRQs of control samples for analysis and visualization in R log10 transformed CNRQs of treated samples for analysis and visualization in R"},{"title":"05 Selecting reference genes: exercises","url":"/topics/qbase-plus/tutorials/reference-genes/tutorial.html","tags":[],"body":"Since normalization of qPCR data is based on the assumption that the reference targets have the same expression level in all samples it is crucial that the expression of the chosen reference genes is stable. However, none of the so-called housekeeping genes is universally stably expressed. Genevestigator, both the commercial and the free version, contains a tool, called RefGenes, that allows to identify candidate reference genes that display very stable expression in the context that you are working in, typically a certain tissue of a certain organism. Genevestigator is a platform that contains curated public microarray data from thousands of experiments/conditions. RefGenes allows you to select the conditions that are relevant for you, e.g. mouse liver, human fibroblasts, or Arabidopsis thaliana leaves. In a next step, RefGenes identifies the genes with the most stable expression in the selected conditions. Starting the RefGenes tool | How to start the RefGenes tool ? | | :——————————– | | - Open the RefGenes page. Click start GENEVESTIGATOR Click the Install/Start button This will automatically open a Genevestigator startup page. Keep this page open during the analysis. Closing this page will close Genevestigator. Login. Also for the free version you need to create an account (use your academic email for this since you will need your vib email to get access to the commercial version). Genevestigator is opened automatically The Genevestigator user interface The Genevestigator consists of the following components: Sample Selection panel: to choose the experimental conditions you’re interested in (green) Gene Selection panel: to choose the genes you’re interested in (blue) Center panel shows an overview of all available tools (purple). Once you have selected a tool, the panel will show the results of the analysis that is done by the tool. Home button (red) allows to return to the overview of the tools at any time. The text next to the home button indicates the toolset that you have selected. Click the RefGenes tool at the bottom. Using the RefGenes tool to find reference genes STEP 1: Choose samples from a biological context similar to those in your qPCR expriment | How to choose the samples you want to analyze ? | | :——————————– | | Click the New button in the Sample Selection panel. The selection of samples defines which data are used for the analysis. Select the organism you’re interested in (in this example: human) Select the array type you want to analyze (in this example: human 133_2). For most organisms Genevestigator contains expression data from multiple types of microarrays, e.g. different generations of Affymetrix GeneChips®. On these arrays, genes are sometimes represented by different sets of probes. To keep the analysis results easily interpretable, data from different array types are not mixed. Click the Select particular conditions button to select all samples with a certain annotation, e.g. all data from a certain tissue type. Select the type of conditions (red) you want to base your selection on (in this example: Anatomy). For each type (anatomy, neoplasms, perturbations, development…) you can browse the corresponding ontologies and select the desired condition(s) (green) (in this example: cardiac muscle). Click OK Note that you can select multiple tissues. When you select samples for use in the RefGenes tool, you have to focus on microarrays from samples that were collected in conditions similar to those in your qPCR experiment. Don’t make a too general selection, e.g. all human samples: you might end up with genes that are stable in most conditions but not in yours. Don’t make a very specific selection either, e.g. human heart samples from patients taking the same medication as yours. If you want to broaden your study later on with samples from other patients, your reference genes might not be valid anymore. It is recommended to select reference genes in the same organism and the same / a similar tissue type as the one that you used in your experiments. STEP 2: Select the gene(s) you want to measure in your qPCR experiment This step is not essential, but it helps you to see whether your target gene(s) is (are) strongly or weakly expressed in the conditions of interest selected in STEP1. This allows you to search for candidate reference genes in a similar range of expression. | How to choose the genes you want to analyze ? | | :——————————– | | Click the New button in the Gene Selection panel. Enter the name of your target gene in the text area (in this example: GOT1) and click OK Open the RefGenes tool (if you haven’t done that already). A red box plot representing the distribution of the expression levels of GOT1 in the 68 selected human heart samples appears in the center panel. As you can see, this gene is highly expressed in heart. STEP 3: Find candidate reference genes The reference genes that are suggested by GeneVestigator have the following characteristics: They have the most stable expression levels across all selected samples (a small boxplot) Their overall expression level is similar to that of the target gene(s) of your qPCR experiment | How to find the candidate reference genes ? | | :——————————– | |Click the Run button in the RefGenes tool. RefGenes will show the top 20 most stable genes with similar expression levels: Exercises Finding candidate reference genes in the free version of Genevestigator Now we will make a more elaborate exercise on finding candidate reference genes. We will do the analysis in the free version of RefGenes but the analysis in the commercial version is very similar. Suppose we want to compare the expression stability of the 4 commonly used reference genes for qPCR on mouse liver samples (ACTB, GAPDH, HPRT and TUBB4B) to that of 4 reference genes that are suggested by Genevestigator. To this end we open the RefGenes tool and select the liver samples of the mouse 430_2 arrays. | Check the expression stability of the 4 commonly used reference genes ? | | :——————————– | | Click the New button in the Gene Selection panel to create a new selection. The selection of samples defines which data are used for the analysis. Enter the name of your target gene in the text area (for example: ACTB) and click OK When you are using the commercial version, you may enter multiple genes at the same time, in the free version you have to enter them one by one. This means that you have to add the first gene as described above and then add the next gene by clicking the Add button and so on… Finally you end up with an expandable list of the genes you asked for and you can tick or untick them to control the display of their expression data in the main window. When you tick the 4 commonly used reference genes you can see how stable they are expressed in the 651 mouse liver samples that are stored in Genevestigator: As you can see, the expression levels of the commonly used reference genes in the selected mouse liver samples is pretty variable which is also confirmed by their relatively high SD values. Often there are multiple probe sets for the same gene. When you use the free version you may only choose one probe set per gene so you have to make a choice. How to make that choice ? Affymetrix probe set IDs have a certain meaning: what comes after the underscore tells you something about the quality of the probes: _at means that all the probes of the probe set hit one known transcript. This is what you want: probes specifically targeting one transcript of one gene _a_at means that all the probes in the probe set hit alternate transcripts from the same gene. This is still ok the probes bind to multiple transcripts but at least the transcripts come from the same gene (splice variants) _x_at means that some of the probes hit transcripts from different genes. This is still not what you want: the expression level is based on a combination of signals of all the probes in a probe set so also probes that cross-hybridize _s_at means that all the probes in the probe set hit transcripts from different genes. This is definitely not what you want: if the probes bind to multiple genes you have no idea whose expression you have measured on the array So I always ignore probe sets with s or x. If you have two specific probe sets for a gene, they should more or less give similar signals. If this is not the case, I base my choice upon the expression level that I expect for that gene based on previous qPCR results. As you can see, each of these 4 commonly used reference genes has a high expression level. Most genes do not have such high expression levels. In most qPCR experiments your genes of interest will have low or medium expression levels, so these reference genes will not be representative for the genes of interest. Reference genes should ideally have similar expression levels as the genes of interest. Therefore, we will select the four most stably expressed genes with a medium expression level (between 8 and 12) according to the RefGenes tool. | Select the 4 most stably expressed candidate reference gene with medium expression levels. | | :——————————– | | Untick all target genes. Click the Run button at the top of the main window and check if the range is set correctly Select the 4 candidates with the lowest SD: Then, we performed qPCR on a representative set of 16 of our liver samples to measure the expression of these 8 candidate reference genes and analyzed the data (See how to select the best reference genes using geNorm in qbase+). Finding candidate reference genes in the commercial version of Genevestigator We will do the same exercise as above in the commercial version of Genevestigator. The difference between the free and commercial version of RefGenes is the number of target genes you can select. In the free version you have to select one gene and then gradually add all other genes one at a time. The commercial version allows you to load as many target genes as you want simultaneously. As a consequence, you can select multiple probe sets for the same gene. All VIB scientists have free access to the commercial version of Genevestigator via their VIB email address. If you don’t know your VIB email address, check the Who’s Who of VIB. Open a browser and go to the Genevestigator website If it’s your first time to access Genevestigator, create an account by clicking join now button. You will be redirected to a new window in which you will give some personal information including a valid VIB email address. Click Register and check your email to activate your new account. Go back to the GeneVestigator website Choose the research field you want to investigate: pharma/biomediacal or plant biology by clicking the corresponding button Click Start Use your VIB email address and password to login to Genevestigator. This will automatically open a Genevestigator startup page in your browser. Keep this page open during the analysis. Closing this page will close Genevestigator. Genevestigator is opened automatically Open the RefGenes tool by clicking its icon in the Further tools secion and select the liver samples of the mouse 430_2 arrays as explained in the previous exercise. | Check the expression stability of the 4 commonly used reference genes ? | | :——————————– | | - Click the New button in the Gene Selection panel to create a new selection. The selection of samples defines which data are used for the analysis. Enter the names of the 4 commercial reference genes in the text area and click OK I still remove probe sets with an _s or _x since they do not specifically bind to one single gene: Finally you end up with an expandable list of the genes you asked for and you can tick or untick them to control the display of their expression data in the main window. By default all probe sets are ticked so you can see how stable the commonly used reference genes are expressed in the 651 mouse liver samples that are stored in Genevestigator: As you can see, the expression levels of the commonly used reference genes in the selected mouse liver samples is pretty variable which is also confirmed by their relatively high SD values. The next step of selecting the 4 most stable candidate reference genes with medium expression levels is exactly the same as described above for the free version of RefGenes. Create a new gene selection with 20 found candidate reference genes and call it mouse_references. Click the New button at the top of the main window to create a new selection. To change the name of the selection right click the name in the Gene selection panel and select Rename Identify perturbations where the mouse_references genes show more than 1,5 fold differential expression using the Condition perturbations tool. Click the Home button at the top to go back to the tools overview page. Click the Perturbations tool in the Condition Search tools section Make a New Sample selection including all mouse 430_2 arrays. Untick all genes except for the first one and filter the long heatmap for at least 1.5 fold change differential expression: You now get a list of mouse samples in which the gene is not stably expressed so you can check if any of these samples is related to the samples in your study. Hover your mouse over the name of a sample to see more details about the sample. You can do this for each of the candidate reference genes and select the ones that best fit your needs Exercise on selecting reference genes for metacaspases in Arabidopsis thaliana. In a geNorm pilot experiment you analyze a set of candidate reference genes in a representative set of samples that you want to test in your final experiment. Based on the M-values and CVs that are calculated by qbase+, you can choose the genes that most satisfy the criteria for a good reference gene. Exercise 1: reference genes for mouse liver We come back on the 8 candidate reference genes that we selected for mouse liver: 4 commonly used reference genes: ACTB, TUBB4B, GAPDH and HPRT 4 candidate reference genes with very stable medium expression levels selected based on expression data coming from more than 600 microarrays of mouse liver samples using Genevestigator: Gm16845, MUSK, OTOP3, EDN3 We have measured their expression in a represetative set of 16 of our mouse liver samples, each in triplicate. We will now analyze the stability of these candidate reference genes in our samples. Creating a new Experiment Create a new Experiment called GeNormMouse in Project1 Open qbase+ or, if the software is already open, click the Launch Wizard button. You can find the details on how to create a new experiment in Creating a project and an experiment Loading the data into qbase+ The data is stored in the RefGenes folder. It consists of 8 Excel files, one file for each candidate reference gene. If you are not working on a BITS laptop, download and unzip the folder. Import the data. This files are in qBase format. You can find the details on how to start the data import in Loading data into qbase+ Unlike the previous exercise, qbase+ does not allow you to do a quick import this time. In the Import Run window Manual import is selected: Make sure that Upload file to Biogazelle support for further analysis is NOT selected and click Next Make sure the correct File type is selected (qBase) and click Finish. This file contains the data of the geNorm pilot experiment. In the pilot experiment, 8 candidate reference genes were measured in 16 representative mouse liver samples. Analyzing the geNorm pilot data Specify the aim of the experiment. In this experiment we want to select the ideal reference genes for our next experiments so we choose selection of reference genes (geNorm) Check the quality of the replicates (use default parameter settings). You can find the details on how to check the quality of the replicates in the Checking the quality of technical replicates and controls section of Analyzing gene expression data in qbase+ We haven’t included any positive or negative controls so you don’t need to show their details. Select the Amplification efficiencies strategy you want to use. You can find the details on how to select the Amplification effciencies strategy in the Taking into account amplification efficiencies section of Analyzing gene expression data in qbase+ We haven’t included dilution series nor do we have data from previous qPCR experiments regarding the amplification efficiencies so we choose to use the same efficiency for all genes. It is of course better to include a dilution series for each gene to have an idea of the amplification efficiencies of each primer pair. Convert all genes to Reference genes. You can convert all the genes simultaneously by selecting Use all targets as candidate reference genes Click Finish. | Which genes are you going to use as reference targets in further experiments ? | | :——————————————- | | Upon clicking Finish, the geNorm window containing the analysis results is automatically opened. The geNorm window consists of three tabs. The tabs are located at the bottom of the window: geNorm M, geNorm V and Interpretation. The first tab, geNorm M, shows a ranking of candidate genes according to their stability, expressed in M values, from the most unstable genes at the left (highest M value) to the best reference genes at the right (lowest M value): The second tab, geNorm V, shows a bar chart that helps determining the optimal number of reference genes to be used in subsequent analyses: The number of reference genes is a trade-off between practical considerations and accuracy. It is a waste of resources to quantify more genes than necessary if all candidate reference genes are relatively stably expressed and if normalization factors do not significantly change when more genes are included. However, Biogazelle recommends the minimal use of 3 reference genes and stepwise inclusion of more reference genes until the next gene has no significant contribution to the normalization factors. To determine the need of including more than 3 genes for normalization, pairwise variations Vn/n+1 are calculated between two sequential normalization factors. Simply stated: V is measure of the added value of adding a next reference gene to the analysis. A large variation means that the added gene has a significant effect and should be included. In normal experiments like the Gene expression experiment (see Analyzing gene expression data in qbase+), we only have 3 reference genes so we will see only 1 bar here. But in this geNorm pilot experiment, we analyzed 8 candidate reference genes, so we see 6 bars. All pairwise variations are very low, so even the inclusion of a third gene has no significant effect. Based on a preliminary experiment that was done by Biogazelle, 0.15 is taken as a cut-off value for V, below which the inclusion of an additional reference gene is not required. Normally this threshold is indicated by a green line on the geNorm V bar chart. However since all V-values fall below the threshold in this geNorm pilot experiment, you don’t see this line on the bar chart. So, these results mean that for all subsequent experiments on these samples, two reference genes, EDN3 and MUSK, would be sufficient. However, as stated before, Biogazelle recommends to always include at least three reference genes in case something goes wrong with one of the reference genes (so also include Gm16845). | These are artificial data. But when you read the paper by Hruz et al., 2011 you see that the genes that are selected by Genevestigator are often outperforming the commonly used reference genes. Exercise 2: reference genes for human heart Creating a new Experiment Create a new Experiment called GeNormHuman in Project1 You can find the details on how to create a new experiment in Creating a project and an experiment Loading the data into qbase+ | Import Run6 . This file is in qBase format. | | :——————————————- | | You can find the details on how to start the data import in Loading data into qbase+. Unlike the previous exercise, qbase+ does not allow you to do a quick import this time. In the Import Run window Manual import is selected: Make sure that Upload file to Biogazelle support for further analysis is NOT selected and click Next. Select the correct File type (qBase) and click Finish. This file contains the data of the geNorm pilot experiment. In the pilot experiment, 10 candidate reference genes were measured in 20 representative samples. Analyzing the geNorm pilot data Specify the aim of the experiment. In this experiment we want to select the ideal reference genes for our next experiments so we choose selection of reference genes (geNorm) Check the quality of the replicates and the controls (use default parameter settings). You can find the details on how to check the quality of the replicates in the Checking the quality of technical replicates and controls section of Analyzing gene expression data in qbase+ All replicates and controls have met the quality criteria so there’s no need to inspect them further.     Select the Amplification efficiencies strategy you want to use.   You can find the details on how to select the Amplification effciencies strategy in the Taking into account amplification efficiencies section of Analyzing gene expression data in qbase+. We haven’t included dilution series nor do we have data from previous qPCR experiments regarding the amplification efficiencies so we choose to use the same efficiency (E=2) for all genes. It is of course better to include a dilution series for each gene to have an idea of the amplification efficiencies of each primer pair. | Convert all genes to Reference genes. | | :———————————————————————————————————— | | You can convert all the genes simultaneously by selecting Use all targets as candidate reference genes | Click Finish. | Which genes are you going to use as reference targets in further experiments ? | | :——————————————- | | Upon clicking Finish, the geNorm window containing the analysis results is automatically opened. The geNorm window consists of three tabs. The tabs are located at the bottom of the window: geNorm M, geNorm V and Interpretation. The first tab, geNorm M, shows a ranking of candidate genes according to their stability, expressed in M values, from the most unstable genes at the left (highest M value) to the best reference genes at the right (lowest M value): The second tab, geNorm V, shows a bar chart that helps determining the optimal number of reference genes to be used in subsequent analyses: The number of reference genes is a trade-off between practical considerations and accuracy. It is a waste of resources to quantify more genes than necessary if all candidate reference genes are relatively stably expressed and if normalization factors do not significantly change when more genes are included. However, Biogazelle recommends the minimal use of the 3 most stable candidate reference genes and stepwise inclusion of more reference genes until the next gene has no significant contribution to the normalization factors. To determine the need of including more than 3 genes for normalization, pairwise variations Vn/n+1 are calculated between two sequential normalization factors. Simply stated: V is measure of the added value of adding a next reference gene to the analysis. A large variation means that the added gene has a significant effect and should be included. In normal experiments like the Gene expression experiment, see Analyzing_gene_expression_data_in_qbase+, we only have 3 reference genes so we will see only 1 bar here. But in this geNorm pilot experiment, we analyzed 10 candidate reference genes, so we see 8 bars. All pairwise variations are very low, so even the inclusion of a third gene has no significant effect. Based on a preliminary experiment that was done by Biogazelle, 0.15 is taken as a cut-off value for V, below which the inclusion of an additional reference gene is not required. Normally this threshold is indicated by a green line on the geNorm V bar chart. However since all V-values fall below the threshold in this geNorm pilot experiment, you don’t see this line on the bar chart. So, these results mean that for all subsequent experiments on these samples, two reference genes, HPRT1 and GADP, would be sufficient. However, as stated before, Biogazelle recommends to always include at least three reference genes in case something goes wrong with one of the reference genes (so also include YHWAZ). In this example we will analyze data from an artificial expression study containing the following samples: 6 treated samples: treated1, treated2, … treated6 6 control samples: control1, control2, … control6 In this study, the expression of the following genes was measured: 4 commonly used reference genes: ACTB, HPRT, GAPDH, and TUBB4. We have seen in the previous exercise that the expression of these reference genes in mouse liver samples is not as stable as generally thought. 3 genes of interest: Low: a gene with low expression levels Medium: a gene with moderate expression levels HighVar: a gene with low and very noisy expression In general, the lower the expression level, the more noisy the qPCR results will become. For each of the genes of interest we have included a run in which a 2-fold difference in expression between control and treated samples was created (Low1, Medium1 and HighVar1) and a run with a 4-fold difference in expression (Low2, Medium2 and HighVar2). There are three technical replicates per reaction. In a second experiment we used the reference genes that were obtained via Genevestigator and that proved to be more stably expressed in mouse liver samples than the commonly used references. The data can be found in the NormGenes folder on the BITS laptops or can be downloaded: from our website. Creating a new experiment Create a new Experiment called NormGenes1 in Project1 You can find the details on how to create a new experiment in Creating a project and an experiment Loading the data Import Run1 to Run5. These files are in qBase format. You can find the details on how to import the data file in the Loading the data into qbase+ section of Analyzing data from a geNorm pilot experiment in qbase+ We are going to compare expression in treated versus untreated samples so we need to tell qbase+ which samples are treated and which not. To this end, we have constructed a sample properties file in Excel containing the grouping annotation as a custom property called Treatment. Import the Sample Properties file. You can find the details on how to import the data file in the Adding annotation to the data section of Loading data into qbase+. Select to import the custom property. So as you can see we have 6 treated and 6 untreated samples and we have measured the expression of the 4 commonly used reference genes and 6 genes of interest: Analyzing the data Which amplification efficiencies strategy are you going to use ? You don’t have data of serial dilutions of representative template to build standard curves so the only choice you have is to use the default amplification efficiency (E = 2) for all the genes. Appoint the reference genes. ACTB, GAPDH, HPRT and TUBB4B are the reference genes: You can find the details on how to appoint reference targets in the Normalization section of Analyzing gene expression data in qbase+   Is the stability of the reference genes ok ? The M and CV values of the reference genes are shown in green so the stability of the reference genes is ok. Which scaling strategy are you going to use ? Since you have a treated and a control group, it seems logical to use the average of the control group for scaling. You can find the details on how to specify the scaling strategy in the Scaling section of Analyzing gene expression data in qbase+ Look at the target bar charts. In the target bar charts plot the average expression level of each group. In the Grouping section at the bottom of the chart you can select Plot group average: Now do exactly the same for the second experiment with the same genes of interest but with other reference genes. This means that you have to return to the Analysis wizard. To this end, click the Launch wizard button a the top of the page: Create a new Experiment called NormGenes2 in Project1 You can find the details on how to create a new experiment in Creating a project and an experiment Import Run5 to Run9. These files are in qBase format. You can find the details on how to import the data file in the Loading the data into qbase+ section of Analyzing data from a geNorm pilot experiment in qbase+ Import the Sample Properties file. You can find the details on how to import the data file in the Adding annotation to the data section of Loading data into qbase+. Select to import the custom property. So as you can see we have 6 treated and 6 untreated samples and we have measured the expression of the 4 new reference genes and 6 genes of interest: | Appoint the reference genes. |EDN3, Gm16835, MUSK and OTOP3 are the reference genes: | :—————————————————- | | You can find the details on how to appoint reference targets in the Normalization section of Analyzing gene expression data in qbase+ | Is the stability of the reference genes ok ? The M and CV values of the reference genes are shown in green so the stability of the reference genes is ok. As you can see the M and CV values of these reference genes is much lower than these of the 4 commonly used reference genes pointing to the fact that genes are more stably expressed. It’s not that the commonly used reference genes are bad references. Then qbase+ would not display them in green. It’s just that the other reference genes are more stable. But this can have a big impact on the results of your analysis… Use the average of the control group for scaling You can find the details on how to specify the scaling strategy in the Scaling section of Analyzing gene expression data in qbase+ Plot the average expression level of each group. Now we will compare the target bar charts of the second and the first experiment to assess the influence of the stability of the reference targets on the analysis results. How to display the target bar charts of the second and the first experiment next to each other ? You can display the bar charts next to each other by clicking the tab of the bar chart of the second experiment. Drag the tab to the right while you hold down the mouse button until you see and arrow at the right side of the qbase+ window and a dark grey box in the right half of qbase+ window. Release the mouse button when you see the arrow and the box. Now the two bar charts should be next to each other. Some laptop screens are too small to nicely display the two bar charts next to other. If this is the case switch to full screen mode by double clicking the tab of the first experiment. Now you can compare the expression of each gene in the first and in the second experiment. When we do this for HighVar1 for instance, you see that the average expression levels of both groups are the same in the first and the second experiment (check the scales of the Y—axis!). Both experiments detect the two-fold difference in expression level between the groups. However, the error bars are much larger in the first experiment than in the second. The variability of the reference genes does have a strong influence on the errors and the size of the error bars will influence the outcome of the statistical test to determine if a gene is differentially expressed or not. The larger the error bars the smaller the less likely it is that the test will say that the groups differ. Remember that the error bars represent 95% confidence intervals: if the error bars of the two groups do not overlap: you are certain that the difference between the means of the two groups is significant if they do not overlap: you know nothing with certainty: the means can be different or they can be the same. Of course the more they overlap the smaller the chance that there is a significant difference between the groups. Check out the results of HighVar2. Here, you clearly see the influence of the reference genes. Again, the fourfold difference in expression is detected by both experiments but: the least stable reference genes (experiment 1) give large overlapping error bars the most stable reference (experiment 2) give smaller, barely overlapping error bars This means that in experiment 2, a statistical test will probably declare that HighVar2 is differentially expressed while in experiment 1 this will not be the case. We will test this assumption by performing a statistical test. Statistical analysis of differential expression Use a non-parametric test to identify DE genes in experiment 1 ? You can find full details on statistical analyses in qbase+ in the statistical analysis section of analyzing gene expression data in qbase+. In brief, you need to perform the following steps: Open the Statistical wizard The goal of this analysis is to compare the mean expression levels of our genes of interest in treated and untreated samples Use the Treatment property to identify treated and untreated samples Analyze all genes of interest Use the default settings to perform the non-parametric Mann-Whitney test As you can see, none of the genes is considered DE by the very conservative non-parametric test. Additionally most genes have the same p-value. That’s normal when you don’t have many replicates. In our case, we have 6 replicates. Non-parametric tests are based on a ranking of the data values and there are not so many ways to rank 6 data points. This is why you see the same p-values for many genes. As said before, the non-parametric test is very stringent. If the data do come from a normal distribution, the test will generate false positives. Some of the genes might have have been labeled not DE while in fact they are DE so you might have missed some differential expression. The choice of statistical test with 6 biological replicates depends on what you prefer: false negatives or false positives. Most people will choose false negatives since they don’t want to invest time and money in research on a genes that was labeled DE while in fact it is not DE. Suppose I don’t mind false positives but I don’t want to miss any potential DE genes. In that case, it’s better to go for a t-test. Let’s repeat the test n ow choosing a parametric t-test. | Use a t-test to identify DE genes in experiment 1 ? | | :—————————————————- | | You can find full details on statistical analyses in qbase+ in the statistical analysis section of analyzing gene expression data in qbase+. In brief, you need to perform the following steps: Open the Statistical wizard The goal of this analysis is to compare the mean expression levels of our genes of interest in treated and untreated samples Use the Treatment property to identify treated and untreated samples Analyze all genes of interest Describe the data set as log-normally distributed Still none of the genes is considered DE but you do see that the p-values of the t-test are lower than these of the Mann-Whitney test. | Use a non parametric test to identify DE genes in experiment 2 ? | | :—————————————————- | | You can find full details on statistical analyses in qbase+ in the statistical analysis section of analyzing gene expression data in qbase+. In brief, you need to perform the following steps: Open the Statistical wizard The goal of this analysis is to compare the mean expression levels of our genes of interest in treated and untreated samples Use the Treatment property to identify treated and untreated samples Analyze all genes of interest Use default settings Now you see that 4 out of the 6 genes are considered DE. This is also what we expected since 3 of our genes of interst have a 4-fold difference in expression level between the two groups. It’s understandable that it’s hard to detect 2-fold differences in expression especially when the expression of the gene is somewhat variable as is the case for Low1 and HighVar1 but a 4-fold difference is a difference that you would like to detect. | Use a t-test to identify DE genes in experiment 2 ? | | :—————————————————- | | You can find full details on statistical analyses in qbase+ in the statistical analysis section of analyzing gene expression data in qbase+. In brief, you need to perform the following steps: Open the Statistical wizard The goal of this analysis is to compare the mean expression levels of our genes of interest in treated and untreated samples Use the Treatment property to identify treated and untreated samples Analyze all genes of interest Describe the data as log normally distributed Again the t-test generates lower p-values than the Mann-Whitney test but realize that choosing the t-test when the data is not normally distributed will generate false positives !"},{"title":"03 Primer design exercises","url":"/topics/qbase-plus/tutorials/primer-design/tutorial.html","tags":[],"body":"The following exercise will make you familiar with the Primer3Plus software for designing primers for PCR. Primer3Plus is the user-friendly version of Primer3, the standard software for primer design. Criteria for qPCR primers Primers for qPCR have to follow all the gudelines for regular primers is and an additional set of rules specific for qPCR primers: qPCR products are small: 80-160 bp use intron or exon-exon junction spanning primers to detect genomic DNA contamination in the RNA samples. Primers of intron spanning primer pairs are located at both sides of an intron and will therefore generate a larger product on genomic DNA (containing the intron). Primer pairs containing an exon-exon junction spanning primer will not generate a PCR product on genomic DNA since the exon-exon junction only exist in the cDNA. primer length between 9 and 30 bp with an optimum at 20 bp melting temperature (Tm) of the primers between 58 and 60°C with an optimum at 59°C maximum Tm difference between the primers of a pair: 2°C GC content of the primers between 30 and 80% with an optimum at 50% the 5 nucleotides at the 3’ end of the primers should have no more than 2 G or C bases avoid runs of 4 or more identical nucleotides (especially Gs) primers must specifically target the region you want to amplify There are many programs for designing primers, the most important ones: Primer3 [1] or use it’s user-friendly version: Primer3Plus[2] PrimerBLAST[3] The major downside of Primer3 and Primer3Plus is the fact that you have to check the specificity of the primers yourself. Primer3 will suggest a number of primer pairs that fulfill all of the above requirements, but Primer3 will not check the specificity of the primers. So you have use BLAST to check the specificity of the suggested primer pairs. Very often, the selected primers are not specific and you have to repeat the entire Primer3 analysis. If you use Primer3 and do the BLAST yourself, BLAST against Refseq sequences unless they are not available for the organism you work with or you have reasons to believe that they are not complete (i.e. they do not represent the full genome). For model organisms, you can BLASTagainst the Refseq database. Limit the database to sequences from the organism you work with. Additionally, it is especially important to check that the primers are specific at the 3’ end because that’s the site where the polymerase will attach nucleotides. So it is recommended to not use primers that contain long identical stretches (> 15nt for primers of 20nt long) to other regions in the genome, and certainly not if these stretches comprise the last nucleotide at the 3’ end of the primer. For these exercises we will use PrimerBLAST since it uses the same algorithm to pick primers as Primer3 [4] and does the specificity check for you! Designing qPCR primers for the fruit fly tap gene Designing qPCR primers using PrimerBLAST The RefSeq entry NM_079400 contains the sequence of the D. melanogaster mRNA coding for tap, the target of Poxn. Tap encodes a bHLH protein expressed in larval chemosensory organs and involved in the response to sugar and salt. We wish to amplify the region encoding the Helix-loop-helix domain. In the sequence of the RefSeq record, the domain is located between position +577 and +745. We want to design qPCR primers for measuring the expression level of the hlh domain using SYBR green. Remember that it is advised to design intron/exon-exon junction spanning primers for qPCR experiments that are based on fluorescent labels to detect/avoid amplification of contaminating genomic DNA. Check in NCBIs Gene database if the hlh domain contains any introns ? To know the location of the introns, you need the genomic sequence instead of the mRNA sequence. Go to the NCBI RefSeq record. In the right menu click the link to the Gene record In the Genomic regions, transcripts and products secton you can see that the gene contains no introns: the transcript is not chopped up into pieces when aligned to the genome. Click here for an example of a gene with introns. Next, we will design primers to measure the expression of the hlh domain. Go to Primer BLAST by using the link in the Refseq record Go back to the RefSeq mRNA record. There, you can go directly to PrimerBLAST by clicking the Pick Primers link in the Analyze this sequence section of the right menu. Since you want to measure the expression of the hlh domain you want primers that are located inside the domain. Define the range of the sequence in which you want to design primers. You have to specify the range as follows: Define the primer parameters to comply with the rules of qPCR primer design: product size and Tm. To comply with the rules for qPCR primer design, you have to change the settings for PCR product size and melting temperature: The PrimerBLAST automatically decides to check primer specificity in the Drosophila (organism ID: 7227) RefSeq mRNA database which is exactly what you want. For the qPCR you are going to use RNA samples from fruitfly. This means that the primers will only come into contact with Drosophila mRNAs so you only have to check their specifity in this database. Make sure the last 2 nucleotides are completely specific. You want to ensure that the 3’ end of the primers really is specific: The PrimerBLAST gives you a set of 9 primer pairs that are specific (according to the criteria that you have specified) and that fulfill all other requirements that you have defined. Look at the detailed report of the first primer pair: All parameters are quite self-explanatory except for the Self complementary and Self 3’complementarity scores. The first score represents the local alignment score when aligning a primer to itself. The scoring system gives 1.00 for a match, -1.00 for a mismatch. This means that the lower the score (the more mismatches), the less likely that the primer binds to itself. The second score represents the global alignment score when aligning a primer to itself. Here again, the lower the score, the better. The scores are followed by information on the specificity of the primer: alignments of the two primers to all target sequences from the database that match the criteria that you specified. In these alignments dots represent matching nucleotides while letters represent mismatches. A specific primer pair will have two alignments (one for each primer): both perfect alignments (all dots) to the sequence you want to amplify. Analyzing primer characteristics using OligoAnalyzer OligoAnalyzer is a tool implemented by ID\\&T (who sell primers) to check the characteristics of your primers. Take the first primer that is suggested by Primer-BLAST, the pair resulting in a product of 100bp. | What’s the Tm of the first primer ? | | :—————————— | |Copy the sequence of the first primer in the Sequence box, adjust the concentrations to these that are typically used in PCR (see slides) and click Analyze: As you can see the predicted melting temperature is 63.9 ºC, which is slightly different from the prediction made by BLAST. There are many different methods to predict Tm and each method will give a different result. Assumed concentrations of primers and ions have an enormous impact on the Tm prediction. So don’t worry about these differences: these are theoretical calculations anyway, the only way to determine Tm values is by doing actual PCR. As long as the difference in Tm between the two primers is not too large, everything is fine. What’s the Tm of the second primer ? Copy the sequence of the second primer in the Sequence box and click Analyze. The predicted melting temperature is also 63.9 ºC , the same Tm as the first primer. Remember that the second primer had a large Self complementarity score according to PrimerBLAST. Check the self-complementarity of the second primer in OligoAnalyzer ? Click Self-Dimer: You see that the highest scoring alignment indeed has 6 matches, giving a score of 6 as predicted by PrimerBLAST. | Do you expect this self-complementarity will give problems in the PCR ? | | :—————————— | |No, the complementarity is concentrated at the center of the primer, not at the 3’ end. Since polymerases add bases at the 3’ end of the primer, the primer duplex cannot be extended so it will not give rise to aspecific products. ID&T recommends to avoid complementary stretches of more than 2 bp at the 3’ end. However, even if the primer dimer cannot be extended, it could interfere when its formation competes with the annealing of primer and target. This is only the case when the stability of the dimer is similar to the stability of a perfectly matched primer-target duplex. The stability of the perfectly matched duplex is shown as a Maximum Delta G at the top of results. So non-extendable dimer structures that are much shorter than the intended duplex, as we have here, are not going to disrupt the PCR reaction. It is advised to review all possible interactions between primers so both Self-Dimer (primers binding to themselves) and Hetero-Dimer (primers binding to each other) interactions between primers are examined. Is it likely that the primers bind to each other ? Click Hetero-Dimer: This opens a text box to enter the second primer. Click Analyze. There is one structure (the fourth one) that looks problematic because there is a stretch of 3 matching nucleotides at the 3’end of one of the primers. So you might consider taking a look at the second pair of primers that PrimerBLAST suggests. On the other hand, this structure is has relatively high free energy (delta G). The structure with the lowest total free energy, the target-primer duplex, is most important because it will dominate in solution. Structures with higher free energy are less stable and will be present in smaller amounts in the reaction mixture. Take a look at the second primer pair that was suggested by PrimerBLAST. Is it likely that these primers bind to each other ? No these primers do not form duplex structures that could pose a problem during PCR. Designing qPCR primers for the human F9 gene Designing qPCR primers using PrimerBLAST The RefSeq entry NM_000133.3 contains the sequence of the human mRNA coding for coagulation factor F9. The gene contains 8 coding exons and gives rise to a transcript of 2780 bp encoding a protein of 461 amino acids. Next, we want to design primers to measure the expression of the F9 gene. Go to the RefSeq record of this transcript to study its structure. When you scroll down to the features section you see that the CDS is located from position 40 to position 1415. Since RNA degradation starts at the 5’end of transcripts, we don’t want to pick primers at the 5’end. On the other hand, we don’t want to pick primers in the long 3’UTR either because it doesn’t contain any introns (the exons are all coding) and we want to design exon-exon junction or intron spanning primers. Let’s try to find exon-exon junction spanning primers between position 400 and 1600, with optimal anneal temperature = 60. Find primers that fulfill the above defined criteria Go to PrimerBLAST and fill in the form as follows: Exclude predicted sequences in the database to search in . Find primers that fulfill the above defined criteria Go to PrimerBLAST and fill in the remainder of the form as follows: The PrimerBLAST gives you a set of 10 primer pairs. Look at the detailed report of the first primer pair: As you can see the primers are not specific: they can bind to various other targets albeit with lower affinity because of the mismatches . The best option seems to be primer pair 7, which binds to both F9 transcript variants and potentially to one unintended target, but as you can see the last nucleotide at the 3’ end of both primers are specific. In silico PCR in the UCSC Browser We will proceed using the third primer pair Primer-BLAST suggests. You can visualize the PCR product (and additional annotation) in the UCSC Genome Browser using UCSC’s In Silico PCR tool. Select the most recent version of the human genome and paste the sequences of forward and reverse primers in their respective boxes. Click submit Normally, this returns the location and the sequence of the PCR product but our primer pair doesn’t return a match. When you think about this was to be expected since we are working with exon-exon junction spanning primers that are not able to match the genome sequence. So checking SNPs is not so straight-forward in the case of exon-exon junction spanning primers. We will repeat the primer search now searching for intron-spanning primers to show you how to use the in silico PCR tool. Taking into account the fact that the results for the exon-exon junction spanning primers were so messy we will make the search more stringent this time: We will the minimum number of mismatches to 4 and at least 3 mismatches in the last 3 bps at the 3’end Find intron spanning primers that fulfill the above defined criteria Go back to the Primer-BLAST and fill in the form like in the previous exercise except that they should span an intron: Primer-BLAST returns 10 primer pairs. Again the seventh primer pair is the specific one. | Take the seventh suggested primer pair and check for SNPs in the UCSC Browser | | :—————————— | |Go to PrimerBLAST and paste the sequences of forward and reverse primers in their respective boxes. This time the search finds a PCR product: Clicking the location visualizes the PCR product in the UCSC genome browser. Remove unnecessary trancks by right clicking the box in front of them and selecting hide Add tracks showing relevant annotation like position of SNPs… Setting the SNPs track from hide to full shows the SNPs in the browser. Center the forward primer by grabbing and dragging it to the center. Zoom in to base display to see if the forward primer is matching any SNPs. As you can see the forward primer does match two SNPs but none of them are located near the 3’end of the primer. http://frodo.wi.mit.edu/ http://primer3plus.com/cgi-bin/dev/primer3plus.cgi http://www.ncbi.nlm.nih.gov/tools/primer-blast/index.cgi?LINK_LOC=BlastHome http://www.ncbi.nlm.nih.gov/tools/primer-blast/primerinfo.html"},{"title":"04 Text mining, scripting and loops","url":"/topics/linux/tutorials/textmining-scripting-loops/tutorial.html","tags":[],"body":"A script A script is just a plain text file. I will show this below. It contains written instructions, that can be understood by a programming language, in our case bash . An example script Create a text file named ‘buddy’ in your home with following content: badday=\"Cheer up\" goodday=\"Doing great\" echo \"$badday, $USER !\" echo \"$goodday, $USER !\" One way of doing this is: nano buddy and copy of the contents of the header above. Save the contents by pressing +O. Close nano with +x What type of file did you create? file buddy buddy: ASCII text That file contains plain text. To execute the commands in that file, feed it as an argument to the program ‘bash’. bash buddy Cheer up, bits ! Doing great, bits ! Few things to notice: in the script, we have defined 2 variables ‘badday’ and ‘goodday’ their values can be displayed by the program echo which takes as an argument the name of the variable preceded by a $ sign. the $USER variable, is an environment variable. They can be used in scripts. Env variables are typically written in capitals. Getting more professional We can make this easier. If you start your script with the symbol ‘#’ and next specify the path to the interpreter, the terminal will feed this script automatically to the right interpreter for you! To see what this means, follow these steps. Find out the path to the program bash which bash /bin/bash Now we know the path to bash, we have to provide this path, on the very first line, preceded by #! (shebang or crunchbang). If you have another type of script, let’s say perl, you find out the path to perl, and at this path behind a #! on the very first line. Open the text file ‘buddy’, and add at the start of the file ‘#!’ followed by the path to bash: nano buddy … edit the text cat buddy #!/bin/bash badday=\"Cheer up\" goodday=\"Doing great\" echo \"$badday, $USER !\" echo \"$goodday, $USER !\" Before turning the text file into a script, set the execute permission (to allow execution) with chmod chmod +x buddy What type of file is your script? file buddy buddy: Bourne-Again shell script, ASCII text executable By setting the shebang, the interpreter on the command line knows that this is a bash script! Now run your script as if it were a program (./) ./buddy Cheer up, bits ! Doing great, bits ! To make it more readable, often the extension .sh is given to the text file. Note that this is not necessary! Linux does not define file types by extensions. Rename your script to ‘buddy.sh’ $ mv buddy buddy.sh Alternative (less typing!) $ mv buddy{,.sh} A good habit The last line of your script should be ‘exit 0’. If bash reaches this lines, it means that the script was successfully executed. Add it by opening the file with ‘nano’ and modifying its contents. $ cat buddy.sh #!/bin/bash badday=\"Cheer up\" goodday=\"Doing great\" echo \"$badday, $USER !\" echo \"$goodday, $USER !\" exit 0 Alternative. Less typing! echo \"exit 0\" >> buddy.sh This was our first bash script! I hope it was a painless experience. Download a Perl script Many bioinformatics programs are written in python or perl. It’s quick to type some python or perl code in a text file, and get your job done. Those scripts are text files. You can download and store scripts on your computer. Usually these files have .py or .pl extension. As long as you have python or perl on your system (by default in Linux!), you can run the scripts. Run perl code Let’s try a small script below. Download a simple perl script here Download the dna file here Save the file, under ~/Downloads for now. Open Geany on your computer, and copy the script code to Geany. Execute the script by clicking the little ‘gear’ box. For this script, you will need to download the dna.txt file as input. The results of the script appear in a small window. It will ask for an input (depending on your script). Enter the required details. Extract some lines Download the bed file here via command line wget https://dl.dropbox.com/u/18352887/TAIR9_mRNA.bed Look at the first 10 lines of this file. $ head TAIR9_mRNA.bed chr1 2025600 2027271 AT1G06620.1 0 + 2025617 2027094 0 3 541,322,429, 0,833,1242, chr5 2625558 2628110 AT5G08160.1 0 - 2625902 2627942 0 6 385,143,144,186,125,573, 2167,1523,1269,928,659,0, chr5 2625558 2628110 AT5G08160.2 0 - 2625902 2627942 0 7 258,19,143,144,186,125,573, 2294,2167,1523,1269,928,659,0, chr4 12006985 12009520 AT4G22890.5 0 + 12007156 12009175 0 10 370,107,97,101,57,77,163,98,80,263, 0,802,1007,1196,1392,1533,1703,1945,2120,2272, chr4 12007040 12009206 AT4G22890.2 0 + 12007156 12009175 0 9 315,113,97,101,57,77,163,98,101, 0,741,952,1141,1337,1478,1648,1890,2065, chr4 12006985 12009518 AT4G22890.3 0 + 12007156 12009175 0 10 370,113,97,101,57,77,163,98,80,257, 0,796,1007,1196,1392,1533,1703,1945,2120,2276, chr4 12006985 12009520 AT4G22890.4 0 + 12007156 12009175 0 10 370,104,97,101,57,77,163,98,80,263, 0,805,1007,1196,1392,1533,1703,1945,2120,2272, chr4 12006985 12009520 AT4G22890.1 0 + 12007156 12009175 0 10 370,113,97,101,57,77,163,98,80,263, 0,796,1007,1196,1392,1533,1703,1945,2120,2272, chr2 14578539 14581727 AT2G34630.2 0 + 14578688 14581632 0 11 293,93,81,72,132,87,72,86,133,189,275, 0,797,1120,1320,1488,1711,1898,2165,2435,2649,2913, chr2 14578629 14581727 AT2G34630.1 0 + 14579725 14581632 0 11 203,96,81,72,132,87,72,86,133,189,275, 0,704,1030,1230,1398,1621,1808,2075,2345,2559,2823, This is a typical bioinformatics text file, with every row divided in field by tabs. Extract all lines that start with chr1 from the TAIR9_mRNA.bed and put them in a new text file “chr1_TAIR9_mRNA.bed”. $ grep \"^chr1\" TAIR9_mRNA.bed > chr1_TAIR9_mRNA.bed Checking the data Download human chromosome 21 from this link and unzip the file. wget https://data.bits.vib.be/pub/trainingen/Linux/Homo_sapiens.dna.chromosome21.zip gunzip Homo_sapiens.dna.chromosome21.zip Entries in a fasta file start with > How many entries are in that fasta file? Remember you can combine commands with a |. grep \"^>\" Homo_sapiens.GRCh37.73.dna.chromosome.21.fa | wc -l How many? Use the TAIR9_mRNA.bed file used in the first exercise. Remember it looks like this chr1 2025600 2027271 AT1G06620.1 0 + 2025617 2027094 0 3 chr5 2625558 2628110 AT5G08160.1 0 - 2625902 2627942 0 6 chr5 2625558 2628110 AT5G08160.2 0 - 2625902 2627942 0 7 chr4 12006985 12009520 AT4G22890.5 0 + 12007156 12009175 0 10 chr4 12007040 12009206 AT4G22890.2 0 + 12007156 12009175 0 9 If you want to find entries that lie on the + strand of a certain chromosome, you need to find lines that start with the chromosome number and that contain a + sign. The number of characters between the chromosome number and the + sign is variable. How many genes are lying on the + strand of the first chromosome ? Since you need to use the + sign to represent a set of characters of variable length you need to use egrep for this: grep \"^chr1.+\\+\" TAIR9_mRNA.bed | wc -l More complex extraction Get the last exon size for all mRNA records in Arabidopsis. Use TAIR9_mRNA.bed for this: this file contains the exon sizes. See the .BED page to check that the field we need is field 11. This contains a comma separated list of the sizes of all the exons of a mRNA Get the exon sizes for all mRNA records in Arabidopsis. Write them to a file called exons.txt awk '{ print $11 }' TAIR9_mRNA.bed > exons.txt Take a look at the first 10 lines of exons.txt head exons.txt If we try to print the last field with awk, using ‘,’ as a delimiter, things go wrong: awk -F',' '{ print $NF }' > lastexons.txt The reason is that the last field is empty, because the lines end with a ‘,’. We need to remove the last ‘,’ and can use sed for this. Remove the last comma from the lines and save in a file called exonsclean.txt. You want to substitute the comma at the end of the line by nothing: sed 's/,$//' exons.txt > exonsclean.txt head exonsclean.txt Fetch the last field from exonsclean.txt and save in a file called lastexons.txt awk -F',' '{ print $NF }' exonsclean.txt > lastexons.txt head lastexons.txt Sort exonsizes from largest to smallest into a file called lastexonssort.txt sort -nr lastexons.txt > lastexonssort.txt head lastexonssort.txt You can use uniq to summarize the results uniq -c lastexonssort.txt | head 2 6885 1 5616 1 5601 1 5361 1 5239 1 4688 2 4470 1 4446 1 4443 1 4275 Analyzing a short read alignment SAM (‘sequence alignment map’) file format is the format which summarizes the alignment of reads to a reference genome. Is is one of the key files in NGS analysis, and you can learn a lot from it. See the SAM page for a description of this format. Download the sam file from here wget http://data.bits.vib.be/pub/trainingen/Linux/sample.sam How many lines has the SAM file? wc -l sample.sam 100015 lines How many lines start with ‘@’, which is the comment symbol in the SAM format. grep '^@' sample.sam | wc -l 15 lines You can use grep to skip the lines starting with ‘@’, since they are comment lines. grep -v '^@' sample.sam | head Write the FLAG field (second field) to a file called flags.txt and pipe the grep results to awk to print the second field. grep -v '@' sample.sam | awk '{ print $2 }' > flags.txt head flags.txt Sort and summarize (using uniq) flags.txt and pipe the grep results to awk to print the second field. sort -nr flags.txt | uniq -c Sort the results on number of times observed (the first field). We build on the previous command, and just pipe the output to sort -nr. We do not have to use the option -k, since sort always takes the first field. sort -nr flags.txt | uniq -c | sort -nr Advanced We use the TAIR9_mRNA.bed to answer this. First we check how many different genes are in the file. A gene has the code ATG. Splice variants have to same AT number but different version number (the numbers after the . are different. We are not interested in splice variants so want to remove the .1, .2… before counting. You can do this by using the . as a field delimiter Remove everything after the . and save in a file called TAIRpart.txt awk -F'.' '{ print $1 }' TAIR9_mRNA.bed > TAIRpart.txt head TAIRpart.txt Now you need to summarize the fourth column of this file and count the lines of the result How many different genes are in the file? cut -f4 TAIRpart.txt | sort | uniq | wc -l 27379 When you look at TAIR9_mRNA.bed you see that the the fifth column contains 0. Check if there is any entry that contains another number in that column ? (summarize will give you the answer) cut -f5 TAIR9_mRNA.bed | sort -nr | uniq -c No Another example: Show all Arabidopsis mRNA with more than 50 exons awk '{ if ($10>50) print $4 }' TAIR9_mRNA.bed Print the number of exons (field number 10) of mRNAs from the first chromosome. grep '^chr1' TAIR9_mRNA.bed | awk '{ print $10 }' Obtain AT numbers (field 4) and exon info (field 11) awk '{ print $4,\",\",$11 }' TAIR9_mRNA.bed Bash Aliases to enhance your productivity You specify aliases in the .bashrc file in your home directory. alias myalias## \"\" Change ‘my fancy command’ to a real command!! Before you can use your new aliases, you have to reload the .bashrc file. You do this by $ source ~/.bashrc or $ . ~/.bashrc Now, let’s do this exercise. Sometimes you might want to open a big text file from the end on, and start scrolling towards the top. We will create an alias for this in this exercise. Create an alias that starts scrolling from the bottom. Tip: it’s less and the appropriate option you must configure. Read through the man page of less. To help you: you can search for the string “at the end”. Open the man page of less $ man less Type “/at the end” and . Less will search in the content for \"at the end\". Examine the entries with the string./ Go to the following result by typing \"/\" followed by ENTER. The option is: add the alias by opening .bashrc with an editor, and adding the line: alias sell## \"less +G\" When you have changed the content of .bashrc, it needs to be reloaded. Close your terminal and fire it up again. OR execute: $ . ~/.bashrc $ source ~/.bashrc We now have sell to our disposal, which starts scrolling large text files from the end of the file. $ sell /var/log/syslog Show all aliases on your system Forgot an alias? To see all your aliases, run the command $ alias. Writing loops For loops are used to repeat commands a number of times. We will start with two simple examples. Write a for loop to create 3 files: test1.txt, test2.txt, test3.txt for i in 1 2 3 do touch test$i.txt done ls -l Write a for loop to create 3 folders: folder1, folder2, folder3 for i in 1 2 3 do mkdir folder$i done ls -l"},{"title":"Functional enrichment analysis","url":"/topics/functional_analysis/tutorials/Enrichment_Analysis/tutorial.html","tags":[],"body":"What’s the biology behind a list of genes ? Omics experiments typically generate lists of hundreds of interesting genes: up- or downregulated genes identified in an RNA-Seq experiment somatically mutated genes in a tumor identified by exome sequencing proteins that interact with a bait identified in a proteomics experiment … Over-representation analysis Since it’s impossible to evaluate each gene individually, the most meaningful approach is to see what functional annotations the genes in the list have in common e.g. are many of them involved in the same pathway ? Functional characterization of a gene list involves the following steps: Add functional annotations to the genes in the list Define a background: typically the full set of all genes in the genome Perform a statistical test to identify enriched functions, diseases, pathways Enriched means over-represented, occurring more frequently in the list than expected by chance based on the background data. It is recommended to characterize up- and downregulated genes separately. !! Thousands of pathways are tested for enrichment, this could lead to false positives. Multiple testing correction is used to correct the p-values from the individual enrichment tests to reduce the chance of false positives !! ToppGene: most up-to-date but only human, mouse and rat ToppGene is the most up-to-date portal for gene list functional enrichment. See this overview of their resources of functional annotations and their last update date. The ToppFun tool returns enriched terms from GO, phenotypes, pathways, protein interactions, domains, transcription factor binding sites, miRNA-target genes, disease associations, drug-gene interactions compiled from various data sources… It supports gene symbols, Ensembl, Entrez, RefSeq and UniProt IDs from human. However, since gene symbols for human, mouse and rat are identical the tool can also be used for mouse and rat. hands_on Exercise ToppGene How to do functional enrichment analysis with ToppFun ? solution answer On the ToppGene page click the first link ToppFun: Transcriptome, ontology, phenotype, proteome… Enter gene symbols or Ensembl IDs in the box Training Gene Set Click Submit Query If the gene list contains non-approved symbols or duplicates, they are listed under Genes Not found. In the section Calculations select the functional annotation types you want to test (all in our case) and select the multiple correction method (default FDR is ok) and the significance cut-off level (default 0.05 is ok) Click Start Input Parameters summarizes the input parameters of the search. Click the Show Detail (red) link to see them. Training results contains the enrichment analysis results. Download all (blue) will download the analysis results as a text file. Click the Display chart (green) link to visualize the results If you want to see which genes from your list belong to a certain annotation click the number in the Genes from innput column Enrichr: longest list of resources but not so up-to-date Enrichr use a respectable number of resources to compute enrichment but they are not as regularly updated as those of ToppGene. To learn more about Enrich, see their FAQ page. Enrichr uses a list of gene symbols as input (one per line). It only supports human, mouse and rat. You can upload the list by selecting a text file or by simply pasting the list of gene symbols into the text box. hands_on Exercise 1 Enrichr How to perform functional enrichment analysis in Enrichr ? Browse to Enrichr submission page and click the Submit button The results page consists of multiple tabs, each tab giving an overview of a specific type of annotation (Transcription, Pathways, Ontologies…), e.g. hands_on Exercise 2 Enrichr How to visualize the results for KEGG pathways as a bar chart ? solution answer Go to the Pathways tab and expand the results for KEGG as a bar chart. The bar charts are interactive: hover your mouse over the bars to see the enrichment scores. Clicking the bars will order the terms according to different scores. The length of the bar represents the significance of that specific term. In addition, the brighter the color, the more significant that term is. Enrichr implements three approaches to compute enrichment scores: The p-value comes from a test implemented in most enrichment analysis tools: the hypergeometric test The q-score is the adjusted p-value using the Benjamini-Hochberg method for correction for multiple hypotheses testing. EnrichR computes enrichment using the hypergeometric test for many random gene sets to compute mean and standard deviation of the expected rank for each annotation. Then it computes an odds ratio reflecting the deviation of the actual rank from this expected rank. They combine the p-value of the hypergeometric test with the odds ratio into a combined score hands_on Exercise 3 Enrichr How to obtain the table containing the actual scores ? Sort the terms according to adjusted p-value solution answer If you want to see the actual scores click the Table tab Click the name of the column you want to use for sorting hands_on Exercise 4 Enrichr Look at the results for GO Biological processes, OMIM disease, and TargetScan miRNAs hands_on Exercise 5 Enrichr How to visualize enriched Transfac and Jaspar TFBS as a network ? solution answer Go to the Transcription tab and click TRANSFAC and JASPAR PWMs Visualize the results as a network by clicking then Network tab Each node in the network represents a term (in this case a Transcription Factor) and a link between two nodes means that the 2 transcription factors have some genes from the list in common. the genes are linked to both transcription factors. Since these transcription factors share target genes from the list they might interact to regulate the process you’re studying. Webgestalt: all organisms but one resource at a time This tool largely overlaps in data-sources with Enrichr but updates them more regularly. WebGestalt accepts many ID types and supports 12 different model organisms. For other organisms it allows to upload your own functional annotation database (see section 3.1 of the manual of this tool). hands_on Exercise 1 WebGestalt How to calculate enrichment of KEGG pathways in a list of genes ? solution answer In the Organism box select the correct organism In the Method box select Over-Representation Analysis In the Functional Database boxes select pathway and KEGG In the Gene ID type box select the correct ID type Upload the list of IDs In the Select Reference Set box select the correct background, for lists generated by RNA-Seq experiments genome, protein-coding is a good choice because that is what you have measured Click the Submit button The Enrichment results can be visualized as a table, a bar chart or a Volcano plot. Dark blue bars are considered significantly enriched. Clicking a bar shows the details on the bottom half of the page: FDR is the corrected p-value (blue) -Mapped input represents your gene list gene set is the total group of genes in the genome with this annotation overlap is the number of genes from your list with this annotation. They are listed in the table. hands_on Exercise 2 WebGestalt Repeat the enrichment analysis on Wiki pathways Again, many more tables can be generated in WebGestalt and you should choose the type of enrichment that fits your experimental needs. Data can be saved back to disk for further use. g:Profiler: many organisms but limited resources g:Profiler supports a long list of organisms but has less resources than the other tools since it retrieves functional annotations from Ensembl representing GO terms, pathways, networks, regulatory motifs, and disease phenotypes. It is very regularly updated. hands_on Exercise 1 g:Profiler How to calculate enrichment in a list of genes ? solution answer For Enrichment analysis you need to use the g:GOSt tool. Upload query: a file with gene IDs (in this example Ensembl IDs - one per line). In the Functional Database boxes select pathway and KEGG In the Gene ID type box select the correct ID type Select the Organism you need Click the Run query button This tool produces visually attractive results. Every dot in the graph represents a functional annotation. Hover your mouse over a dot to show details like the name of the annotation and the corrected p-value. Also the detailed results are very visual. Gene set enrichment analysis Some omics experiments generate a ranked list of genes: genes ranked by differential expression score from a RNA-Seq experiment genes ranked by sensitivity in a genome-wide CRISPR screen mutated genes ranked by a score from a cancer driver prediction method … To analyze these lists, the following steps are taken The genes are divided into groups based on functional annotation (gene sets) For every group enrichment of high or low scores is calculated Groups of related genes are called gene sets: a pathway gene set includes all genes in a pathway. This is why this type of analysis is called GSEA, Gene Set Enrichment Analysis. It assumes a whole-genome ranked list as input. GSEA GSEA is most often done in R or via software that you install on your computer like GSEA from the Broad Institute. GSEA is recommended when ranks are available for all or most of the genes in the genome (e.g. RNA-Seq data). It is not suitable when only a small portion of genes have ranks available (e.g. an experiment that identifies mutated cancer genes). You have to install the tool on your computer. An icon will appear on your desktop. Input files The format of the input file is very important. It should be a tab-delimited text file where: column 1 should contain gene IDs column 2 should contain descriptions but may be NAs next columns should contain normalized counts (one column/sample) Columns must have headers: NAME for column 1 Description for column 2 Sample names for the next columns The first line of the file should be: #1.2 The second line should be: number_of_genes tab number_of_samples Save the file as .gct ! Apart from these data you also need a .cls file with the metadata (grouping info of the samples). This is a space delimited text file: line 1: number_of_samples space number_of_groups space 1 line 2: # space class0_name space class1_name line 3: for every sample 0 or 1 separated by spaces Analysis Originally GSEA was created to analyze microarray results but you can use it for analyzing RNA-Seq data, albeit with some tweaking of the parameter settings. hands_on Exercise 1 GSEA How to perform GSEA on a full list of genes with normalized counts ? solution answer Load the data into GSEA. Load both the .gct and the .cls file. Run GSEA: fill in the parameter settings. Click the question mark (red) on the bottom of the page to view descriptions of these parameters. Use the GO: all (green) gene set Use 10 permutations (green). If all goes well, repeat the analysis with 1000 permutations. Select Collapse (blue): this is necessary for RNA-Seq data to associate the gene IDs of your list to the probes of the chip platform Use gene set permutations (blue). Broad advises phenotype permutations (group labels will be shuffled to create random data to compare with) but they will only work when you have at least 7 samples per group. Choose Human_ENSEMBL_Gene_ID_MSigDB.vX.chip as ChIP Platform (blue). Although you didn’t actually do chips (microarrays) he needs to map the Ensembl Gene IDs in the data file to functional annotations. Click the Run button at the bottom of the page. In the left lower corner of the user interface there’s a section called GSEA reports. It shows the status of analyses run in this session, including the currently running analysis: Click the green text to display the results in a browser. Resources of functional annotation Functional annotations can be very diverse: molecular functions, pathways (genes that work together to carry out a biological process), interactions, gene regulation, involvement in disease… Online enrichment analysis tools often have functional annotation built-in for a limited set of organisms but some tools like WebGestallt also allow to upload your own annotation. Pathguide contains info about hundreds of pathway and molecular interaction related resources. It allows organism-based searches to find resources that contain functional info on the organism you work on. Gene sets based on GO, pathways,omics studies, sequence motifs, chromosomal position, oncogenic and immunological expression signatures, and various computational analyses maintained by the GSEA team of MSigDB. The GSEA tool from Broad will use this database by default. Choosing the right background Functional enrichment methods require the definition of background genes for comparison. All annotated protein-coding genes are often used as default. This leads to false-positive results if the experiment measured only a subset of all genes. For example, setting a custom background is important in analyzing data from targeted sequencing or phosphoproteomics experiments. The appropriate custom background in this example would include all genes in the sequencing panel or all known or all phosphoproteins."},{"title":"02 Linux command line","url":"/topics/linux/tutorials/command-line/tutorial.html","tags":[],"body":"Tutorial on the linux command line We will first hold your hand: type over these commands below step by step, and watch what they do. Use cd to change the current working directory (user bits). To create your own directories use the mkdir (make directory) command. $ cd ~ $ mkdir sequences $ cd sequences $ mkdir proteins $ cd proteins $ pwd /home/bits/sequences/proteins $ cd ../.. $ pwd /home/bits To create a new file, use the touch command: $ cd ~/sequences/proteins/ $ touch my_sequence.txt $ ls -l -rw-r--r-- 1 bits users 0 Sep 19 15:56 my_sequence.txt In the last command above, the -l (a lowercase “L”, not a “1” (one)) option was used with the ls command. The -l indicates that you want the directory contents shown in the “long listing” format. Most commands accept options. But which options can you use? The command man helps you. Type man followed by the command name. E.g. man ls to see what options are available for the ls command. You get a the list of options. Keep pressing Space until the page stops scrolling, then enter “q” to return to the command prompt. Luckily, most tools have the –help option. (ls –help for example). These 2 methods should help you further. To see what options can be used with ls, enter man ls. $ man ls To delete a file, use the rm (remove) command: $ cd ~/sequences/proteins/ $ ls my_sequence.txt $ rm my_sequence.txt $ ls $ To remove a directory, use the rmdir (remove directory) command. The directory needs to be empty to do this. $ cd ~/sequences/ $ ls proteins $ rmdir proteins $ ls $ To copy a file, use the cp (copy) command: $ cd ~/sequences $ touch testfile1 $ ls testfile1 $ cp testfile1 testfile2 $ ls testfile1 testfile2 To rename a file, or to move it to another directory, use the mv (move) command: $ cd $ touch testfile3 $ mv testfile3 junk $ mkdir testdir $ mv junk testdir $ ls testdir junk To download a file, use the wget command: $ cd ~/Downloads $ wget http://data.bits.vib.be/pub/trainingen/Linux/sample.sam $ ls sample.sam $ The commands covered so far represent a small but useful subset of the many commands available on a typical Linux system. Make a project folder structure We assume that start from your home folder. Create the following directory structure Figure 1: Tree $mkdir -p docs/{pdf/{man,readme},html/{man,readme}} The ‘{‘ and ‘}’ can group arguments but you can also create the structure step by step. The little tree figure above is created with the ‘tree’ command. Display such a tree. tree /home/bits/docs/ Downloading and storing bioinformatics data Create a project folder The first thing to do when you start a bioinformatics project, is to create a structure of folders to put your data in an organised fashion. Downloading As an example, we will download the rice genome from the Rice Annotation Project database. But first create the folder structure. Create following folder structure. $ mkdir \"Rice Example\" $ cd Rice\\ Example $ mkdir Genome\\ data $ cd Genome\\ data $ mkdir Sequence $ mkdir Annotation $ cd ** Be aware of white spaces on the command line!** On the command line, programs, options and arguments are separated by white spaces. If you choose to use a folder name containing a white space, it will interpret every word as an option or argument. So you have to tell Bash to ignore the white space. This can be done by: putting strings between quotes like ‘ or “ escape a white space with . See the examples above. Hence, you might save yourself some trouble (and typing!) by putting _ instead of white spaces in names. Also make sure to use tab expansion, wherever possible! Download the genome data directly on the command line You can fetch the rice genome from this link. Download the genome data to the “Rice example”/”Genome data”/Sequence folder. Use wget to download from the link. Right-click on the download link, and copy the download link. The download link is: http://rapdb.dna.affrc.go.jp/download/archive/build5/IRGSPb5.fa.masked.gz Go the directory and execute wget $ cd ## to go back to the home directory $ cd Ric $ cd Gen/Seq $ wget http://rapdb.dna.affrc.go.jp/download/archive/build5/IRGSPb5.fa.masked.gz --2013-10-15 09:36:01-- http://rapdb.dna.affrc.go.jp/download/archive/build5/IRGSPb5.fa.masked.gz Resolving rapdb.dna.affrc.go.jp (rapdb.dna.affrc.go.jp)... 150.26.230.179 Connecting to rapdb.dna.affrc.go.jp (rapdb.dna.affrc.go.jp)|150.26.230.179|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 122168025 (117M) [application/x-gzip] Saving to: `IRGSPb5.fa.masked.gz' 100%[======================================>] 122,168,025 973K/s in 2m 40s 2013-10-15 09:38:42 (747 KB/s) - `IRGSP-1.0_genome.fasta.gz' saved [122168025/122168025] $ ls IRGSPb5.fa.masked.gz Allright. We have fetched our first genome sequence! Did your data get through correctly? Large downloads or slow downloads like this can take a long time. Plenty of opportunity for the transfer to go wrong. Therefore, large downloads should always have a checksum mentioned. You can find the md5 checksum on the downloads page. The md5 checksum is an unique string identifying (and calculated from) this data. Once downloaded, you should calculate this string yourself with md5sum. $ md5sum IRGSPb5.fa.masked.gz 7af391c32450de873f80806bbfaedf05 IRGSPb5.fa.masked.gz You should go to the rice genome download page, and compare this string with the MD5 checksum mentioned over there. You can do this manually. Now that you know the concept of checksums, there is an easier way to verify the data using md5sum. Can you find the easier way? Search how to use md5sum to check the downloaded files with the .md5 file from the website. Check the man page $ man md5sum It does not say much: in the end it refers to $ info coreutils 'md5sum invocation' Reading the options, there is one option sounding promising: `-c' `--check' Read file names and checksum information (not data) from each FILE (or from stdin if no FILE was specified) and report whether the checksums match the contents of the named files. This way we can check the download: $ wget http://rapdb.dna.affrc.go.jp/download/archive/build5/IRGSPb5.fa.masked.gz.md5 --2013-10-15 09:47:02-- http://rapdb.dna.affrc.go.jp/download/archive/build5/IRGSPb5.fa.masked.gz.md5 Resolving rapdb.dna.affrc.go.jp (rapdb.dna.affrc.go.jp)... 150.26.230.179 Connecting to rapdb.dna.affrc.go.jp (rapdb.dna.affrc.go.jp)|150.26.230.179|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 55 [application/x-gzip] Saving to: `IRGSPb5.fa.masked.gz.md5' 100%[======================================>] 55 --.-K/s in 0s 2013-10-15 09:47:03 (757 KB/s) - `IRGSPb5.fa.masked.gz.md5' saved [55/55] $ ls IRGSPb5.fa.masked.gz IRGSPb5.fa.masked.gz.md5 $ md5sum -c IRGSPb5.fa.masked.gz.md5 IRGSPb5.fa.masked.gz: OK Ensuring integrity of downloads A handy tool to use is the DownThemAll addon for Firefox, in which you have to provide the checksum at the time of download. It will automatically check whether the download is finished. The Short Read Archive (SRA), storing NGS data sets, makes use of Aspera to download data a great speeds, ensuring integrity. To download from SRA using aspera in linux, follow the this guide from EBI. Extracting the data What type of file have you downloaded? $ file IRGSPb5.fa.masked.gz IRGSPb5.fa.masked.gz: gzip compressed data, was \"IRGSPb5.fa.masked\", from Unix, last modified: Wed Aug 18 03:45:47 2010 It is a compressed file. Files are compressed to save storage space. Before using these files, you have to decompress them. What can you do with this type of file? Check the command apropos. $ apropos gzip gzip (1) - compress or expand files lz (1) - gunzips and shows a listing of a gzip'd tar'd archive tgz (1) - makes a gzip'd tar archive uz (1) - gunzips and extracts a gzip'd tar'd archive zforce (1) - force a '.gz' extension on all gzip files apropos is a command that helps you discover new commands. In case you have a type of file that you don’t know about, use apropos to search for corresponding programs. Decompress the file. Check the man page of gzip. From the man page:gunzip [ -acfhlLnNrtvV ] [-S suffix] [ name … ] $ gunzip IRGSPb5.fa.masked.gz $ ls IRGSPb5.fa.masked IRGSPb5.fa.masked.gz.md5"},{"title":"03 Linux file system","url":"/topics/linux/tutorials/file-system/tutorial.html","tags":[],"body":"Tutorial on the linux file system Which protocol achieves highest compression ratio? Let’s do a little test. Download this compressed file. Create a folder named ‘Compression_exercise’ in your home. Copy the downloaded tar.gz to it. $ cd $ mkdir Compression_exercise $ cp Downloads/data_linux_training.tar.gz Compression_exercise/ Unpack the data_linux_training.tar.gz file. $ tar -xvzf data_linux_training.tar.gz Alternative: you can specify the options without the ‘-‘ sign. $ tar xvfz data_linux_training.tar.gz Decompress the file DRR000542_1.fastq.subset.gz $ gunzip DRR000542_1.fastq.subset.gz Copy the DRR000542_1.fastq.subset file to a new file called ‘bzip2_test.fastq’. Compress this file with bzip2. $ bzip2 bzip2_test.fastq Tip! If you would like to know how long the command took to finish, use “time” $ time bzip2 bzip2_test.fastq real 0m5.878s user 0m5.728s sys 0m0.112s Three different times are given. What matters to you is the line ‘real’, also called the wall-clock time. Copy DRR000542_1.fastq.subset file to a new file called gzip_test.fastq and compress with gzip. $ time gzip gzip_test.fastq real 0m5.878s user 0m5.728s sys 0m0.112s A relatively unknown package is lrzip, ‘long range zip’, which achieves very good results on big files. Let’s try that one also! Copy DRR000542_1.fastq.subset file to a new file called lrzip_test.fastq and compress with lrzip. $ lrzip lrzip_test.fastq The program 'lrzip' is currently not installed. You can install it by typing: sudo apt-get install lrzip apt-get is the command line tool to install software on Debian distro’s. Equivalent to the software center. $ sudo apt-get install lrzip [sudo] password for joachim: Reading package lists... Done Building dependency tree Reading state information... Done The following packages were automatically installed and are no longer required: libnet-ip-perl diffstat libnet-dns-perl libparse-debianchangelog-perl gir1.2-unique-3.0 kde-l10n-engb python-webpy libnet-domain-tld-perl libemail-valid-perl libapt-pkg-perl python-flup kde-l10n-zhcn Use 'apt-get autoremove' to remove them. The following NEW packages will be installed: lrzip 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded. Need to get 159 kB of archives. After this operation, 313 kB of additional disk space will be used. Get:1 http://be.archive.ubuntu.com/ubuntu/ precise/universe lrzip amd64 0.608-1 [159 kB] Fetched 159 kB in 0s (780 kB/s) Selecting previously unselected package lrzip. (Reading database ... 662617 files and directories currently installed.) Unpacking lrzip (from .../lrzip_0.608-1_amd64.deb) ... Processing triggers for man-db ... Setting up lrzip (0.608-1) ... Now we can compress: Output filename is: lrzip_test.fastq.lrz lrzip_test.fastq - Compression Ratio: 6.724. Average Compression Speed: 0.563MB/s. Total time: 00:03:02.97 real 3m3.026s user 3m1.947s sys 0m0.804s Compare the sizes of the different resulting compressed files. $ ls -lh *zip* -rw------- 1 bits bits 17M Oct 22 14:06 bzip2_test.fastq.bz2 -rw------- 1 bits bits 21M Oct 22 14:06 gzip_test.fastq.gz -rw------- 1 bits bits 104M Oct 22 14:06 lrzip_test.fastq -rw------- 1 bits bits 16M Oct 22 14:10 lrzip_test.fastq.lrz Decide for yourself whether the extra time needed for higher compression is worth the gain in compression. Put the three files in a newly created folder ‘results’, and make an archive of it. $ mkdir results $ mv *{bz2,q.gz,lrz} results/ $ ls results/ bzip2_test.fastq.bz2 gzip_test.fastq.gz lrzip_test.fastq.lrz $ tar cvf results.tar results/ $ rm -rf results/ $ ls -lh total 281M -rw------- 1 bits bits 104M May 4 2011 ERX000016.test.fastq -rw-r--r-- 1 bits bits 21M Oct 22 14:02 ERX000016.test.fastq.tar.gz -rw------- 1 bits bits 104M Oct 22 14:06 lrzip_test.fastq -rw-r--r-- 1 bits bits 53M Oct 22 14:28 results.tar Symbolic links Symbolic links (symlinks) point to a file, making the file accessible in another directory than where the file is. So you can avoid copying! When the original file is deleted, the symlink is dead. When you remove the symlink, the original file is still present. The syntax for symbolic links is: $ ln -s /home/bits /data/large.fastq /home/bits /Projects/ProjectA/ Tip: when using ln, preferably provide absolute paths. If you want to use relative paths, make sure first going to the directory you want the link to be in, and create the link using a relative path (using ‘.’ and ‘..’ to make the path). Removing symbolic links as such: $ unlink /home/bits /Projects/ProjectA In contrast, there is also something as a “hard link” (ln without the -s option). When you delete a hard link, the file to which it referred is gone. So ‘ln -s’ is mostly used. Linking data instead of copying In the Rice Example directory (should be available under your home): download this annotation file into the ‘Genome data’/’Annotation’ directory. Make a symbolic link to this file in the ‘Genome data’/’Sequence’ directory. Read the first 10 lines from the symbolic link file. When you have tried yourself, see the solution. $ cd Rice\\ Example/ ~/Rice Example $ ls bin Genome data ~/Rice Example $ cd Genome\\ data/Annotation/ ~/Rice Example/Genome data/Annotation $ ls ~/Rice Example/Genome data/Annotation $ wget http://rice.plantbiology.msu.edu/pub/data/Eukaryotic_Projects/o_sativa/annotation_dbs/pseudomolecules/version_7.0/all.dir/all.gff3 --2013-10-28 11:45:26-- http://rice.plantbiology.msu.edu/pub/data/Eukaryotic_Projects/o_sativa/annotation_dbs/pseudomolecules/version_7.0/all.dir/all.gff3 => `all.gff3' Resolving http://rice.plantbiology.msu.edu (http://rice.plantbiology.msu.edu)... 35.8.196.190 Connecting to http://rice.plantbiology.msu.edu (http://rice.plantbiology.msu.edu)|35.8.196.190|:21... connected. Logging in as anonymous ... Logged in! ==> SYST ... done. ==> PWD ... done. ==> TYPE I ... done. ==> CWD (1) /pub/data/Eukaryotic_Projects/o_sativa/annotation_dbs/pseudomolecules/version_7.0/all.dir ... done. ==> SIZE all.gff3 ... 81498659 ==> PASV ... done. ==> RETR all.gff3 ... done. Length: 81498659 (78M) (unauthoritative) 100%[======================================>] 81,498,659 1.34M/s in 65s 2013-10-28 11:46:33 (1.20 MB/s) - `all.gff3' saved [81498659] ~/Rice Example/Genome data/Annotation $ ls .. Annotation Sequence ~/Rice Example/Genome data/Annotation $ cd ../Sequence/ ~/Rice Example/Genome data/Sequence $ ln -s ../Annotation/all.gff3 . ~/Rice Example/Genome data/Sequence $ ls -l total 381300 lrwxrwxrwx 1 bits bits 22 Oct 28 11:49 all.gff3 -> ../Annotation/all.gff3 -rw-r--r-- 1 bits bits 390444160 Mar 8 2013 IRGSPb5.fa.masked -rw-r--r-- 1 bits bits 55 Mar 8 2013 IRGSPb5.fa.masked.gz.md5 ~/Rice Example/Genome data/Sequence $ head all.gff3 ##gff-version 3 Chr1 MSU_osa1r7 gene 2903 10817 . + . ID=LOC_Os01g01010;Name=LOC_Os01g01010;Note=TBC%20domain%20containing%20protein%2C%20expressed Chr1 MSU_osa1r7 mRNA 2903 10817 . + . ID=LOC_Os01g01010.1;Name=LOC_Os01g01010.1;Parent=LOC_Os01g01010 Chr1 MSU_osa1r7 exon 2903 3268 . + . ID=LOC_Os01g01010.1:exon_1;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 3354 3616 . + . ID=LOC_Os01g01010.1:exon_2;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 4357 4455 . + . ID=LOC_Os01g01010.1:exon_3;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 5457 5560 . + . ID=LOC_Os01g01010.1:exon_4;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 7136 7944 . + . ID=LOC_Os01g01010.1:exon_5;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 8028 8150 . + . ID=LOC_Os01g01010.1:exon_6;Parent=LOC_Os01g01010.1 Chr1 MSU_osa1r7 exon 8232 8320 . + . ID=LOC_Os01g01010.1:exon_7;Parent=LOC_Os01g01010.1 Introduction: symbolic links to easily install manually applications If a package is not available via a package manager, manual installation might be an option. I put manually applications in ‘’‘/opt’’’. Next, I link them to a correct location on our system, usually ‘’‘/usr/local/bin’’’. Below you have some examples of this, which you can try out yourself. If you want to manually install apps, ‘’‘/opt’’’ is the advised directory. However, only the administrator (‘root’) can access /opt. You can check that the /opt directory belongs to root with ls -l /opt To be able to copy and write stuff into /opt, we need root permissions. To do so, precede your commands with ‘'’sudo’’’, as exemplified in the next exercise below. When we do that, our password will first be asked. Next, the command is executed with root permissions. In this way, we can edit contents in root-owned directories! You are a sudoer! Transpose, a tool to transpose Transpose is an extremely convenient text tool to transpose tabular data. We will use it later. The code is hosted on SourceForge. Download transpose installation file (zip) via the browser. Copy them to /opt using sudo cp. Go to the Sourceforce website with the browser, and click on the Download button. Downloads $ sudo cp transpose-2.0.zip /opt [sudo] password for joachim: Downloads $ We need to precede the ‘‘cp’’ command with the ‘‘sudo’’ command, since only the root user can copy into ‘‘/opt’’. Unpack the installation in /opt, compile the binary and test it with ‘tranpose –help’. Use sudo to do so. $ pwd /opt $ ls trans* transpose-2.0.zip $ sudo unzip transpose-2.0.zip Archive: transpose-2.0.zip creating: transpose-2.0/ creating: transpose-2.0/win32-bin/ inflating: transpose-2.0/win32-bin/transpose.exe creating: transpose-2.0/src/ inflating: transpose-2.0/src/transpose.c inflating: transpose-2.0/README The zip file is now unpacked. Let us now compile the code. ALWAYS have a look at the README file for this. $ cd transpose-2.0 $ head README To Compile: gcc transpose.c -o transpose To Install - Just copy into your path. e.g.: cp transpose /usr/local/bin/ $ cd src/ $ sudo gcc transpose.c -o transpose The program gcc compiles the human readable code in the file transpose.c and produces a binary file out of it, called transpose. We can now run the binary file from within the directory. $ ./transpose --help Description: This software is released under the GPL license Reshapes delimited text data - amongst other things, it can transpose a matrix of plain text data. Create a symbolic link to the newly created binary to /usr/local/bin. This directory collects binaries/commands to be used on the command line. $ sudo ln -s /opt/transpose-2.0/src/transpose /usr/local/bin $ which transpose /usr/local/bin/transpose"},{"title":"01 Linux installation and training material","url":"/topics/linux/tutorials/introduction-installation/tutorial.html","tags":[],"body":"What is Linux? Linux is a very popular operating system in bioinformatics. In this training you will learn why that is and how it can help you with your bioinformatics analysis. After this training you will be able to: install software on Linux use command line to run tools use command line to handle files write small scripts to automate your analysis Linux installation Live modus Want to test a Linux distribution? Follow this procedure: Grab an USB key and put your Linux distribution (e.g. Ubuntu) on it. Boot your computer from that bootable USB key, and you have a full linux OS to play around with. This ‘live modus’ is an easy way to test the new stuff linux has to offer. Before you test anything else, check if your hardware works (printer, sound,…) and check internet connection. Secondly, do you like the desktop environment? Does it suit your needs? Play around and test. Done testing? Just reboot your computer, remove the USB key, and the original operating system will start up again as if nothing has happened… Virtual machine Go to https://www.virtualbox.org and choose Downloads. Download the correct installer for your platform and install VirtualBox on your computer. Sometimes VirtualBox displays errors when starting. Or trying VirtualBox for the first time, a virtual machine might not start. These problems might be related to not having virtualization enabled on your CPU. All the latest processors and motherboards support virtualization technology (vt-x/amd-v). It many cases, VirtualBox requires this to be enabled. To do so, you have to reboot your computer, and get into the BIOS menu. In the BIOS menu, you should enable virtualization. Where this setting is located is different between computers, so check your hardware vendor for the BIOS options, or browse around in your BIOS menu until you find it. Most of the times it is named in a decent way. Enable the option, and boot your computer. We need to download an .iso file, which is a (binary) copy of an installation DVD containing your distribution of choice. You can find it in the downloads section of the distribution’s web page. You can download it using a direct download, depending on your preference and the options offered by the distribution’s web page. You can run Linux in ‘live modus’ (see instructions above) and install it directly on your virtual machine. Afterwards you have to reboot your virtual machine to get out of the live modus. Dual boot Multi-booting allows more than one operating system to reside on one computer, for example if you have a primary operating system and an alternate system that you use less frequently. Another reason for multi-booting can be to investigate or test a new operating system without switching completely. Multi-booting allows a new operating system to configure all applications needed, and migrate data before removing the old operating system. Training material slides On the training there is a Linux Ubuntu installation available on a cloud environment. To access Linux we use Google Chrome and the ‘VNC Viewer for Google Chrome’ application. When you launch the application, you have to enter an IP address, this will be mentioned on the training. Additional information Linux Beginner’s Cheat page The practical command line cheat sheet AWK Terminal keyboard shortcuts"},{"title":"01 Installation and support","url":"/topics/eln/tutorials/installation/tutorial.html","tags":[],"body":"Installation Windows Requirements to install E-Notebook 2014: Microsoft Windows MS Office, Adobe Reader (or similar) ChemBioDraw (optional - see STEP 2) Valid VIB login credentials. Check your login and password on https://storefront.vib.be/. STEP 1: E-Notebook 2014 Browse to https://eln.vib.be/clickonce/ Click “Install” and open the file After the installation, the software is automatically launched and the login window appears Log in with your VIB credentials (see requirements) Close E-Notebook after successful launch: File - Exit or ‘X’ in the right upper corner Generate a shortcut on the desktop (right click - Send to - Desktop): All Programs - PerkinElmer - E-Notebook 2014 Client Install ChemBioDraw (STEP 2) STEP 2: ChemBioDraw Note: In case you only reinstall the ELN client, you don’t have to reinstall the ChemBioDraw component Download the ChemBioDraw installation file from the same website as E-Notebook 2014: https://eln.vib.be/clickonce Start the installation Install ChemBioDraw ActiveX component in suggested destination Follow the installation wizard instructions Click on “Install” and subsequently on “Finish” Why use ELN throught Citrix on Windows? Some older Windows versions cause problems with the E-Notebook 2014 Client installation. STEP 1: Citrix Workspace app Browse to [http://www.citrix.com www.citrix.com] Click on Download Select Citrix Workspace app from the list of possible downloads Download and install Citrix Workspace app STEP 2: Launch ELN online Browse to https://storefront.vib.be Login with your VIB credentials Launch the ELN application by clicking on the icon If your browser asks to download and open an .ica file, please agree Citrix Workspace will open en launch the application MacOS, Linux, mobile devices STEP 1: Citrix Workspace app Browse to [https://www.citrix.com www.citrix.com] Click on Download Select Citrix Workspace app from the list of possible downloads Download and install Citrix Workspace app After the installation on Linux execute the following command: sudo cp -a /usr/share/ca-certificates/mozilla/DigiCert_Assured_ID_ Root_ CA.crt /opt/Citrix/ICAClient/keystore/cacerts/ STEP 2: Launch ELN online Browse to https://storefront.vib.be Login with your VIB credentials Launch the ELN application by clicking on the icon If your browser asks to download and open an .ica file, please agree Citrix Workspace will open en launch the application Support Call us at +32 (0)9 248 16 15 Mail us at eln@vib.be"},{"title":"01 Fundamental Image Aspects","url":"/topics/gimp-inkscape/tutorials/fundamental-image-aspects/tutorial.html","tags":[],"body":"Bitmap vs Vector images Bitmap Pixels in a grid/map Resolution dependent Restricted to rectangle Resizing reduces visual quality Easily converted Minimal support for transparency Popular file formats: BMP, GIF, JPEG, JPG, PNG, TIFF Bit depth or color depth is the amount of data assigned to every pixel (e.g. 1-bit = black/white, 4-bit = 16 colors/shades of grey, etc.) The more data, the more realistic your image will be. More data per pixel also means larger files. Vector Scalable Resolution independent No background Inappropriate for photo-realistic images XML based text format Popular file formats: SVG, AI, CGM, DXF, WMF, EMF Pixels Resolution = number of pixels = how much detail an image holds PPI: pixel per inch Screen pixel density (monitor/smartphone) Tells you how large an image is DPI: dots per inch Print-out dots density (inkjet/laser printer) Printer settings An image at 300 PPI will look fine on a monitor, but printing is another matter! Print it on paper and you will notice the difference between 72 DPI and 300 DPI File formats and compression JPG/JPEG Supports 26 million colours (24 bit) Lossy compression (information is lost from original file) Small file size (compressed) Photographs BMP Supports 8/16/24-bit Uncompressed file format Large file size TIFF Tagged Image File Format All colour and data information is stored Uncompressed (lossy and lossless compression is possible) Very large file size GIF Graphics Interchange Format Only 256 colours possible (8-bit) Replace multiple occuring patterns into one Small file size Animation PNG Portable Network Graphics 256 / 16M colours 8-bit transparancy Lossless compression SVG Scalable Vector Graphics XML-based format Lossless data compression Creatable and editable with a text editor Can contain both bitmap and vector data PDF Portable Document Format Can contain both bitmap and vector data RAW/DNG Digital Negative (DNG) is a universal RAW file format Raw image file (without white balance, color saturation, contrast settings, …) RAW files can be camera brand specific Large file size Multiple options without taking the picture again Publication vs Presentation Key features for publications: Raw/uncompressed image file (e.g. TIFF) High quality image (300 PPI) and resolution Lossless compression (e.g. PNG) Compression is sometimes allowed (check journal website!) Key features for presentation: Normal quality image (72 PPI) and smaller resolution (max width: 1920 pixels) Compression is allowed (e.g. JPEG) Smaller file size Guidelines on image editing Scientific accepted image manipulations are described in guidelines. VIB also has a document to guide you in what is and what isn’t acceptible when adjusting your images. Some examples are: No specific feature within an image may be enhanced, obscured, moved, removed or introduced Adjustments of brightness, contrast or color balance are acceptable if they are applies to the whole image as long as they do not misrepresent information in the original Grouping of images from different parts of the same or different gel, fields or exposures must be made explicit by the arrangement of the figure (dividing lines) The original data must be available by the author when asked to provide it, otherwise acceptance of the publications may be revoked you can find all the VIB guidelines here."},{"title":"02 Inkscape","url":"/topics/gimp-inkscape/tutorials/inkscape/tutorial.html","tags":[],"body":"What is Inkscape? Inkscape is professional quality vector graphics software which runs on Windows, Mac OS X and GNU/Linux. It is used by design professionals and hobbyists worldwide, for creating a wide variety of graphics such as illustrations, icons, logos, diagrams, maps and web graphics. Inkscape uses the W3C open standard SVG (Scalable Vector Graphics) as its native format, and is free and open-source software. During this training we will use Inkscape 0.92 on Windows. To download the most recent version, browse to the Inkscape Download page. For Windows 10 S: the Inkscape app is also available in the Microsoft Store. External training material Online Inkscape tutorials. Nick Saporito Inkscape tutorials for beginners Nick Saporito Inkscape intermediate/advanced tutorials User Interface Inkscape is a single-window program. Drawing tools are on the left hand side, option docks are on the right. In the central window, you have the drawing area with default an A4 page as document layout. To select another format for e.g. posters, go to File - Document Properties. Next to the document size, you can adjust the background colour (default: transparant). Import Images You can import scalable vector graphic files (.svg) and also GraphPad Prism graphs (.emf or .pdf format). Inkscape is not used for editing images like GIMP. If you import bitmap images, note that they are not scalable like vector objects! Drawing lines and objects You can draw a line with the Draw Bezier tool. You can make your own shape or just draw a line or path. On top of your drawing area you can select the Mode: Regular Bezier curves, Spiro paths, straight line segments and paraxial line segments. When selecting the straight line mode, you can hold the Ctrl button to make your line snap every 15 degrees around your first/previous point. You can draw shapes by using the Rectangle tool, Ellipse tool and the Create Stars and Polygons tool. On top of the drawing area you can specify your polygon and star properties, size and lock aspect ration. Here is the Crtl key useful as well for creating squares, circles or specify the position of your object. When you have an object (polygon or others) you can select a color for the stroke and inside of the object. Selecting an object using the Selection tool will give you more options on top of the view area. You have the option to rotate, flip, change dimensions and XY position (in different units). You can change the position of the selected object compared to others (move up/down). Paths A path consist of lines and nodes. These lines can be straight or curved and you can make an object using paths ( closed path). When in Path mode you have several options; add or remove a node, joining or breaking nodes apart and changing the node properties. You can also change the segment (line between nodes) properties with the options on top of the screen. You can convert an object into a path to gain more flexibility by selecting the object and go to Path – Object to path. Afterwards you can use the object tool or the path tool to manipulate the object. Fill and stroke Paths, lines and objects can be given a plain color, patterns, gradient color or left blank/transparent. You can also configure the stroke style and color. Click Object – Fill and Stroke to see all the options. Paths/lines can be transformed into arrows using the Stroke style option Markers. Text At the left there is also a Text tool available. With this tool you can create and change text, it’s colour, font, style and size. After entering text, you’re able to manipulate it like an object. You can also attach text into a frame by selecting both objects and click on Text – Flow into Frame. You can also align text to a path. Select both text and path and click Text – Put on Path. Once the text in aligned to the path it stays adaptable and can be removed from the path; Text - Remove from Path. Text is an object at first. When you select Path - Object to path you can modify your text like any other object that is converted into a path. Grouping, aligning and arranging object/paths To group several object you must select them all (hold Shift) and select Object – Group. To unite several paths you must select Path – Combine. Both options are the same and allow you to manipulate objects/paths as one. Both actions can be reversed (Ungroup / Break Apart). Several object must be aligned before you group them, think of text inside a box. To display the options, go to Object - Align and Distribute. When multiple objects are selected, you can align the top, bottom, left and right edges of the objects. Aligning on the central axes is also possible, this in both horizontal as vertical direction. The aligned objects always need an anchor, this can be changed in the box on top of the toolbox (Relative to:). This anchor can be an object (first, last, smallest or biggest) or the page, a selection or the complete drawing. Distributing objects works in a similar way, but manages the space between objects. For paths you can only align the nodes. Aligning or distributing objects allows you to manipulate the X and Y position of your objects. There is also a virtual Z axis. When you have multiple objects with different colours, you can move the one above the other. Every new object you draw will be on top of all the rest. To raise an object one step or to the top, you can use the buttons on top of your screen. The same can be done to lower an object one step or to the bottom. Path Effects and operations When you want to distribute/multiply an object along a guideline, there is a tool called Path Effects. First draw and select the object or group of objects and past it in the clipboard (Ctrl + C). Draw or select your path (guideline) and select Path – Path Effects. Click on the ‘+’ sign and select the effect Pattern Along Path. In the new box on the right: select ‘Repeated’ on the option Pattern copies. Now click on ‘Paste path’ to paste the object you want to multiply. Note that only the shape is pasted, not the color. When adjusting the color, it will affect the entire path. To copy the colour, use Crtl+C again on your original, select your path of objects and go to Edit - Paste Style - Paste Style. There are also standard patterns to distribute along a path. When clicking on the ‘+’ sign to add an effect, select ‘Gears’ or ‘Hatches (rough)’. Each of these effects have their own options to create an effect and to adjust the pattern. When it comes to paths, you can do much more than combining them. When you want to cut one shape out of another shape, you can use the options in the Path menu; Union, Difference, Intersection, Exclusion, Division and Cut Path. Diagrams To make a diagram with objects (circles, rectangles, stars, etc.) connected by lines, there is the Diagram connector tool. First you must draw and align the objects to create your diagram. Then select the Diagram connector tool. Every object can be selected by clicking in the white box in the middle of the object. Once connected the lines will follow the object if you move it to another place. The lines can be used as a path, therefore you can also modify them to e.g. dashed lines, arrows, etc. Exercises hands_on Hands-on: Exercise 1 Image 1 PNG: Image 1 Image 1 SVG: Image 1 SVG Task: Reproduce the top strand. Afterwards, reproduce the bottom strand using the first one. hands_on Hands-on: Exercise 2 Image 2 PNG: Image 2 Image 2 SVG: Image 2 SVG Task: Reproduce one of the sets of this image. Afterwards, reproduce the others using the first set. hands_on Hands-on: Exercise 3 Image infographic 1: Image 1 Image infographic 2: Image 2 Image infographic 3: Image 3 Task: Try to reproduce one of these images using the video tutorial series from Nick (see top of this page)."},{"title":"02 GIMP","url":"/topics/gimp-inkscape/tutorials/gimp/tutorial.html","tags":[],"body":"What is GIMP? GIMP is short for GNU Image Manipulation Program. It is a free and Open-source, cross-platform image editor available for GNU/Linux, MacOS and Windows operating systems. During this training we will use GIMP 2.10 on Windows. To download the most recent version for your OS, browse to the GIMP Download page. External training material GIMP Manual page. GIMP 2.10 Basics on YouTube Nick Saporito GIMP Tutorials User Interface GIMP has a ‘Single-window’ mode, this allows you to switch from multiple windows (for e.g. multiple monitors) to a single window. When the ‘Single-window’ mode is disabled, you have separate windows for toolboxes, view area and dockable dialogs. When enabled you have one window with all tools, options and dockable dialogs attached to the central view area. For beginners, we would advise the ‘Single-window’ enabled. On the left panel you have the ‘Toolbox’ (if not present: Windows - Toolbox or press Ctrl + B) and underneath the ‘Tool Options’ dialog. Selecting a tool will result in a different Tool Option bar. Every tool has his own set of parameters and functions, best to keep them close to each other. On the right-hand panel you can find other ‘dockable dialogs’. These are easy to move, remove and re-introduce if necessary. To get a list of all ‘dockable dialog’ go to Windows – Dockable Dialogs - … . If you want a full screen view of your image select Windows – Hide Docks. Import data and image properties To import an image: File – Open When you select an image (any file type) in the import window, you get a preview and information on the right side. Click Open and the image(s) will be displayed in the middle box at zoom level 100% (1 pixel image = 1 pixel screen) or fitted to your windows. To zoom use Ctrl + mouse scroll up or down. Multiple images in GIMP are displayed in different tabs on top of the View Area. Before you export your image, make sure it has the right resolution and pixel density. Image - Image Properties will give you all the information your image holds. This information can be very useful when you open an image from an unknown source. Selection Rectangular selection has several options and shortcut keys. The first icons in the tool options are the selection modes: add to selection (Shift), subtract from selection (Ctrl) and intersect with selection (Shift+Ctrl). More options are: feathering edges, rounding of the corners, expand from center, lock aspect ratio, size and position and if necessary to highlight the selection). The Ellipse selection tool has more or less the same options. There are other selection tools available: Free Selection, Select by Color, Fuzzy Selection, Scissor Selection, Foreground Selection. Those tools have different tool options and are only used in specific cases. Transforming There are several ways to transform your image or selection; rotating, scaling, shearing and flipping. You can transform a selection, a layer or the image. When using the rotation tool, you have several options in the dockable dialog below. An important option is “Clipping” this will change the aspect ratio of your image after rotating. Another way of rotating an entire image is: Image – Transform – … then you have the option to flip (horizontal/vertical) or rotate (90°/180°). The entire image will be rotated including the selection and image orientation. Layers Make sure you have the dockable dialog ‘Layers’ in your window. All options for layers can be found in the menu bar “Layer”. You can make a new blank layer or duplicate the current layer (e.g. copy of original image to compare or as back-up). In the dockable dialog you can hide or show a layer (eye button), rename them or move them up and down in the layer stack. If you want to link/connect two or more layers, you can use the chain button (next to the eye button). To copy a selection to a new layer, perform a regular copy/past action of that selection (Ctrl+C and then Ctrl+V) and select Layer - To New Layer If you want to merge all layers into one layer you can select Image – Merge Visible Layers. Brightness and contrast In the menu bar you can find Colors . This menu has multiple option to manipulate your image; Color Balance will change the cyan, magenta and yellow color levels of your image Brightness and Contrast will change brightness and contrast and you can save these settings as a favorite Threshold will reduce your image to two colors by using a threshold value Adjust color curve will change the gamma setting of your image Posterize will change the number of colors (2-256) Guides and cropping You can split your image in different sub-images. This can be done by using ‘Guides’. To create such a break-line, go to Image - Guides - New Guide… or (by Percent)…. You can create a horizontal or vertical guide at the value/percentage you enter. A guide will be displayed as a blue dashed line. To chop your image in multiple parts, go to Filters- Web- Slice (Older versions: Image - Transform - Guillotine). The sub-images will be generates in the folder you selected. If you only want a selection of your image without all the rest you can crop by clicking Image – Crop to Selection or use the Crop tool from the Toolbox. Scaling and print size When you want to scale your image to a smaller resolution you can select Image – Scale Image. There you can scale in pixels (or another unit) and you can lock the aspect ratio (chain symbols). If you want to change the print size to make your image suitable for publication you can select Image - Print Size…. There you can change the dimension/resolution and pixel density of your image. Remove background color If you download an image of a company or university logo, it might have a white (or any other color) background. This can be very annoying when the destination background is different. In order to remove the background color, we first have to add an alpha channel to this image: Layer - Transparency - Add Alpha Channel - If the Alpha channel is already present, skip this step. Now you’re able to get a transparent background using the option: Image - Color to Alpha. In the new window you can select the color which you would like to convert to transparent pixels. You can either select by clicking the color bar or use the color picker icon. Exporting Select File – Export as… If you click on the ‘+’ next to Select File Type, you have a list of all possible extensions in which you can export your image. Each of those file formats has different compression options. Exercises on image manipulations in GIMP hands_on Hands-on: Exercise 1 Source file: [http://data.bits.vib.be/pub/trainingen/GIMP_Inkscape/original_file.tif Image 1] Task: Split this image in 2 parts, one for each gel. Make sure the band are horizontal and export the 2 new images in the same file format as the original. You can adjust brightness and contrast to make all the band more visible. hands_on Hands-on: Exercise 2 Source file: [http://data.bits.vib.be/pub/trainingen/GIMP_Inkscape/Exercise1.1.jpg Image 2] Task: Rotate this image 45 degrees and crop an image of 500x500 pixels out of the original. Make sure the printing resolution is set to 300 ppi and export this image as a PNG file. Adjust brightness and contrast to make this image look better. hands_on Hands-on: Exercise 3 Source file: [http://data.bits.vib.be/pub/trainingen/GIMP_Inkscape/Exercise1.2.jpg Image 3] Task: Cut this image in 4 equal parts. Know that the printing width is 150 mm and the journal demands a minimum op 300 ppi for all 4 images. Also export each of them in a different file formats without losing image quality. Adjust brightness and contrast to your own opinion. hands_on Hands-on: Exercise 4 Source file: [http://data.bits.vib.be/pub/trainingen/GIMP_Inkscape/Exercise1.3.jpg Image 4] Task: Adjust brightness and contract of this images and export it in a way to make the file as small as possible. Use preferably lossless compression (try lossy compression to compare file size), there is no restriction on file formats. Be sure your image is exported with at least 300 ppi. hands_on Hands-on: Exercise 5 Source file: select from the internet Task: Download an image from your most favorite brand and remove the white (or other color) background. Export this new image in a format that support transparent pixels."},{"title":"01 ELN Functionalities","url":"/topics/eln/tutorials/functionalities/tutorial.html","tags":[],"body":"Login When launching the application (Windows: double-click the E-notebook 2014 client icon – Citrix: click on the ELN 2014 icon and open the .ica file, Citrix Workspace will launch the application), you will see the following login window: In order to login on ELN, you need a valid VIB account. The VIB username usually has a format like: firstname lastname. More information on https://help.vib.be or mail eln@vib.be. When clicking on Connect the application will retrieve your data. The Work Offline option is only available with the client installation and will allow you to make adjustments to the data in your Offline folder. Note: when launching the application for the first time, a download of all collections will start, this usually takes 1 or 2 minutes. Layout The layout is resembling to Microsoft Office. It has 3 main parts; the ribbon with options on top, the navigation and history area on the left and the working area on the right. The default starting point is the Home location, this gives an overview of all data in the navigation area on the left and any modified experiments since one month on the right. In the Audit Trail (bottom left) you can find the history of the object selected above. This history allow you to access previous versions of an experiment and retrieve a file in order to bring it back to the present. Every version has a timestamp and operator (= user that pressed the save button). Previous versions of an experiment can**t be modified, only the last version is adjustable. Navigating to your colleagues or Home can be done with the orange icons in the upper left corner. Next to the navigation buttons you find the Save button. When saving you can add annotations as well. Ribbon The Ribbon is where you can find the options corresponding with your selection (navigation area or section). By default, there are three tabs: Home, View and Data. Sections have specific tabs in the ribbon, e.g. Document, Image, Text, Table, Property List, etc. An example can be found below (Text): Project, Notebook, Experiment There are 3 basic levels to organize your data: Project, Notebook and Experiment (see icons below). You can see them as folders with a certain hierarchy. Only an experiment contains files. To add one of the levels click on the icon in the Home tab in the ribbon. Sections An experiment consists of sections, every section is a file or page. To add a section, select the icon in the Home tab in the ribbon. Some sections are hidden behind the Other button. You can add sections automatically by drag and dropping them into your experiment. E-Notebook will recognize Word, Excel and PowerPoint files, PDF documents and images. GraphPad Prism files are not native to E-Notebook and will result in an Ancillary data section, this will happen with any other file type that is not native to the program. General Page Creating a new experiment will give you a blank experiment with only one section, by default this is the General page. This is an example of a General Page: Every lab group has a slightly different version of this General page. The universal parts of this section are the General Information and the Reference to experiment field. In the first field you have the option to enter general properties of your experiment such as start date, project, etc. Adding extra properties is available in the Property List tab in the ribbon. Adding a reference to your experiment can be very useful to link similar experiment to each other or make a series of experiments. This refence can be any experiment within your group. To add a reference, click on the option in the Home tab in the ribbon. As last there are 3 or 4 text boxes to add keywords, aim of experiment, results, specifications or a conclusion. Microsoft Office sections Three MS Office applications are supported in the E-Notebook software: Word, Excel and PowerPoint. All other MS Office files can be uploaded using the Ancillary Data section. For the supported application you can add files using the corresponding section. This will initially display a (print) preview of the file, double-clicking the preview will launch the MS Office application to make adjustments. All other options are displayed in the ribbon: Images Using the Image section in E-Notebook will allow you to import one (1) image file. All common image extensions are supported, camera brand specific files (e.g. RAW or DNG) can be uploaded using a non-file-specific section. Next to the image file itself you can add a title and notes. PDF files and Captured Image Using the PDF section in E-Notebook will allow you to import 1 PDF file. Next to the PDF file itself you can add a description, date and a document name. Ancillary Data (a.k.a. Binder) This non-file-specific section will save 1 file. In order to open the file , you must double-clicking on it, this will launch the according application outside ELN. Closing the external application again (e.g. after making adjustments) will result in this window: Click Continue to save your changes and re-upload the new file in ELN or click Cancel to ignore the changes. Supplementary Data Management (SDM) Files imported in this section will be saved on an internal network drive linked to ELN. This means that files in SDM wont be accessible outside of your research center or university network. Files in the SDM section are not limited to the file size limit of 30 MB. Next to the default list of sections, there are some lab-specific sections for PCR or Western Blot. To add one of these lab-specific sections, click on the **Other icon and select your section. Sharing data and linking experiments Access rights for others To grant a colleague access to your data, you simple select the object and click on the View tab in the ribbon. In the Properties field you click on Security. A new window will appear (left picture). The inherited privileges are default settings, you’re not able to modify this. The assigned privileges on the other hand can be modified by clicking ‘Grant’. By filtering on user group or user you can select the group/person (right picture). The type of privilege can be: read, read and write, full control. You can define this in the next window. Removing the privilege can de done by selecting the person or group and click on ‘Remove’. For both granting or removing access privileges there is no notification system, you have to tell them yourself. Experiment shortcuts When a colleague granted you access to a project/notebook/experiment you can place a link to this object in your own ELN. This makes navigating to this object easier and allows you to group all your collaborations within your own ELN hierarchy. To create such a shortcut, follow these steps: Select the object of interest Right click – Copy Navigate to your own ELN Right-click on the location you want the link to appear Select Paste Reference Note: shortcuts can be removed, the original data however is not deleted. Templates Templates can be created by every user and can be shared with your colleagues. To create a template, follow this procedure: navigate to ‘User Configuration’ – ‘Templates’ create new experiment build your new default experiment/template by adding information/sections save your template Next time you want to create a new experiment, you will have the option to create a blank or template experiment. Search The collection search can be used for users, projects, notebooks and experiments. No content can be found with the search box in the upper right corner. The Advanced Search option can find experiment content. You can find it in ‘Quick Links’ above the navigation pane."},{"title":"02 Experiment design exercises","url":"/topics/qbase-plus/tutorials/experiment-design/tutorial.html","tags":[],"body":"Exercise 1: simple gene expression study In my qPCR experiment I want to study the expression of 12 genes of interest in 8 samples of interest. I want to use 2 PCR replicates for each reaction. How many 96 well plates do I need for this experiment ? I have 12 genes in 8 samples which gives a total of 96 reactions (one plate). I want to perform each reaction twice (2 PCR replicates) so I need two plates. However, I need to include reference genes in my experiment, preferably more than one. I can put these reference genes on a separate plate, I do not have to include them on each plate. Ideally, you need to include 3 reference genes so having 8 samples and 2 replicates this gives an additional 48 reactions. Thus, I need three 96 well plates to perform this experiment. Do I need to include IRCs (inter-run calibrators) ? No, I can easily fit all samples of the same gene on the same plate so I don’t need to include IRCs. Exercise 2: a large study In my qPCR experiment I want to study the pattern of expression of 96 genes (genes of interest and reference genes) in 96 samples of interest, divided into a few groups. I want to use 2 PCR replicates for each reaction. Do I need to include IRCs (inter-run calibrators) ? No, I can fit all samples of the same gene on the same plate so I don’t need to include IRCs. I want to include PCR replicates. Do I need to include IRCs when I work on a 96 well plate ? Yes, I have 192 reactions per gene so I cannot place them on the same plate. Remember that replicates have to be located on the same plate ! Do I need to include IRCs when I work on a 384 well plate ? No, I have 192 reactions per gene so I can even place two genes on the same plate. I want to include no template controls but I don’t want to increase the number of plates. What is the most elegant strategy to make room for including negative controls ? This kind of study screen for expression patterns and requires statistical analysis. Since you have many samples divided over a few groups it means you have many biological replicates so you could easily do without the PCR replicates. By doing so you preserve the biological variability which is often far greater than the technical variation. Exercise 3: how to fill plates ? In my qPCR experiment I want to study the pattern of expression of 5 genes (genes of interest and reference genes) in 38 samples (samples of interest and control samples). I want to use 2 PCR replicates for each reaction. | What is the minimum number of 96 well plates I need for this experiment ? | | :—————————— | | 5 genes * 38 samples * 2 replicates = 380 reactions. I need a minimum of 4 plates for this experiment. If I use the minimum number of 96 well plates do I need to include IRCs ? Yes, 5 genes spread over 4 plates with 72 reactions per gene means that at least one gene will be spread over multiple plates. What can I do to avoid inter-run variability ? I can use 5 plates and fill them with one gene each. They will not be completely filled (72 reactions) but at least I do not have to use IRCs (which are additional reactions that also cost money) and I have no inter-run variation. Suppose there’s only one 96-well plate left in your lab. You have 10 samples (samples of interest + control samples) and you want to make the most of what you have. | How many genes of interest would you measure ? | | :—————————— | | Since you want to make most of what you have, let’s assume you are omitting PCR replicates. Theoretically, you could fit 9 genes on your 96-well plate. However, to avoid pipetting mistakes I would measure only 8 genes so I can work with one row / gene. This is very handy for multichannel pipets. Exercise 4: a growing study In my qPCR experiment I want to study the pattern of expression of 24 genes (genes of interest and reference genes) in 48 samples (samples of interest and control samples). I want to use 2 PCR replicates for each reaction. | How many genes can I analyze on one 384 well plate ? | | :—————————— | | 48 samples * 2 replicates = 96 reactions per gene. I can analyze 4 genes on each 384 well plate. Each week I receive 2 additional samples to analyze. | Do I analyze them immediately after I get them ? | | :—————————— | | No. Since the samples are placed on different plates as in the previous experiment, you have to use IRCs. You typically need 3 IRCs and a no template control sample. It means that if you want to analyze these 2 samples you have to include 4 additional samples for each gene. This is a lot of overhead for just 2 samples ! Try to avoid this: it’s better to wait a few weeks until you have 6 or 8 or even more samples. Exercise 5: a diagnostic copy number screen In diagnostic screens all samples are important: you cannot leave out samples and all measurements need to be of the highest quality possible. In my qPCR experiment I want to study copy number variation of 16 genes (genes of interest and reference genes) and 2 calibrator samples (samples with known copy number). Since we need high quality data we will use 4 technical replicates. Are we going to use sample maximization ? No. In contrast to gene expression studies, where we want to compare expression levels of a gene between different groups of samples, copy number analyses do compare genes. It means that in this case the sample maximization approach (placing all samples of the same gene on the same plate) is not valid. Instead we use a gene maximization approach here (placing same sample for different genes on the same plate). | How many samples can I fit on a 384 well plate ? | | :—————————— | | We have 16 (genes) * 4 (replicates) = 64 reactions per sample. This means that we can fit 6 samples on a 384 well plate: 4 unknowns and 2 calibrators. Exercise 6: fix experiments with bad or missing data In my qPCR experiment I want to study gene expression of 6 genes (3 genes of interest and 3 reference genes) in 20 samples (samples of interest and control samples). I want to use 2 technical replicates. One of my genes of interest failed completely and I want to repeat the measurements for this gene in a new run. Do I need to include IRCs ? No. We can put the 20 samples of the gene that failed on a single plate so we do not have to include IRCs. Do I need to include reference genes ? No. We just repeat all samples for the gene that failed and replace the old data with the new results. One of the reference genes failed completely. What should I do ? Depending on the quality of the two remaining reference genes, you should either do nothing or do the same as in the previous example where one of your genes of interest failed. If the two remaining reference genes are stable you can do the normalization with the two remaining reference genes. Three samples failed completely. What’s the first thing I need to do ? Since they failed completely, they are probably of low quality. Therefore, you have to prepare the samples again, check their quality and then use them for qPCR. Do I need to include IRCs ? Yes. If you want to compare these samples with the samples that didn’t fail, you have to perform inter-run calibration. Three samples failed for one of the genes of interest What is the first question I need to ask ? Is the gene expressed in these samples ? Is it possible the RNA of these three samples was of low quality ? Not likely, the measurements for the other genes in these samples are ok. Three samples failed for one of the reference genes Can I use the measurements of that reference gene in the non-failing samples for normalization ? No, qbasePLUS requires that you use the same reference genes for all samples so you have to discard all samples for that reference gene. Exercise 7: dilution series for calculating amplification efficiencies In my qPCR experiment I want to study 8 new genes for which I had to design new primer pairs in 12 samples (samples of interest and control samples). I want to use 2 technical replicates and 96 well plates. What is the first thing I need to do ? Perform a pilot experiment to determine the amplification efficiencies of these primer pairs. For this I need a dilution series of representative cDNA template. How many dilutions would you include ? A dilution series with 6 dilutions for 8 genes nicely fits into a 96 well plate. A few weeks after my initial qPCR experiment I want to test these 8 genes in a new set of samples. Do I have to repeat the pilot experiment ? No, dilution series do not need to be repeated."},{"skip_index":true,"title":"Search results","url":"/search/","tags":[],"body":""},{"title":"","url":"/CODE_OF_CONDUCT.md","tags":[],"body":"Project Code of Conduct ============================== This code of conduct outlines our expectations for participants within the community, as well as steps to reporting unacceptable behavior. We are committed to providing a welcoming and inspiring community for all and expect our code of conduct to be honored. Anyone who violates this code of conduct may be banned from the community. Our open source community strives to: * **Be friendly and patient.** * **Be welcoming**: We strive to be a community that welcomes and supports people of all backgrounds and identities. This includes, but is not limited to members of any race, ethnicity, culture, national origin, colour, immigration status, social and economic class, educational level, sex, sexual orientation, gender identity and expression, age, size, family status, political belief, religion, and mental and physical ability. * **Be considerate**: Your work will be used by other people, and you in turn will depend on the work of others. Any decision you take will affect users and colleagues, and you should take those consequences into account when making decisions. Remember that we're a world-wide community, so you might not be communicating in someone else's primary language. * **Be respectful**: Not all of us will agree all the time, but disagreement is no excuse for poor behavior and poor manners. We might all experience some frustration now and then, but we cannot allow that frustration to turn into a personal attack. It’s important to remember that a community where people feel uncomfortable or threatened is not a productive one. * **Be careful in the words that we choose**: We are a community of professionals, and we conduct ourselves professionally. Be kind to others. Do not insult or put down other participants. Harassment and other exclusionary behavior aren't acceptable. This includes, but is not limited to: Violent threats or language directed against another person, Discriminatory jokes and language, Posting sexually explicit or violent material, Posting (or threatening to post) other people’s personally identifying information (“doxing”), Personal insults, especially those using racist or sexist terms, Unwelcome sexual attention, Advocating for, or encouraging, any of the above behavior, Repeated harassment of others. In general, if someone asks you to stop, then stop. * **Try to understand why we disagree**: Disagreements, both social and technical, happen all the time. It is important that we resolve disagreements and differing views constructively. Remember that we’re different. Diversity contributes to the strength of our community, which is composed of people from a wide range of backgrounds. Different people have different perspectives on issues. Being unable to understand why someone holds a viewpoint doesn’t mean that they’re wrong. Don’t forget that it is human to err and blaming each other doesn’t get us anywhere. Instead, focus on helping to resolve issues and learning from mistakes. ### Diversity Statement We encourage everyone to participate and are committed to building a community for all. Although we will fail at times, we seek to treat everyone both as fairly and equally as possible. Whenever a participant has made a mistake, we expect them to take responsibility for it. If someone has been harmed or offended, it is our responsibility to listen carefully and respectfully, and do our best to right the wrong. Although this list cannot be exhaustive, we explicitly honor diversity in age, gender, gender identity or expression, culture, ethnicity, language, national origin, political beliefs, profession, race, religion, sexual orientation, socioeconomic status, and technical ability. We will not tolerate discrimination based on any of the protected characteristics above, including participants with disabilities. ### Reporting Issues If you experience or witness unacceptable behavior, or have any other concerns, please report it by contacting Alexander Botzki (bits@vib.be). To report an issue involving Alexander Botzki please email hr@vib.be. All reports will be handled with discretion. In your report please include: - Your contact information. - Names (real, nicknames, or pseudonyms) of any individuals involved. If there are additional witnesses, please include them as well. Your account of what occurred, and if you believe the incident is ongoing. If there is a publicly available record (e.g. a mailing list archive or a public IRC logger), please include a link. - Any additional information that may be helpful. After filing a report, a representative will contact you personally, review the incident, follow up with any additional questions, and make a decision as to how to respond. If the person who is harassing you is part of the response team, they will recuse themselves from handling your incident. If the complaint originates from a member of the response team, it will be handled by a different member of the response team. We will respect confidentiality requests for the purpose of protecting victims of abuse. ### Attribution & Acknowledgements This code of conduct is based on the Open Code of Conduct from the TODOGroup."},{"title":"","url":"/badges/index.html","tags":[],"body":"VIB Bioinformatics Core Bioinformatics Training Collection of courses developed and maintained by VIB Bioinformatics Core help Help Contact us Server Training Badges Collection of badges denoting compatibility with training workflows from the training materials repository. github View on GitHub SCIENCE MEETS LIFE"},{"title":"","url":"/courses/index.html","tags":[],"body":"Lia This page requires JavaScript to be enabled! LiaScript - Problem Hello! Welcome to LiaScript. This site is not fully supported in Internet Explorer 11 (and earlier) versions. As an alternative, you can use either of the options below to browse the site: Use Firefox browser. Here is the download link. Use Google Chrome browser. Here is the download link. Thanks."},{"title":"","url":"/shared/literature.md","tags":[],"body":"# Literature ##Deep sequencing **Zentner and Henikoff (2012):** [Surveying the epigenomic landscape, one base at a time](https://genomebiology.biomedcentral.com/articles/10.1186/gb-2012-13-10-250), (doi:10.1186/gb-2012-13-10-250) - Overview of popular *-seq techniques; very nice description of DNase-seq, MNase-seq, FAIRE-seq etc. **Son and Taylor (2011):** [Preparing DNA Libraries for Multiplexed Paired-End Deep Sequencing for Illumina GA Sequencers](https://www.ncbi.nlm.nih.gov/pubmed/21400673), (doi:10.1002/9780471729259.mc01e04s20) - Paper on multiplexing; describes the individual steps of the Illumina deep sequencing protocols quite in detail **Illumina's technical report** - focuses on [Illumina's sequencing technology](https://www.illumina.com/technology.html); nice educative figures ##NGS data formats - UCSC has a very good overview with brief descriptions of BED, bedGraph, bigWig etc.: https://genome.ucsc.edu/FAQ/FAQformat.html - [VCF format](https://gatkforums.broadinstitute.org/gatk/discussion/1268/how-should-i-interpret-vcf-files-produced-by-the-gatk) (encoding SNPs, indels etc.): Very readable, albeit not exhausting description - Transcriptomes are often saved in [GFF3 format](https://github.com/The-Sequence-Ontology/Specifications/blob/master/gff3.md) (this is what TopHat needs, for example), but just to make things more complicated, GTF is another format used for transcriptome information, too ##Bioinformatic Tools (Linux, R, BEDTools etc.) - Manuals, courses, original papers - Why and how is bioinformatics software special? **Altschul et a. (2013)** [The anatomy of successful computational biology software](https://www.ncbi.nlm.nih.gov/pubmed/24104757), (doi:10.1038/nbt.2721) **(Highly recommended to read!)** - **Bild et al. (2014)** [A Field Guide to Genomics Research](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001744), (doi:10.1371/journal.pbio.1001744) - Very readable introduction about the different caveats of genomics research (with cute cartoons!) **Linux Command Line** - [Linux & Perl Primer for Biologists](http://korflab.ucdavis.edu/Unix_and_Perl/unix_and_perl_v3.1.1.html) - Very entertaining introduction to command line commands and perl scripts with a focus on bioinformatic application, i.e. handling of DNA sequences - [Linux Tutorial for Beginners](http://www.ee.surrey.ac.uk/Teaching/Unix/) - Thorough, but concise online tutorial introducing the very basics of handling the Linux command line - [Writing Linux shell scripts](http://www.freeos.com/guides/lsst/index.html) - Useful for slightly more advanced Linux command line users **R** - [Hands on R course](https://www.uwyo.edu/mdillon/hor.html) - For beginners - R is probably the most widely used open-source statistical software; through our epicenter website you can also access RStudio which provides are very nice interface to working and plotting with R. In fact, most of the plots generated within Galaxy are generated through R scripts, so if you're not happy with the default formats of the Galaxy graphs, definitely have a look at R yourself. The learning curve is steep, but it is worth it. **BEDTools** - [BEDTools Manual](https://bedtools.readthedocs.org) - When working with genomic intervals (e.g. genes, peaks, enriched regions...), BEDTools are invaluable! The manual is a very good read and we refer to it almost daily."},{"title":"","url":"/snippets/add_custom_build.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Adding a custom Database/Build (dbkey) > > - In the top menu bar, go to the *User*, and select *Custom Builds* > > - Choose a name for your reference build `{{ include.name }}` > > - Choose a dbkey for your reference build `{{ include.dbkey }}` > > - Under **Definition**, select the option `FASTA-file from history` > > - Under **FASTA-file**, select your fasta file `{{ include.fasta }}` > > - Click the **Save** button > {: .tip} >"},{"title":"","url":"/snippets/add_tag.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Adding a tag > > * Click on the dataset > > * Click on {% icon galaxy-tags %} **Edit dataset tags** > > * Add a tag starting with `#` > > > > Tags starting with `#` will be automatically propagated to the outputs of tools using this dataset. > > > > * Check that the tag is appearing below the dataset name > > > {: .tip} >"},{"title":"","url":"/snippets/ansible_local.md","tags":[],"body":"> ### {% icon tip %} Tip: Running Ansible on your remote machine > It is possible to have ansible installed on the remote machine and run it there, not just from your local machine connecting to the remote machine. > > Your hosts file will need to use `localhost`, and whenever you run playbooks with `ansible-playbook -i hosts playbook.yml`, you will need to add `-c local` to your command. > > Be **certain** that the playbook that you're writing on the remote machine is stored somewhere safe, like your user home directory, or backed up on your local machine. The cloud can be unreliable and things can disappear at any time. {: .tip}"},{"title":"","url":"/snippets/build_dataset_list.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Build a dataset list > > * Click on {% icon galaxy-selector %} **Operations on multiple datasets** at the top of the history panel > > * Check the boxes next the datasets to select > > * In **For all selected...**, choose **Build dataset list** > > * Ensure that only the wanted samples are selected > > * Enter a name for the new collection > > * Click on **Create list** > {: .tip} >"},{"title":"","url":"/snippets/build_list_collection.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Creating a dataset collection > > > > * Click on **Operations on multiple datasets** (check box icon) at the top of the history panel ![Operations on multiple datasets button](../../../galaxy-data-manipulation/images/historyItemControls.png) > > * Check all the datasets in your history you would like to include > > * Click **For all selected..** and choose **Build dataset list** > > > > ![build list collection menu item]({{site.baseurl}}/topics/galaxy-data-manipulation/images/buildList.png){:width=\"15%\"} > > > > * Enter a name for your collection > > * Click **Create List** to build your collection > > * Click on the checkmark icon at the top of your history again > {: .tip} >"},{"title":"","url":"/snippets/change_datatype.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Changing the datatype > > * Click on the {% icon galaxy-pencil %} **pencil icon** for the dataset to edit its attributes > > * In the central panel, click on the {% icon galaxy-chart-select-data %} **Datatypes** tab on the top > > * Select `{{ include.datatype }}` > > * Click the **Change datatype** button > {: .tip} >"},{"title":"","url":"/snippets/change_dbkey.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Changing Database/Build (dbkey) > > - Click on the {% icon galaxy-pencil %} **pencil icon** for the dataset to edit its attributes > > - In the central panel, change the **Database/Build** field > > - Select your desired database key from the dropdown list: `{{ include.dbkey }}` > > - Click the **Save** button > {: .tip} >"},{"title":"","url":"/snippets/create_dataset_collection.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Create a dataset collection > > > > 1. Click on {% icon galaxy-selector %} icon (**Operation on multiple datasets**) on the top of the history > > 2. Select all the datasets for the collection > > 3. Expand **For all selected** menu > > 4. Select **Build dataset list** > > 5. Enter a name for the collection > > 6. Tick **Hide original elements?** > > 5. Click on **Create list** (and wait a bit) > {: .tip} >"},{"title":"","url":"/snippets/create_new_file.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Creating a new file > > > > * Open the Galaxy Upload Manager > > * Select **Paste/Fetch Data** > > * Paste the file contents into the text field > > {% if include.convertspaces %} * From the Settings menu ({% icon galaxy-gear %}) select **Convert spaces to tabs** {% endif %} > > * Press **Start** and **Close** the window > {: .tip} >"},{"title":"","url":"/snippets/create_new_history.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Creating a new history > > > > 1. Click on the {% icon galaxy-gear %} icon (**History options**) on the top of the history panel > > 2. Click on **Create New** > {: .tip} >"},{"title":"","url":"/snippets/display_extra_training.md","tags":[],"body":"{% for training in include.extra_trainings %} {% if training.type == \"internal\" %} {% assign extra_topic_metadata = site.data[training.topic_name] %} {% assign extra_topic = site.pages | topic_filter:training.topic_name %} {{ extra_topic_metadata.title }} {% if training.tutorials %} {% for extra_tuto in training.tutorials %} {% for topic_tuto in extra_topic %} {% if extra_tuto == topic_tuto.tutorial_name %} {% assign sep = \"\" %} {{ topic_tuto.title }}: {% if topic_tuto.slides %} {% icon slides %} slides {% assign sep = \"-\" %} {% endif %} {% if topic_tuto.hands_on %} {% if topic_tuto.hands_on_url %} {{ sep }} {% icon tutorial %} hands-on {% else %} {{ sep }} {% icon tutorial %} hands-on {% endif %} {% endif %} {% endif %} {% endfor %} {% endfor %} {% endif %} {% elsif training.type == \"none\" %} {{ training.title }} {% else %} {{ training.title }} {% endif %} {% endfor %}"},{"title":"","url":"/snippets/display_extra_training_slides.md","tags":[],"body":"{% capture newLine %} {% endcapture %} {% for training in include.extra_trainings %} {% if training.type == \"internal\" %} {% assign extra_topic_metadata = site.data[training.topic_name] %} {% assign extra_topic = site.pages | topic_filter:training.topic_name %} {% capture topic_desc %}[{{ extra_topic_metadata.title }}]({{ site.baseurl }}/topics/{{ training.topic_name }}){% endcapture %} {% if training.tutorials %} {% for extra_tuto in training.tutorials %} {% for topic_tuto in extra_topic %} {% if extra_tuto == topic_tuto.tutorial_name %} {% assign tuto_desc = topic_tuto.title | append: \": \" | prepend: \" - \" | prepend: newLine %} {% if topic_tuto.slides %} {% capture tuto_slide_desc %}[{% icon slides %} slides]({{ site.baseurl }}/topics/{{ training.topic_name }}/tutorials/{{ topic_tuto.tutorial_name }}/slides.html){% endcapture %} {% assign tuto_desc = tuto_desc | append: tuto_slide_desc %} {% assign sep = \" - \" %} {% endif %} {% if topic_tuto.hands_on %} {% if topic_tuto.hands_on_url %} {% capture tuto_hands_on_desc %}{{ sep }}[{% icon tutorial %} hands-on]({{ topic_tuto.hands_on_url }}){% endcapture %} {% else %} {% capture tuto_hands_on_desc %}{{ sep }}[{% icon tutorial %} hands-on]({{ site.baseurl }}/topics/{{ training.topic_name }}/tutorials/{{ topic_tuto.tutorial_name }}/tutorial.html){% endcapture %} {% endif %} {% assign tuto_desc = tuto_desc | append: tuto_hands_on_desc %} {% endif %} {% assign topic_desc = topic_desc | append: tuto_desc %} {% endif %} {% endfor %} {% endfor %} {% endif %} - {{ topic_desc }} {% elsif training.type == \"none\" %} - {{ training.title }} {% else %} - [{{ training.title }}]({{ training.link }}) {% endif %} {% endfor %}"},{"title":"","url":"/snippets/extra_protein.md","tags":[],"body":"additional HTML formatted text could be inserted as well technical setup of the course could be included or we develop a semiautomated mechanism or we add it as a lesson"},{"title":"","url":"/snippets/extract_workflow.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Extracting a workflow from history > > > > 1. Remove any failed or unwanted jobs from your history. > > 2. Click on **History options** (gear icon {% icon galaxy-gear %}) at the top of your history panel. > > 3. Select **Extract workflow** > > 4. Check the steps, enter a name for your workflow, and press the **Create Workflow** button. > > > {: .tip} >"},{"title":"","url":"/snippets/history_create_new.md","tags":[],"body":"> > > ### {% icon tip %} Starting a new history > > > > * Click the **gear icon** at the top of the history panel > > * Select the option **Create New** from the menu > {: .tip} >"},{"title":"","url":"/snippets/import_from_data_library.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Importing data from a data library > > > > As an alternative to uploading the data from a URL or your computer, the files may also have been made available from a *shared data library*: > > > > * Go into **Shared data** (top panel) then **Data libraries** > > {% if include.path %} > > * {{ include.path }} > > {% else %} > > * Find the correct folder (ask your instructor) > > {% endif %} > > * Select the desired files > > * Click on the **To History** button near the top and select **{{ include.astype | default: \"as Datasets\" }}** from the dropdown menu > > * In the pop-up window, select the history you want to import the files to (or create a new one) > > * Click on **Import** > {: .tip} >"},{"title":"","url":"/snippets/import_via_link.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Importing data via links > > > > * Copy the link location > > * Open the Galaxy Upload Manager ({% icon galaxy-upload %} on the top-right of the tool panel) > > {% if include.collection %} > > * Click on **Collection** on the top > > {% endif %} > > {% if include.collection_type %} > > * Click on **Collection Type** and select `{{ include.collection_type }}` > > {% endif %} > > * Select **Paste/Fetch Data** > > * Paste the link into the text field > > {% if include.link %} > > `{{ include.link }}` > > {% endif %} > > {% if include.link2 %} > > `{{ include.link2 }}` > > {% endif %} > > {% if include.format %} > > * Change **Type** from \"Auto-detect\" to `{{ include.format }}` > > {% endif %} > > {% if include.genome %} > > * Change **Genome** to `{{ include.genome }}` > > {% endif %} > > * Press **Start** > > {% if include.collection %} > > * Click on **Build** when available > > {% if include.pairswaptext %} > > * Ensure that the forward and reverse reads are set to {{ include.pairswaptext }}, respectively. > > * Click **Swap** otherwise > > {% endif %} > > * Enter a name for the collection > > {% if include.collection_name_convention %} > > * A useful naming convention is to use {{ include.collection_name_convention }} > > {% endif %} > > {% if include.collection_name %} > > * {{ include.collection_name }} > > {% endif %} > > * Click on **Create list** (and wait a bit) > > {% else %} > > * **Close** the window > > {% endif %} > > By default, Galaxy uses the URL as the name, so rename the files with a more useful name. > {: .tip} >"},{"title":"","url":"/snippets/import_workflow.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Importing a workflow > > - Click on *Workflow* on the top menu bar of Galaxy. You will see a list of all your workflows. > > - Click on the upload icon {% icon galaxy-upload %} at the top-right of the screen > > - Provide your workflow > > - Option 1: Paste the URL of the workflow into the box labelled *\"Archived Workflow URL\"* > > - Option 2: Upload the worflow file in the box labelled *\"Archived Workflow File\"* > > - Click the **Import workflow** button > {: .tip} >"},{"title":"","url":"/snippets/rename_dataset.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Renaming a dataset > > - Click on the {% icon galaxy-pencil %} **pencil icon** for the dataset to edit its attributes > > - In the central panel, change the **Name** field {% if include.name %} to `{{ include.name }}` {% endif %} > > - Click the **Save** button > {: .tip} >"},{"title":"","url":"/snippets/rename_history.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Renaming a history > > > > 1. Click on **Unnamed history** (or the current name of the history) (**Click to rename history**) at the top of your history panel > > 2. Type the new name > > 3. Press Enter > {: .tip} >"},{"title":"","url":"/snippets/run_workflow.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Running a workflow > > - Click on *Workflow* on the top menu bar of Galaxy. You will see a list of all your workflows. > > - Click on the dropdown menu {% icon galaxy-dropdown %} next to your workflow > > - Select **Run** from the list > > - Configure the workflow as needed > > - Click the **Run Workflow** button at the top-right of the screen > > - You may have to refresh your history to see the queued jobs > {: .tip} >"},{"title":"","url":"/snippets/select_collection.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Select collection of datasets > > > > 1. Click on {% icon param-collection %} **Dataset collection** > > 2. Select the interesting collection in the list > {: .tip} >"},{"title":"","url":"/snippets/select_multiple_datasets.md","tags":[],"body":"> > > ### {% icon tip %} Tip: Select multiple datasets > > > > 1. Click on {% icon param-files %} **Multiple datasets** > > 2. Select several files by keeping the Ctrl (or > > COMMAND) key pressed and clicking on the files of interest > {: .tip} >"},{"title":"","url":"/snippets/use_scratchbook.md","tags":[],"body":"> > > ### {% icon tip %} Tip : Using the Scratchbook > > > > Multiple plots can be compared side-by-side by enabling the *Scratchbook* > > 1. Click on the {% icon galaxy-scratchbook %} *Scratchbook* icon > > 1. Click on the {% icon galaxy-eye %} symbol of the first dataset > > 1. Resize the window of the dataset to desired dimensions > > 1. Click any point in the grey space to close the Scratchbook > > 1. Click on the {% icon galaxy-eye %} symbol of the second dataset > > 1. Resize both windows to desired dimensions > {: .tip} >"},{"title":"","url":"/snippets/warning_results_may_vary.md","tags":[],"body":"> ### {% icon comment %} Note: results may vary > > Your results may be slightly different from the ones presented in this tutorial > due to differing versions of tools, reference data, external databases, or > because of stochastic processes in the algorithms. > {: .comment}"},{"title":"","url":"/topics/protein-structure-analysis/tutorials/visualise-structures/Visualizing_protein_structures_with_YASARA_exercises.md","tags":[],"body":"Exercises created by Joost Van Durme > Part of [Protein Structure Analysis > training](Protein_Structure_Analysis_training \"wikilink\") ## Install Python and PovRay Python and PovRay should be installed already, so you can skip this part. The programming language Python must be installed to use some very useful YASARA features. Simply start YASARA as administrator. Right click the YASARA icon on the desktop and choose \"Run as administrator\". Once the program is opened, click Help > Install program > Python PovRay is used to make high quality publication-ready images and should be downloaded first with: Help > Install program > Povray ## Tutorial movie Play the movie \"Working with YASARA\" movie: Help > Play help movie > General: Working with YASARA ## Scene styles Open the PDB with code 1TRZ in YASARA. File > Load > PDB file from Internet If this option is not there, it means you haven't installed Python yet. Please check above. The molecule will be loaded and presented in the ball style. Different scene styles exist to rapidly change the view: - F1: Ball - F2: Ball & Stick - F3: Stick - F4: C-alpha - F5: Tube - F6: Ribbon - F7: Cartoon - F8: Toggle sidechains on/off (press multiple times and see what happens) Be careful! If you have just made a nice close-up of e.g. an active site where you show some residues and hide others, and put some atoms in balls while others are in sticks, you will lose everything when you press one of the F-keys!!! The F-keys change the viewing style without asking. Try all the different scene styles! ## Showing and hiding residues The function keys F1-F3 show all atoms and residues by default. The keys F4-F7 do not explicitly show atoms and residues but are merely a impressionistic representation of the structure. The F8 keys does, to a certain extent, show atoms, but only of side chains, not main chain atoms. Mostly to do structure analysis, we want to show only the most interesting residues, the ones we want to analyze, and hide all the others. The structure of insulin was crystallized together with some water molecules. In many cases, it is no problem to permanently delete those waters. To visualize the waters, select an atom view such as F1, F2 or F3. See the red water (oxygen) atoms floating around the surface? Edit > Delete > Waters Then select the base scene style without any explicit atoms, e.g. tube style (F5). Press F5. This is our representation of the backbone. There are several ways to show the residues of interest: 1. From the menu - View > Show atoms in > Residue Select Cys7 from Molecule **A** and press OK 2. From the sequence selector ![seqselector.png](seqselector.png \"seqselector.png\") - Hover the mouse on the bottom of the screen, you will see the sequence selector opening. Open it permanently by pressing the blue nailpin on the left side of it. Search for Cys7 from Molecule **B**, right-click and select: Show > Residue Now show the atoms of His5 in Molecule B using a method of choice. And now that we're on it, what is special about the two cysteines we just visualized? **Hiding** individual atoms or residues works in the same way as showing them, only now you should go to **Hide atoms** in the menus. ## Showing and hiding secondary structure Most published molecular images show a detailed active site and all the rest is hidden for clarity. From the previous exercise we show the atoms of 3 residues (let's assume this is our active site). Now secondary structure of the rest of the molecule is also still visible. To hide all that, we do not have to hide atoms, but hide the secondary structure (the F5 tube view) from the rest of the structure. Atoms and residues in YASARA are not the same as the term 'secondary structure'. Atoms and residues are balls and sticks, 'secondary structure' is an artistic impression of the structure (beta sheet arrows, helix ribbons, ...). If you get this concept, you are a YASARA master. So let's hide many of the secondary structure, but keep just a few stretches around our active site. Our active site is Cys7 (A), Cys7 (B) and His 5 (B). This can be done in several ways. Since we would have to hide almost everything, I propose to hide first everything and then show again those stretches that we want. But if you have a better idea, I would like to hear it. Hide all secondary structure: View > Hide secondary structure of > All Then show stretches of residues 2-10 in Mol B and residues 4-10 in Mol A in tube view as: View > Show secondary structure > Tube through > Residue Then select the correct stretches of residues by keeping the CTRL key pressed to select multiple residues. There are still some metal-bound histidines flying around that weren't hidden because they are metal bound (a YASARA specific thing). Hide those histidines by clicking on one of the sidechain atoms, then right-click and select: Hide atoms > Residue The nasty dative bonds and metals can be removed simply by deleting all of them: Edit > Delete > Residue > Name In the name column select all the metals and ions you can find. Et voilà, a publication ready image\\! [center](image:Insulin_hires.jpg \"wikilink\") ## Labels You can put labels on the residues you want to highlight by going to the main menu or selecting an atom from a residue and right-click. In the latter case you select: Label > Residue Note that *residue name* and *residue number* is automatically selected. Change the height to 0.5 or so and select a nice color for the label. Presto\\! ## Colors You can color on all levels: atoms, residues, molecules and objects. So be careful, if you color a residue, all of its atoms will get that color. If you color a molecule, all atoms in that molecule will get that color. Let's color the secondary structure (the backbone in our case) of our active site in orange. But the sidechains should keep their Element colors. So we shouldn't color entire residues, but only a selected atom set. Therefore our selection will be at the atom level, not the residue level. Go to: View > Color > Atom > Belongs to or has > Backbone Then select the orange color (color code 150) and select 'Apply unique color'. Beautiful, isn't it? ## Saving all the beautiful work It would be a pitty that you spent hours creating fancy molecular graphics for that next Nature paper while you can't continue on the work the next day. That's why YASARA can save the entire Scene including orientations, colors, views, everything. To save the current scene, go to: File > Save as > YASARA Scene Choose a filename such as MyInsulin.sce To load the work again in YASARA go to: File > Load > YASARA Scene Careful: loading a Scene will erase everything else! ## Creating high quality images To save the current view to a high quality publication ready image file, go to: File > Save as > Ray-traced hires screenshot This requires that the PovRay program has been installed. See the first item on this page. Usually, you prefer to have a transparent background, so check the respective box. ## Distances **Distances** between atoms are calculated as follows: - select the first atom - keep CTRL pressed and select the second atom. - left of the screen indicates the 'Marked Distance' in Angstrom. What is the distance between the C-alpha (CA) atoms of Tyr19 and Leu16? To solve the question you need to select a view that shows you atoms including C-alphas. Possible views or scene styles that show these atoms can be F1 (ball), F2 (stick), F3 (ball\\&stick) and F4 (C-alpha). The views F5-F8 won't show you any CA's explicitly. Try it. So you've probably noticed that pressing the CTRL button allows you to select multiple atoms. This is important for the next exercise. ## Hydrogen bonds To show hydrogen bonds, YASARA needs the actual hydrogens to be present. In NMR structures these are normally there. But in X-Ray structures hydrogens are missing. Luckily YASARA can add the hydrogens for you. Select tube view (F5) and toggle on the sidechains with F8. Add hydrogens with: Edit > Add > Hydrogens to all Then show the hydrogen-bonds: View > Show interactions > Hydrogen bonds of> All > OK If the view is to chaotic for you, toggle off the sidechains with F8 (press untill the sidechains are hidden). Do you see the typical helix and beta sheet pattern? Arg22 from Molecule/Chain B is making an hydrogen bonded electrostatic interaction (salt bridge) with another residue. Which residue? To remove the hydrogen bonds, you have multiple choices: View > Hide hydrogen bonds of > All or just delete all hydrogens (this will also delete all hydrogen bonds): Edit > Delete > Hydrogens ## Surfaces It can be very useful and informative to show the molecular surface of a protein. you can visualize cavities, ligand binding sites, etc ... To show the molecular surface of one monomer of dimeric insulin, go to: View > Show surface of > Molecule Select in the *Name* column A and B (these are the two chains in 1 subunit). Press *Continue with surface color* and make sure Alpha is 100. Any number lower than 100 will create transparency in the surface (could be nice as well). ## Molecular graphics exercise Try to reproduce the following image of the 1TRZ insulin structure (hints below): [image:insulin.png](image:insulin.png \"wikilink\") Hints: - choose the proper secondary structure scene style (F6 was used here) - find the correct orientation first - color all backbone atoms in gray - find the residue numbers of the 2 colored helices - color those residues magenta - show the sidechain atoms and the CA of the two histidines and the glutamate - color the sidechain atoms of all residues in the Element color - label the histidines and the glutamate - if you need some help how to change the parameters for the label, please have a look at Help -\\> Show user manual and search in Commands / Index ## More coloring Download GroEL via PDB code 1WE3 in YASARA. Try to reproduce (approximately) the following image (hints below): [image:groel.png](image:groel.png \"wikilink\") Hints: - load the PDB as File \\> Load \\> PDB file from internet - zoom out and find the correct orientation - delete the ADP, DMS and Mg molecules (are treated as residues in YASARA). So Edit \\> Delete \\> Residue \\> Adp ... - color by molecule (every molecule will get another color) and color by gradient (now you need to specify 2 colors, the begin and end color). - choose a first color (eg. color with code 0) - choose a second color (eg. color with code 300, so you go over the entire color wheel spectrum) More exercises can be found on the [basic bioinformatics exercises page](http://wiki.bits.vib.be/index.php/Exercises_on_Protein_Structure)."}]}